{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77f5ae9e-e49e-407b-8fc2-642d14707b2b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import pickle\n",
    "# from statsmodels.graphics.gofplots import qqplot\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR\n",
    "import sklearn.gaussian_process as gp\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "import matplotlib.dates as mdates\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "from skopt import gp_minimize\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from lightgbm import LGBMRegressor\n",
    "import nbformat\n",
    "from bayes_opt import BayesianOptimization\n",
    "import xgboost as xgb\n",
    "from statsmodels.graphics.gofplots import qqplot_2samples\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10b1b230-530b-4fb6-ad75-a76887dd666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "np.random.seed(101)\n",
    "random.seed(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6635e6f-be64-4f9d-ae6c-4f714a303da1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dateTime</th>\n",
       "      <th>pm10_grimm</th>\n",
       "      <th>pm2_5_grimm</th>\n",
       "      <th>pm1_grimm</th>\n",
       "      <th>inhalable_grimm</th>\n",
       "      <th>thoracic_grimm</th>\n",
       "      <th>alveolic_grimm</th>\n",
       "      <th>NH3_loRa</th>\n",
       "      <th>CO_loRa</th>\n",
       "      <th>NO2_loRa</th>\n",
       "      <th>...</th>\n",
       "      <th>Humidity_loRa</th>\n",
       "      <th>pm1Palas</th>\n",
       "      <th>pm2_5Palas</th>\n",
       "      <th>pm4Palas</th>\n",
       "      <th>pm10Palas</th>\n",
       "      <th>pmTotalPalas</th>\n",
       "      <th>dCnPalas</th>\n",
       "      <th>temperaturePalas</th>\n",
       "      <th>humidityPalas</th>\n",
       "      <th>pressurehPalas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>04/08/2019 03:22:30 +0000</td>\n",
       "      <td>8.06</td>\n",
       "      <td>7.48</td>\n",
       "      <td>6.06</td>\n",
       "      <td>8.06</td>\n",
       "      <td>8.06</td>\n",
       "      <td>7.74</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0</td>\n",
       "      <td>4.92</td>\n",
       "      <td>...</td>\n",
       "      <td>67</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>9.210000</td>\n",
       "      <td>11.390000</td>\n",
       "      <td>13.516667</td>\n",
       "      <td>15.256667</td>\n",
       "      <td>191.830000</td>\n",
       "      <td>28.303333</td>\n",
       "      <td>43.425000</td>\n",
       "      <td>987.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>04/08/2019 03:23:00 +0000</td>\n",
       "      <td>7.84</td>\n",
       "      <td>7.04</td>\n",
       "      <td>6.06</td>\n",
       "      <td>7.84</td>\n",
       "      <td>7.82</td>\n",
       "      <td>7.32</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0</td>\n",
       "      <td>4.90</td>\n",
       "      <td>...</td>\n",
       "      <td>68</td>\n",
       "      <td>7.493333</td>\n",
       "      <td>9.210000</td>\n",
       "      <td>11.386667</td>\n",
       "      <td>13.506667</td>\n",
       "      <td>15.246667</td>\n",
       "      <td>191.786667</td>\n",
       "      <td>28.286667</td>\n",
       "      <td>43.316667</td>\n",
       "      <td>987.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>04/08/2019 03:23:30 +0000</td>\n",
       "      <td>8.98</td>\n",
       "      <td>7.58</td>\n",
       "      <td>6.08</td>\n",
       "      <td>8.98</td>\n",
       "      <td>8.96</td>\n",
       "      <td>8.22</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0</td>\n",
       "      <td>4.90</td>\n",
       "      <td>...</td>\n",
       "      <td>68</td>\n",
       "      <td>7.490000</td>\n",
       "      <td>9.206667</td>\n",
       "      <td>11.380000</td>\n",
       "      <td>13.493333</td>\n",
       "      <td>15.226667</td>\n",
       "      <td>191.736667</td>\n",
       "      <td>28.250000</td>\n",
       "      <td>43.301667</td>\n",
       "      <td>987.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04/08/2019 03:24:00 +0000</td>\n",
       "      <td>8.90</td>\n",
       "      <td>7.60</td>\n",
       "      <td>5.96</td>\n",
       "      <td>8.90</td>\n",
       "      <td>8.74</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0</td>\n",
       "      <td>4.90</td>\n",
       "      <td>...</td>\n",
       "      <td>68</td>\n",
       "      <td>7.490000</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>11.370000</td>\n",
       "      <td>13.480000</td>\n",
       "      <td>15.206667</td>\n",
       "      <td>191.703333</td>\n",
       "      <td>28.215000</td>\n",
       "      <td>43.483333</td>\n",
       "      <td>987.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>04/08/2019 03:24:30 +0000</td>\n",
       "      <td>9.72</td>\n",
       "      <td>7.72</td>\n",
       "      <td>6.14</td>\n",
       "      <td>9.72</td>\n",
       "      <td>9.66</td>\n",
       "      <td>8.40</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0</td>\n",
       "      <td>4.90</td>\n",
       "      <td>...</td>\n",
       "      <td>68</td>\n",
       "      <td>7.490000</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>13.466667</td>\n",
       "      <td>15.193333</td>\n",
       "      <td>191.676667</td>\n",
       "      <td>28.205000</td>\n",
       "      <td>43.381667</td>\n",
       "      <td>987.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2058</th>\n",
       "      <td>04/10/2019 10:52:30 +0000</td>\n",
       "      <td>8.64</td>\n",
       "      <td>5.62</td>\n",
       "      <td>4.94</td>\n",
       "      <td>8.72</td>\n",
       "      <td>8.18</td>\n",
       "      <td>6.08</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0</td>\n",
       "      <td>4.71</td>\n",
       "      <td>...</td>\n",
       "      <td>52</td>\n",
       "      <td>5.913333</td>\n",
       "      <td>6.883333</td>\n",
       "      <td>7.923333</td>\n",
       "      <td>12.833333</td>\n",
       "      <td>20.376667</td>\n",
       "      <td>204.200000</td>\n",
       "      <td>29.251667</td>\n",
       "      <td>30.115000</td>\n",
       "      <td>980.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2059</th>\n",
       "      <td>04/10/2019 10:53:00 +0000</td>\n",
       "      <td>9.72</td>\n",
       "      <td>5.98</td>\n",
       "      <td>5.20</td>\n",
       "      <td>33.96</td>\n",
       "      <td>11.90</td>\n",
       "      <td>6.86</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0</td>\n",
       "      <td>4.73</td>\n",
       "      <td>...</td>\n",
       "      <td>52</td>\n",
       "      <td>5.923333</td>\n",
       "      <td>6.853333</td>\n",
       "      <td>7.813333</td>\n",
       "      <td>12.610000</td>\n",
       "      <td>20.093333</td>\n",
       "      <td>205.363333</td>\n",
       "      <td>29.228333</td>\n",
       "      <td>30.173333</td>\n",
       "      <td>980.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2060</th>\n",
       "      <td>04/10/2019 10:54:00 +0000</td>\n",
       "      <td>11.14</td>\n",
       "      <td>6.82</td>\n",
       "      <td>5.10</td>\n",
       "      <td>30.24</td>\n",
       "      <td>13.58</td>\n",
       "      <td>7.72</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0</td>\n",
       "      <td>4.76</td>\n",
       "      <td>...</td>\n",
       "      <td>52</td>\n",
       "      <td>5.986667</td>\n",
       "      <td>6.866667</td>\n",
       "      <td>7.680000</td>\n",
       "      <td>12.440000</td>\n",
       "      <td>19.880000</td>\n",
       "      <td>208.336667</td>\n",
       "      <td>29.238333</td>\n",
       "      <td>30.260000</td>\n",
       "      <td>980.038333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2061</th>\n",
       "      <td>04/10/2019 10:54:30 +0000</td>\n",
       "      <td>10.02</td>\n",
       "      <td>6.66</td>\n",
       "      <td>5.12</td>\n",
       "      <td>33.36</td>\n",
       "      <td>14.00</td>\n",
       "      <td>7.46</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0</td>\n",
       "      <td>4.73</td>\n",
       "      <td>...</td>\n",
       "      <td>52</td>\n",
       "      <td>6.023333</td>\n",
       "      <td>6.903333</td>\n",
       "      <td>7.716667</td>\n",
       "      <td>12.476667</td>\n",
       "      <td>19.916667</td>\n",
       "      <td>209.700000</td>\n",
       "      <td>29.180000</td>\n",
       "      <td>30.353333</td>\n",
       "      <td>980.218333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2062</th>\n",
       "      <td>04/10/2019 10:55:00 +0000</td>\n",
       "      <td>8.70</td>\n",
       "      <td>5.76</td>\n",
       "      <td>5.04</td>\n",
       "      <td>20.24</td>\n",
       "      <td>9.96</td>\n",
       "      <td>6.52</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0</td>\n",
       "      <td>4.76</td>\n",
       "      <td>...</td>\n",
       "      <td>52</td>\n",
       "      <td>6.053333</td>\n",
       "      <td>6.943333</td>\n",
       "      <td>7.760000</td>\n",
       "      <td>12.520000</td>\n",
       "      <td>19.963333</td>\n",
       "      <td>211.190000</td>\n",
       "      <td>29.210000</td>\n",
       "      <td>30.280000</td>\n",
       "      <td>980.180000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2063 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       dateTime  pm10_grimm  pm2_5_grimm  pm1_grimm  \\\n",
       "0     04/08/2019 03:22:30 +0000        8.06         7.48       6.06   \n",
       "1     04/08/2019 03:23:00 +0000        7.84         7.04       6.06   \n",
       "2     04/08/2019 03:23:30 +0000        8.98         7.58       6.08   \n",
       "3     04/08/2019 03:24:00 +0000        8.90         7.60       5.96   \n",
       "4     04/08/2019 03:24:30 +0000        9.72         7.72       6.14   \n",
       "...                         ...         ...          ...        ...   \n",
       "2058  04/10/2019 10:52:30 +0000        8.64         5.62       4.94   \n",
       "2059  04/10/2019 10:53:00 +0000        9.72         5.98       5.20   \n",
       "2060  04/10/2019 10:54:00 +0000       11.14         6.82       5.10   \n",
       "2061  04/10/2019 10:54:30 +0000       10.02         6.66       5.12   \n",
       "2062  04/10/2019 10:55:00 +0000        8.70         5.76       5.04   \n",
       "\n",
       "      inhalable_grimm  thoracic_grimm  alveolic_grimm  NH3_loRa  CO_loRa  \\\n",
       "0                8.06            8.06            7.74      0.02        0   \n",
       "1                7.84            7.82            7.32      0.02        0   \n",
       "2                8.98            8.96            8.22      0.02        0   \n",
       "3                8.90            8.74            7.80      0.02        0   \n",
       "4                9.72            9.66            8.40      0.02        0   \n",
       "...               ...             ...             ...       ...      ...   \n",
       "2058             8.72            8.18            6.08      0.14        0   \n",
       "2059            33.96           11.90            6.86      0.14        0   \n",
       "2060            30.24           13.58            7.72      0.14        0   \n",
       "2061            33.36           14.00            7.46      0.14        0   \n",
       "2062            20.24            9.96            6.52      0.14        0   \n",
       "\n",
       "      NO2_loRa  ...  Humidity_loRa  pm1Palas  pm2_5Palas   pm4Palas  \\\n",
       "0         4.92  ...             67  7.500000    9.210000  11.390000   \n",
       "1         4.90  ...             68  7.493333    9.210000  11.386667   \n",
       "2         4.90  ...             68  7.490000    9.206667  11.380000   \n",
       "3         4.90  ...             68  7.490000    9.200000  11.370000   \n",
       "4         4.90  ...             68  7.490000    9.200000  11.360000   \n",
       "...        ...  ...            ...       ...         ...        ...   \n",
       "2058      4.71  ...             52  5.913333    6.883333   7.923333   \n",
       "2059      4.73  ...             52  5.923333    6.853333   7.813333   \n",
       "2060      4.76  ...             52  5.986667    6.866667   7.680000   \n",
       "2061      4.73  ...             52  6.023333    6.903333   7.716667   \n",
       "2062      4.76  ...             52  6.053333    6.943333   7.760000   \n",
       "\n",
       "      pm10Palas  pmTotalPalas    dCnPalas  temperaturePalas  humidityPalas  \\\n",
       "0     13.516667     15.256667  191.830000         28.303333      43.425000   \n",
       "1     13.506667     15.246667  191.786667         28.286667      43.316667   \n",
       "2     13.493333     15.226667  191.736667         28.250000      43.301667   \n",
       "3     13.480000     15.206667  191.703333         28.215000      43.483333   \n",
       "4     13.466667     15.193333  191.676667         28.205000      43.381667   \n",
       "...         ...           ...         ...               ...            ...   \n",
       "2058  12.833333     20.376667  204.200000         29.251667      30.115000   \n",
       "2059  12.610000     20.093333  205.363333         29.228333      30.173333   \n",
       "2060  12.440000     19.880000  208.336667         29.238333      30.260000   \n",
       "2061  12.476667     19.916667  209.700000         29.180000      30.353333   \n",
       "2062  12.520000     19.963333  211.190000         29.210000      30.280000   \n",
       "\n",
       "      pressurehPalas  \n",
       "0         987.790000  \n",
       "1         987.850000  \n",
       "2         987.810000  \n",
       "3         987.810000  \n",
       "4         987.810000  \n",
       "...              ...  \n",
       "2058      980.160000  \n",
       "2059      980.280000  \n",
       "2060      980.038333  \n",
       "2061      980.218333  \n",
       "2062      980.180000  \n",
       "\n",
       "[2063 rows x 33 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r'D:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\data\\calibrate.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f79d4a75-311f-4d88-ba27-d3eda3decbeb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['dateTime', 'pm10_grimm', 'pm2_5_grimm', 'pm1_grimm', 'inhalable_grimm',\n",
       "       'thoracic_grimm', 'alveolic_grimm', 'NH3_loRa', 'CO_loRa', 'NO2_loRa',\n",
       "       'C3H8_loRa', 'C4H10_loRa', 'CH4_loRa', 'H2_loRa', 'C2H5OH_loRa',\n",
       "       'P1_lpo_loRa', 'P1_ratio_loRa', 'P1_conc_loRa', 'P2_lpo_loRa',\n",
       "       'P2_ratio_loRa', 'P2_conc_loRa', 'Temperature_loRa', 'Pressure_loRa',\n",
       "       'Humidity_loRa', 'pm1Palas', 'pm2_5Palas', 'pm4Palas', 'pm10Palas',\n",
       "       'pmTotalPalas', 'dCnPalas', 'temperaturePalas', 'humidityPalas',\n",
       "       'pressurehPalas'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d0a4bc2-9351-4b4f-8ca4-01127328ffee",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2063 entries, 0 to 2062\n",
      "Data columns (total 25 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   alveolic_grimm    2063 non-null   float64\n",
      " 1   dateTime          2063 non-null   object \n",
      " 2   pm10_grimm        2063 non-null   float64\n",
      " 3   pm2_5_grimm       2063 non-null   float64\n",
      " 4   pm1_grimm         2063 non-null   float64\n",
      " 5   inhalable_grimm   2063 non-null   float64\n",
      " 6   thoracic_grimm    2063 non-null   float64\n",
      " 7   P1_lpo_loRa       2063 non-null   int64  \n",
      " 8   P1_ratio_loRa     2063 non-null   float64\n",
      " 9   P1_conc_loRa      2063 non-null   float64\n",
      " 10  P2_lpo_loRa       2063 non-null   int64  \n",
      " 11  P2_ratio_loRa     2063 non-null   float64\n",
      " 12  P2_conc_loRa      2063 non-null   float64\n",
      " 13  Temperature_loRa  2063 non-null   float64\n",
      " 14  Pressure_loRa     2063 non-null   int64  \n",
      " 15  Humidity_loRa     2063 non-null   int64  \n",
      " 16  pm1Palas          2063 non-null   float64\n",
      " 17  pm2_5Palas        2063 non-null   float64\n",
      " 18  pm4Palas          2063 non-null   float64\n",
      " 19  pm10Palas         2063 non-null   float64\n",
      " 20  pmTotalPalas      2063 non-null   float64\n",
      " 21  dCnPalas          2063 non-null   float64\n",
      " 22  temperaturePalas  2063 non-null   float64\n",
      " 23  humidityPalas     2063 non-null   float64\n",
      " 24  pressurehPalas    2063 non-null   float64\n",
      "dtypes: float64(20), int64(4), object(1)\n",
      "memory usage: 403.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df= pd.concat([data.iloc[:,6], data.iloc[:,0:6],data.iloc[:,15:data.shape[1]]],axis = 1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c25a7e5a-8744-47bc-90b8-e94f693c4f76",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alveolic_grimm</th>\n",
       "      <th>dateTime</th>\n",
       "      <th>pm10_grimm</th>\n",
       "      <th>pm2_5_grimm</th>\n",
       "      <th>pm1_grimm</th>\n",
       "      <th>inhalable_grimm</th>\n",
       "      <th>thoracic_grimm</th>\n",
       "      <th>P1_lpo_loRa</th>\n",
       "      <th>P1_ratio_loRa</th>\n",
       "      <th>P1_conc_loRa</th>\n",
       "      <th>...</th>\n",
       "      <th>Humidity_loRa</th>\n",
       "      <th>pm1Palas</th>\n",
       "      <th>pm2_5Palas</th>\n",
       "      <th>pm4Palas</th>\n",
       "      <th>pm10Palas</th>\n",
       "      <th>pmTotalPalas</th>\n",
       "      <th>dCnPalas</th>\n",
       "      <th>temperaturePalas</th>\n",
       "      <th>humidityPalas</th>\n",
       "      <th>pressurehPalas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.74</td>\n",
       "      <td>04/08/2019 03:22:30 +0000</td>\n",
       "      <td>8.06</td>\n",
       "      <td>7.48</td>\n",
       "      <td>6.06</td>\n",
       "      <td>8.06</td>\n",
       "      <td>8.06</td>\n",
       "      <td>2556</td>\n",
       "      <td>0.02</td>\n",
       "      <td>9.48</td>\n",
       "      <td>...</td>\n",
       "      <td>67</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>9.210000</td>\n",
       "      <td>11.390000</td>\n",
       "      <td>13.516667</td>\n",
       "      <td>15.256667</td>\n",
       "      <td>191.830000</td>\n",
       "      <td>28.303333</td>\n",
       "      <td>43.425000</td>\n",
       "      <td>987.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.32</td>\n",
       "      <td>04/08/2019 03:23:00 +0000</td>\n",
       "      <td>7.84</td>\n",
       "      <td>7.04</td>\n",
       "      <td>6.06</td>\n",
       "      <td>7.84</td>\n",
       "      <td>7.82</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>...</td>\n",
       "      <td>68</td>\n",
       "      <td>7.493333</td>\n",
       "      <td>9.210000</td>\n",
       "      <td>11.386667</td>\n",
       "      <td>13.506667</td>\n",
       "      <td>15.246667</td>\n",
       "      <td>191.786667</td>\n",
       "      <td>28.286667</td>\n",
       "      <td>43.316667</td>\n",
       "      <td>987.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.22</td>\n",
       "      <td>04/08/2019 03:23:30 +0000</td>\n",
       "      <td>8.98</td>\n",
       "      <td>7.58</td>\n",
       "      <td>6.08</td>\n",
       "      <td>8.98</td>\n",
       "      <td>8.96</td>\n",
       "      <td>86608</td>\n",
       "      <td>0.58</td>\n",
       "      <td>299.81</td>\n",
       "      <td>...</td>\n",
       "      <td>68</td>\n",
       "      <td>7.490000</td>\n",
       "      <td>9.206667</td>\n",
       "      <td>11.380000</td>\n",
       "      <td>13.493333</td>\n",
       "      <td>15.226667</td>\n",
       "      <td>191.736667</td>\n",
       "      <td>28.250000</td>\n",
       "      <td>43.301667</td>\n",
       "      <td>987.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.80</td>\n",
       "      <td>04/08/2019 03:24:00 +0000</td>\n",
       "      <td>8.90</td>\n",
       "      <td>7.60</td>\n",
       "      <td>5.96</td>\n",
       "      <td>8.90</td>\n",
       "      <td>8.74</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>...</td>\n",
       "      <td>68</td>\n",
       "      <td>7.490000</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>11.370000</td>\n",
       "      <td>13.480000</td>\n",
       "      <td>15.206667</td>\n",
       "      <td>191.703333</td>\n",
       "      <td>28.215000</td>\n",
       "      <td>43.483333</td>\n",
       "      <td>987.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.40</td>\n",
       "      <td>04/08/2019 03:24:30 +0000</td>\n",
       "      <td>9.72</td>\n",
       "      <td>7.72</td>\n",
       "      <td>6.14</td>\n",
       "      <td>9.72</td>\n",
       "      <td>9.66</td>\n",
       "      <td>153937</td>\n",
       "      <td>1.03</td>\n",
       "      <td>531.46</td>\n",
       "      <td>...</td>\n",
       "      <td>68</td>\n",
       "      <td>7.490000</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>13.466667</td>\n",
       "      <td>15.193333</td>\n",
       "      <td>191.676667</td>\n",
       "      <td>28.205000</td>\n",
       "      <td>43.381667</td>\n",
       "      <td>987.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2058</th>\n",
       "      <td>6.08</td>\n",
       "      <td>04/10/2019 10:52:30 +0000</td>\n",
       "      <td>8.64</td>\n",
       "      <td>5.62</td>\n",
       "      <td>4.94</td>\n",
       "      <td>8.72</td>\n",
       "      <td>8.18</td>\n",
       "      <td>73639</td>\n",
       "      <td>0.49</td>\n",
       "      <td>255.12</td>\n",
       "      <td>...</td>\n",
       "      <td>52</td>\n",
       "      <td>5.913333</td>\n",
       "      <td>6.883333</td>\n",
       "      <td>7.923333</td>\n",
       "      <td>12.833333</td>\n",
       "      <td>20.376667</td>\n",
       "      <td>204.200000</td>\n",
       "      <td>29.251667</td>\n",
       "      <td>30.115000</td>\n",
       "      <td>980.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2059</th>\n",
       "      <td>6.86</td>\n",
       "      <td>04/10/2019 10:53:00 +0000</td>\n",
       "      <td>9.72</td>\n",
       "      <td>5.98</td>\n",
       "      <td>5.20</td>\n",
       "      <td>33.96</td>\n",
       "      <td>11.90</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>...</td>\n",
       "      <td>52</td>\n",
       "      <td>5.923333</td>\n",
       "      <td>6.853333</td>\n",
       "      <td>7.813333</td>\n",
       "      <td>12.610000</td>\n",
       "      <td>20.093333</td>\n",
       "      <td>205.363333</td>\n",
       "      <td>29.228333</td>\n",
       "      <td>30.173333</td>\n",
       "      <td>980.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2060</th>\n",
       "      <td>7.72</td>\n",
       "      <td>04/10/2019 10:54:00 +0000</td>\n",
       "      <td>11.14</td>\n",
       "      <td>6.82</td>\n",
       "      <td>5.10</td>\n",
       "      <td>30.24</td>\n",
       "      <td>13.58</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>...</td>\n",
       "      <td>52</td>\n",
       "      <td>5.986667</td>\n",
       "      <td>6.866667</td>\n",
       "      <td>7.680000</td>\n",
       "      <td>12.440000</td>\n",
       "      <td>19.880000</td>\n",
       "      <td>208.336667</td>\n",
       "      <td>29.238333</td>\n",
       "      <td>30.260000</td>\n",
       "      <td>980.038333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2061</th>\n",
       "      <td>7.46</td>\n",
       "      <td>04/10/2019 10:54:30 +0000</td>\n",
       "      <td>10.02</td>\n",
       "      <td>6.66</td>\n",
       "      <td>5.12</td>\n",
       "      <td>33.36</td>\n",
       "      <td>14.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>...</td>\n",
       "      <td>52</td>\n",
       "      <td>6.023333</td>\n",
       "      <td>6.903333</td>\n",
       "      <td>7.716667</td>\n",
       "      <td>12.476667</td>\n",
       "      <td>19.916667</td>\n",
       "      <td>209.700000</td>\n",
       "      <td>29.180000</td>\n",
       "      <td>30.353333</td>\n",
       "      <td>980.218333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2062</th>\n",
       "      <td>6.52</td>\n",
       "      <td>04/10/2019 10:55:00 +0000</td>\n",
       "      <td>8.70</td>\n",
       "      <td>5.76</td>\n",
       "      <td>5.04</td>\n",
       "      <td>20.24</td>\n",
       "      <td>9.96</td>\n",
       "      <td>47132</td>\n",
       "      <td>0.31</td>\n",
       "      <td>163.67</td>\n",
       "      <td>...</td>\n",
       "      <td>52</td>\n",
       "      <td>6.053333</td>\n",
       "      <td>6.943333</td>\n",
       "      <td>7.760000</td>\n",
       "      <td>12.520000</td>\n",
       "      <td>19.963333</td>\n",
       "      <td>211.190000</td>\n",
       "      <td>29.210000</td>\n",
       "      <td>30.280000</td>\n",
       "      <td>980.180000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2063 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      alveolic_grimm                   dateTime  pm10_grimm  pm2_5_grimm  \\\n",
       "0               7.74  04/08/2019 03:22:30 +0000        8.06         7.48   \n",
       "1               7.32  04/08/2019 03:23:00 +0000        7.84         7.04   \n",
       "2               8.22  04/08/2019 03:23:30 +0000        8.98         7.58   \n",
       "3               7.80  04/08/2019 03:24:00 +0000        8.90         7.60   \n",
       "4               8.40  04/08/2019 03:24:30 +0000        9.72         7.72   \n",
       "...              ...                        ...         ...          ...   \n",
       "2058            6.08  04/10/2019 10:52:30 +0000        8.64         5.62   \n",
       "2059            6.86  04/10/2019 10:53:00 +0000        9.72         5.98   \n",
       "2060            7.72  04/10/2019 10:54:00 +0000       11.14         6.82   \n",
       "2061            7.46  04/10/2019 10:54:30 +0000       10.02         6.66   \n",
       "2062            6.52  04/10/2019 10:55:00 +0000        8.70         5.76   \n",
       "\n",
       "      pm1_grimm  inhalable_grimm  thoracic_grimm  P1_lpo_loRa  P1_ratio_loRa  \\\n",
       "0          6.06             8.06            8.06         2556           0.02   \n",
       "1          6.06             7.84            7.82            0           0.00   \n",
       "2          6.08             8.98            8.96        86608           0.58   \n",
       "3          5.96             8.90            8.74            0           0.00   \n",
       "4          6.14             9.72            9.66       153937           1.03   \n",
       "...         ...              ...             ...          ...            ...   \n",
       "2058       4.94             8.72            8.18        73639           0.49   \n",
       "2059       5.20            33.96           11.90            0           0.00   \n",
       "2060       5.10            30.24           13.58            0           0.00   \n",
       "2061       5.12            33.36           14.00            0           0.00   \n",
       "2062       5.04            20.24            9.96        47132           0.31   \n",
       "\n",
       "      P1_conc_loRa  ...  Humidity_loRa  pm1Palas  pm2_5Palas   pm4Palas  \\\n",
       "0             9.48  ...             67  7.500000    9.210000  11.390000   \n",
       "1             0.62  ...             68  7.493333    9.210000  11.386667   \n",
       "2           299.81  ...             68  7.490000    9.206667  11.380000   \n",
       "3             0.62  ...             68  7.490000    9.200000  11.370000   \n",
       "4           531.46  ...             68  7.490000    9.200000  11.360000   \n",
       "...            ...  ...            ...       ...         ...        ...   \n",
       "2058        255.12  ...             52  5.913333    6.883333   7.923333   \n",
       "2059          0.62  ...             52  5.923333    6.853333   7.813333   \n",
       "2060          0.62  ...             52  5.986667    6.866667   7.680000   \n",
       "2061          0.62  ...             52  6.023333    6.903333   7.716667   \n",
       "2062        163.67  ...             52  6.053333    6.943333   7.760000   \n",
       "\n",
       "      pm10Palas  pmTotalPalas    dCnPalas  temperaturePalas  humidityPalas  \\\n",
       "0     13.516667     15.256667  191.830000         28.303333      43.425000   \n",
       "1     13.506667     15.246667  191.786667         28.286667      43.316667   \n",
       "2     13.493333     15.226667  191.736667         28.250000      43.301667   \n",
       "3     13.480000     15.206667  191.703333         28.215000      43.483333   \n",
       "4     13.466667     15.193333  191.676667         28.205000      43.381667   \n",
       "...         ...           ...         ...               ...            ...   \n",
       "2058  12.833333     20.376667  204.200000         29.251667      30.115000   \n",
       "2059  12.610000     20.093333  205.363333         29.228333      30.173333   \n",
       "2060  12.440000     19.880000  208.336667         29.238333      30.260000   \n",
       "2061  12.476667     19.916667  209.700000         29.180000      30.353333   \n",
       "2062  12.520000     19.963333  211.190000         29.210000      30.280000   \n",
       "\n",
       "      pressurehPalas  \n",
       "0         987.790000  \n",
       "1         987.850000  \n",
       "2         987.810000  \n",
       "3         987.810000  \n",
       "4         987.810000  \n",
       "...              ...  \n",
       "2058      980.160000  \n",
       "2059      980.280000  \n",
       "2060      980.038333  \n",
       "2061      980.218333  \n",
       "2062      980.180000  \n",
       "\n",
       "[2063 rows x 25 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def data_checker(df):\n",
    "    # data_frame = df.iloc[:,1:16]\n",
    "    data_frame =  df.iloc[:,7:-3] \n",
    "    # Palas limits are 0-100 mg/m3\n",
    "    #BME limits are :Temp -40C to 85C\n",
    "                   #:Pressure 300hPa to 1100 hPa or  300*100Pa to 1100*100 Pa\n",
    "                   #:Humidity 0% to 100%\n",
    "     #PPD42NS :Operating Temp is 0C to 45C\n",
    "    idx = data_frame[(data_frame['Temperature_loRa']>=0) & (data_frame['Temperature_loRa']<=45) &\n",
    "                    (data_frame['Pressure_loRa']>=300*100) & (data_frame['Pressure_loRa']<=1100*100) &\n",
    "                    (data_frame['Humidity_loRa']>=0) & (data_frame['Humidity_loRa']<=100) &\n",
    "                    (data_frame['pm1Palas']>=0) & (data_frame['pm2_5Palas']>=0) &\n",
    "                    (data_frame['pm4Palas']>=0) & (data_frame['pm10Palas']>=0) &\n",
    "                    (data_frame['pmTotalPalas']>=0) & (data_frame['dCnPalas']>=0)&\n",
    "                    (data_frame['pm1Palas']<=100000) & (data_frame['pm2_5Palas']<=100000) &\n",
    "                    (data_frame['pm4Palas']<=100000) & (data_frame['pm10Palas']<=100000) &\n",
    "                    (data_frame['pmTotalPalas']<=100000) & (data_frame['dCnPalas']<=100000)].index\n",
    "                    # &\n",
    "                    # (data_frame['P1_conc_loRa'] + data_frame['P2_conc_loRa']>=0) &\n",
    "                    # (data_frame['P1_conc_loRa'] + data_frame['P2_conc_loRa']<=28000)].index\n",
    "    return df.loc[idx]\n",
    "df = data_checker(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "472fd119-9d92-43f6-bf89-c83e458afad7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alveolic_grimm</th>\n",
       "      <th>pm10_grimm</th>\n",
       "      <th>pm2_5_grimm</th>\n",
       "      <th>pm1_grimm</th>\n",
       "      <th>inhalable_grimm</th>\n",
       "      <th>thoracic_grimm</th>\n",
       "      <th>P1_lpo_loRa</th>\n",
       "      <th>P1_ratio_loRa</th>\n",
       "      <th>P1_conc_loRa</th>\n",
       "      <th>P2_lpo_loRa</th>\n",
       "      <th>...</th>\n",
       "      <th>Humidity_loRa</th>\n",
       "      <th>pm1Palas</th>\n",
       "      <th>pm2_5Palas</th>\n",
       "      <th>pm4Palas</th>\n",
       "      <th>pm10Palas</th>\n",
       "      <th>pmTotalPalas</th>\n",
       "      <th>dCnPalas</th>\n",
       "      <th>temperaturePalas</th>\n",
       "      <th>humidityPalas</th>\n",
       "      <th>pressurehPalas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2063.000000</td>\n",
       "      <td>2063.000000</td>\n",
       "      <td>2063.000000</td>\n",
       "      <td>2063.000000</td>\n",
       "      <td>2063.000000</td>\n",
       "      <td>2063.000000</td>\n",
       "      <td>2063.000000</td>\n",
       "      <td>2063.000000</td>\n",
       "      <td>2063.000000</td>\n",
       "      <td>2.063000e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>2063.000000</td>\n",
       "      <td>2063.000000</td>\n",
       "      <td>2063.000000</td>\n",
       "      <td>2063.000000</td>\n",
       "      <td>2063.000000</td>\n",
       "      <td>2063.000000</td>\n",
       "      <td>2063.000000</td>\n",
       "      <td>2063.000000</td>\n",
       "      <td>2063.000000</td>\n",
       "      <td>2063.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.626583</td>\n",
       "      <td>10.021008</td>\n",
       "      <td>6.923800</td>\n",
       "      <td>5.866670</td>\n",
       "      <td>18.277838</td>\n",
       "      <td>10.487571</td>\n",
       "      <td>26775.528357</td>\n",
       "      <td>0.178507</td>\n",
       "      <td>93.041299</td>\n",
       "      <td>1.317343e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>46.711100</td>\n",
       "      <td>6.525406</td>\n",
       "      <td>7.786447</td>\n",
       "      <td>9.242287</td>\n",
       "      <td>14.728641</td>\n",
       "      <td>24.882920</td>\n",
       "      <td>209.331043</td>\n",
       "      <td>32.621074</td>\n",
       "      <td>26.367609</td>\n",
       "      <td>982.382139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.123536</td>\n",
       "      <td>2.755813</td>\n",
       "      <td>2.063206</td>\n",
       "      <td>1.780787</td>\n",
       "      <td>15.991072</td>\n",
       "      <td>2.924185</td>\n",
       "      <td>63234.738633</td>\n",
       "      <td>0.421589</td>\n",
       "      <td>218.421909</td>\n",
       "      <td>1.904410e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>18.411397</td>\n",
       "      <td>1.910236</td>\n",
       "      <td>2.170070</td>\n",
       "      <td>2.583149</td>\n",
       "      <td>3.898749</td>\n",
       "      <td>10.253577</td>\n",
       "      <td>58.102719</td>\n",
       "      <td>3.740014</td>\n",
       "      <td>10.466718</td>\n",
       "      <td>15.092447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.080000</td>\n",
       "      <td>3.260000</td>\n",
       "      <td>2.920000</td>\n",
       "      <td>2.440000</td>\n",
       "      <td>3.260000</td>\n",
       "      <td>3.260000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1.536667</td>\n",
       "      <td>2.226667</td>\n",
       "      <td>1.603333</td>\n",
       "      <td>4.316667</td>\n",
       "      <td>4.463333</td>\n",
       "      <td>54.993333</td>\n",
       "      <td>26.543333</td>\n",
       "      <td>6.611667</td>\n",
       "      <td>700.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.040000</td>\n",
       "      <td>7.980000</td>\n",
       "      <td>5.300000</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>9.420000</td>\n",
       "      <td>8.390000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>4.440000</td>\n",
       "      <td>5.723333</td>\n",
       "      <td>7.195000</td>\n",
       "      <td>12.606667</td>\n",
       "      <td>16.770000</td>\n",
       "      <td>154.331667</td>\n",
       "      <td>29.672500</td>\n",
       "      <td>19.890000</td>\n",
       "      <td>981.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.260000</td>\n",
       "      <td>9.920000</td>\n",
       "      <td>6.560000</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>12.320000</td>\n",
       "      <td>10.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>7.565400e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>6.783333</td>\n",
       "      <td>7.960000</td>\n",
       "      <td>9.370000</td>\n",
       "      <td>14.343333</td>\n",
       "      <td>22.623333</td>\n",
       "      <td>208.360000</td>\n",
       "      <td>32.566667</td>\n",
       "      <td>23.921667</td>\n",
       "      <td>983.498333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.480000</td>\n",
       "      <td>11.920000</td>\n",
       "      <td>8.570000</td>\n",
       "      <td>7.360000</td>\n",
       "      <td>21.220000</td>\n",
       "      <td>12.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>1.876995e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>8.123333</td>\n",
       "      <td>9.590000</td>\n",
       "      <td>11.553333</td>\n",
       "      <td>16.238333</td>\n",
       "      <td>30.716667</td>\n",
       "      <td>241.365000</td>\n",
       "      <td>35.459167</td>\n",
       "      <td>27.833333</td>\n",
       "      <td>987.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.040000</td>\n",
       "      <td>24.300000</td>\n",
       "      <td>14.220000</td>\n",
       "      <td>13.240000</td>\n",
       "      <td>192.560000</td>\n",
       "      <td>23.260000</td>\n",
       "      <td>723931.000000</td>\n",
       "      <td>4.830000</td>\n",
       "      <td>2545.390000</td>\n",
       "      <td>2.863259e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>17.293333</td>\n",
       "      <td>19.593333</td>\n",
       "      <td>20.923333</td>\n",
       "      <td>37.983333</td>\n",
       "      <td>119.296667</td>\n",
       "      <td>396.230000</td>\n",
       "      <td>41.660000</td>\n",
       "      <td>47.730000</td>\n",
       "      <td>989.130000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       alveolic_grimm   pm10_grimm  pm2_5_grimm    pm1_grimm  inhalable_grimm  \\\n",
       "count     2063.000000  2063.000000  2063.000000  2063.000000      2063.000000   \n",
       "mean         7.626583    10.021008     6.923800     5.866670        18.277838   \n",
       "std          2.123536     2.755813     2.063206     1.780787        15.991072   \n",
       "min          3.080000     3.260000     2.920000     2.440000         3.260000   \n",
       "25%          6.040000     7.980000     5.300000     4.200000         9.420000   \n",
       "50%          7.260000     9.920000     6.560000     5.600000        12.320000   \n",
       "75%          9.480000    11.920000     8.570000     7.360000        21.220000   \n",
       "max         15.040000    24.300000    14.220000    13.240000       192.560000   \n",
       "\n",
       "       thoracic_grimm    P1_lpo_loRa  P1_ratio_loRa  P1_conc_loRa  \\\n",
       "count     2063.000000    2063.000000    2063.000000   2063.000000   \n",
       "mean        10.487571   26775.528357       0.178507     93.041299   \n",
       "std          2.924185   63234.738633       0.421589    218.421909   \n",
       "min          3.260000       0.000000       0.000000      0.620000   \n",
       "25%          8.390000       0.000000       0.000000      0.620000   \n",
       "50%         10.400000       0.000000       0.000000      0.620000   \n",
       "75%         12.300000       0.000000       0.000000      0.620000   \n",
       "max         23.260000  723931.000000       4.830000   2545.390000   \n",
       "\n",
       "        P2_lpo_loRa  ...  Humidity_loRa     pm1Palas   pm2_5Palas  \\\n",
       "count  2.063000e+03  ...    2063.000000  2063.000000  2063.000000   \n",
       "mean   1.317343e+05  ...      46.711100     6.525406     7.786447   \n",
       "std    1.904410e+05  ...      18.411397     1.910236     2.170070   \n",
       "min    0.000000e+00  ...      20.000000     1.536667     2.226667   \n",
       "25%    0.000000e+00  ...      35.000000     4.440000     5.723333   \n",
       "50%    7.565400e+04  ...      42.000000     6.783333     7.960000   \n",
       "75%    1.876995e+05  ...      48.000000     8.123333     9.590000   \n",
       "max    2.863259e+06  ...      85.000000    17.293333    19.593333   \n",
       "\n",
       "          pm4Palas    pm10Palas  pmTotalPalas     dCnPalas  temperaturePalas  \\\n",
       "count  2063.000000  2063.000000   2063.000000  2063.000000       2063.000000   \n",
       "mean      9.242287    14.728641     24.882920   209.331043         32.621074   \n",
       "std       2.583149     3.898749     10.253577    58.102719          3.740014   \n",
       "min       1.603333     4.316667      4.463333    54.993333         26.543333   \n",
       "25%       7.195000    12.606667     16.770000   154.331667         29.672500   \n",
       "50%       9.370000    14.343333     22.623333   208.360000         32.566667   \n",
       "75%      11.553333    16.238333     30.716667   241.365000         35.459167   \n",
       "max      20.923333    37.983333    119.296667   396.230000         41.660000   \n",
       "\n",
       "       humidityPalas  pressurehPalas  \n",
       "count    2063.000000     2063.000000  \n",
       "mean       26.367609      982.382139  \n",
       "std        10.466718       15.092447  \n",
       "min         6.611667      700.000000  \n",
       "25%        19.890000      981.640000  \n",
       "50%        23.921667      983.498333  \n",
       "75%        27.833333      987.890000  \n",
       "max        47.730000      989.130000  \n",
       "\n",
       "[8 rows x 24 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "51e8eef9-7444-437a-ba12-6b73071253e5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dict_col_names = {'pm1Palas':'PM$_{1.0}$ in $\\mu g/m^3 $ (Palas)', \n",
    "                  'pm2_5Palas': 'PM$_{2.5}$ in $\\mu g/m^3$ (Palas)',\n",
    "                  'pm4Palas': 'PM$_{4.0}$ in $\\mu g/m^3$ (Palas)',\n",
    "                  'pm10Palas': 'PM$_{10.0}$ in $\\mu g/m^3$ (Palas)',\n",
    "                  'pmTotalPalas': 'Total PM Concentration in $\\mu g/m^3$ (Palas)', \n",
    "                  'dCnPalas': 'Particle Count Density in #/cm$^{3}$ (Palas)', \n",
    "                  'P1_lpo_loRa': '> 1 μm LPO (LoRa)',\n",
    "                  'P1_ratio_loRa': '> 1 μm Ratio (LoRa)',\n",
    "                  'P1_conc_loRa':'> 1 μm Concentration in $\\mu g/m^3$ (LoRa)' , \n",
    "                  'P2_lpo_loRa': '> 2.5 μm LPO (LoRa)',\n",
    "                  'P2_ratio_loRa': '> 2.5 μm Ratio (LoRa)', \n",
    "                  'P2_conc_loRa': '> 2.5 μm Concentration in $\\mu g/m^3$ (LoRa)',\n",
    "                  'Temperature_loRa': 'Temperature in ℃ (LoRa)', \n",
    "                  'Pressure_loRa': 'Pressure in Pa (LoRa)',\n",
    "                  'Humidity_loRa': 'Humidity in % (LoRa)'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a137dde7-0cc3-4be6-9069-eef1652f5c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def hist_plots(df):\n",
    "#     for col in df.columns[1:len(df.columns)]:\n",
    "#         sns.histplot(df[col], kde = True, color = 'blue', bins = 30)\n",
    "#         plt.title(f'{dict_col_names[col]} Histogram with KDE')\n",
    "#         plt.xlabel(col)\n",
    "#         plt.ylabel('Count')\n",
    "#         plt.show()\n",
    "# hist_plots(df)\n",
    "# def box_plots(df):\n",
    "#     for col in df.columns[1:len(df.columns)]:\n",
    "#         plt.boxplot(df[col])\n",
    "#         plt.title(f'{dict_col_names[col]} BoxPlot')\n",
    "#         plt.xlabel(col)\n",
    "#         plt.ylabel('Count')\n",
    "#         plt.show()\n",
    "# box_plots(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0fc62238-8bed-4b03-8be8-c8f702b382b5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P1_lpo_loRa</th>\n",
       "      <th>P1_ratio_loRa</th>\n",
       "      <th>P1_conc_loRa</th>\n",
       "      <th>P2_lpo_loRa</th>\n",
       "      <th>P2_ratio_loRa</th>\n",
       "      <th>P2_conc_loRa</th>\n",
       "      <th>Temperature_loRa</th>\n",
       "      <th>Pressure_loRa</th>\n",
       "      <th>Humidity_loRa</th>\n",
       "      <th>pm1Palas</th>\n",
       "      <th>pm2_5Palas</th>\n",
       "      <th>pm4Palas</th>\n",
       "      <th>pm10Palas</th>\n",
       "      <th>pmTotalPalas</th>\n",
       "      <th>dCnPalas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2556</td>\n",
       "      <td>0.02</td>\n",
       "      <td>9.48</td>\n",
       "      <td>151843</td>\n",
       "      <td>1.01</td>\n",
       "      <td>524.26</td>\n",
       "      <td>20.53</td>\n",
       "      <td>98866</td>\n",
       "      <td>67</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>9.210000</td>\n",
       "      <td>11.390000</td>\n",
       "      <td>13.516667</td>\n",
       "      <td>15.256667</td>\n",
       "      <td>191.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>67639</td>\n",
       "      <td>0.45</td>\n",
       "      <td>234.43</td>\n",
       "      <td>20.38</td>\n",
       "      <td>98866</td>\n",
       "      <td>68</td>\n",
       "      <td>7.493333</td>\n",
       "      <td>9.210000</td>\n",
       "      <td>11.386667</td>\n",
       "      <td>13.506667</td>\n",
       "      <td>15.246667</td>\n",
       "      <td>191.786667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>86608</td>\n",
       "      <td>0.58</td>\n",
       "      <td>299.81</td>\n",
       "      <td>332245</td>\n",
       "      <td>2.21</td>\n",
       "      <td>1145.71</td>\n",
       "      <td>20.31</td>\n",
       "      <td>98869</td>\n",
       "      <td>68</td>\n",
       "      <td>7.490000</td>\n",
       "      <td>9.206667</td>\n",
       "      <td>11.380000</td>\n",
       "      <td>13.493333</td>\n",
       "      <td>15.226667</td>\n",
       "      <td>191.736667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>30289</td>\n",
       "      <td>0.20</td>\n",
       "      <td>105.48</td>\n",
       "      <td>20.27</td>\n",
       "      <td>98868</td>\n",
       "      <td>68</td>\n",
       "      <td>7.490000</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>11.370000</td>\n",
       "      <td>13.480000</td>\n",
       "      <td>15.206667</td>\n",
       "      <td>191.703333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>153937</td>\n",
       "      <td>1.03</td>\n",
       "      <td>531.46</td>\n",
       "      <td>285021</td>\n",
       "      <td>1.90</td>\n",
       "      <td>982.52</td>\n",
       "      <td>20.20</td>\n",
       "      <td>98868</td>\n",
       "      <td>68</td>\n",
       "      <td>7.490000</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>13.466667</td>\n",
       "      <td>15.193333</td>\n",
       "      <td>191.676667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2058</th>\n",
       "      <td>73639</td>\n",
       "      <td>0.49</td>\n",
       "      <td>255.12</td>\n",
       "      <td>162046</td>\n",
       "      <td>1.08</td>\n",
       "      <td>559.33</td>\n",
       "      <td>20.69</td>\n",
       "      <td>98121</td>\n",
       "      <td>52</td>\n",
       "      <td>5.913333</td>\n",
       "      <td>6.883333</td>\n",
       "      <td>7.923333</td>\n",
       "      <td>12.833333</td>\n",
       "      <td>20.376667</td>\n",
       "      <td>204.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2059</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>7320</td>\n",
       "      <td>0.05</td>\n",
       "      <td>25.99</td>\n",
       "      <td>20.69</td>\n",
       "      <td>98125</td>\n",
       "      <td>52</td>\n",
       "      <td>5.923333</td>\n",
       "      <td>6.853333</td>\n",
       "      <td>7.813333</td>\n",
       "      <td>12.610000</td>\n",
       "      <td>20.093333</td>\n",
       "      <td>205.363333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2060</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>20.70</td>\n",
       "      <td>98123</td>\n",
       "      <td>52</td>\n",
       "      <td>5.986667</td>\n",
       "      <td>6.866667</td>\n",
       "      <td>7.680000</td>\n",
       "      <td>12.440000</td>\n",
       "      <td>19.880000</td>\n",
       "      <td>208.336667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2061</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>15928</td>\n",
       "      <td>0.11</td>\n",
       "      <td>55.80</td>\n",
       "      <td>20.69</td>\n",
       "      <td>98122</td>\n",
       "      <td>52</td>\n",
       "      <td>6.023333</td>\n",
       "      <td>6.903333</td>\n",
       "      <td>7.716667</td>\n",
       "      <td>12.476667</td>\n",
       "      <td>19.916667</td>\n",
       "      <td>209.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2062</th>\n",
       "      <td>47132</td>\n",
       "      <td>0.31</td>\n",
       "      <td>163.67</td>\n",
       "      <td>121788</td>\n",
       "      <td>0.81</td>\n",
       "      <td>420.90</td>\n",
       "      <td>20.69</td>\n",
       "      <td>98126</td>\n",
       "      <td>52</td>\n",
       "      <td>6.053333</td>\n",
       "      <td>6.943333</td>\n",
       "      <td>7.760000</td>\n",
       "      <td>12.520000</td>\n",
       "      <td>19.963333</td>\n",
       "      <td>211.190000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2063 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      P1_lpo_loRa  P1_ratio_loRa  P1_conc_loRa  P2_lpo_loRa  P2_ratio_loRa  \\\n",
       "0            2556           0.02          9.48       151843           1.01   \n",
       "1               0           0.00          0.62        67639           0.45   \n",
       "2           86608           0.58        299.81       332245           2.21   \n",
       "3               0           0.00          0.62        30289           0.20   \n",
       "4          153937           1.03        531.46       285021           1.90   \n",
       "...           ...            ...           ...          ...            ...   \n",
       "2058        73639           0.49        255.12       162046           1.08   \n",
       "2059            0           0.00          0.62         7320           0.05   \n",
       "2060            0           0.00          0.62            0           0.00   \n",
       "2061            0           0.00          0.62        15928           0.11   \n",
       "2062        47132           0.31        163.67       121788           0.81   \n",
       "\n",
       "      P2_conc_loRa  Temperature_loRa  Pressure_loRa  Humidity_loRa  pm1Palas  \\\n",
       "0           524.26             20.53          98866             67  7.500000   \n",
       "1           234.43             20.38          98866             68  7.493333   \n",
       "2          1145.71             20.31          98869             68  7.490000   \n",
       "3           105.48             20.27          98868             68  7.490000   \n",
       "4           982.52             20.20          98868             68  7.490000   \n",
       "...            ...               ...            ...            ...       ...   \n",
       "2058        559.33             20.69          98121             52  5.913333   \n",
       "2059         25.99             20.69          98125             52  5.923333   \n",
       "2060          0.62             20.70          98123             52  5.986667   \n",
       "2061         55.80             20.69          98122             52  6.023333   \n",
       "2062        420.90             20.69          98126             52  6.053333   \n",
       "\n",
       "      pm2_5Palas   pm4Palas  pm10Palas  pmTotalPalas    dCnPalas  \n",
       "0       9.210000  11.390000  13.516667     15.256667  191.830000  \n",
       "1       9.210000  11.386667  13.506667     15.246667  191.786667  \n",
       "2       9.206667  11.380000  13.493333     15.226667  191.736667  \n",
       "3       9.200000  11.370000  13.480000     15.206667  191.703333  \n",
       "4       9.200000  11.360000  13.466667     15.193333  191.676667  \n",
       "...          ...        ...        ...           ...         ...  \n",
       "2058    6.883333   7.923333  12.833333     20.376667  204.200000  \n",
       "2059    6.853333   7.813333  12.610000     20.093333  205.363333  \n",
       "2060    6.866667   7.680000  12.440000     19.880000  208.336667  \n",
       "2061    6.903333   7.716667  12.476667     19.916667  209.700000  \n",
       "2062    6.943333   7.760000  12.520000     19.963333  211.190000  \n",
       "\n",
       "[2063 rows x 15 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data = df.iloc[:,7:-3]\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f3d3584-1003-49b4-9181-1522f7e5302e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pm1Palas', 'pm2_5Palas', 'pm4Palas', 'pm10Palas', 'pmTotalPalas', 'dCnPalas']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_name = list(filtered_data)\n",
    "x =[]\n",
    "y_Palas  = []\n",
    "for i in col_name:\n",
    "    if \"_loRa\" in i:\n",
    "        x.append(i)\n",
    "    if \"Palas\" in i:\n",
    "        y_Palas.append(i)\n",
    "Palas = {}\n",
    "for i in y_Palas:\n",
    "    Palas_cols = x + [i]\n",
    "    Palas[i[:-len(\"Palas\")]] = filtered_data[Palas_cols]\n",
    "y_Palas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "902262ad-f3b9-4d30-859a-3d702acb4e9e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_residuals(predict_test,y_test,filtered_data):\n",
    "    idx = X_test.index\n",
    "    data_test = filtered_data.loc[idx]\n",
    "    data_test[\"dateTime\"] = pd.to_datetime(data_test[\"dateTime\"])\n",
    "    residuals = (np.array(y_test) - predict_test)**2\n",
    "    plt.figure()\n",
    "    plt.scatter(data_test[\"dateTime\"], residuals)\n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.xlabel('Date Time')\n",
    "    plt.ylabel(y_test.name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "34e5042d-ff89-407e-930d-8208825cdf98",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dict_col_regression = {'pm1Palas':'PM$_{1.0}$', \n",
    "                       'pm2_5Palas': 'PM$_{2.5}$',\n",
    "                       'pm4Palas': 'PM$_{4.0}$',\n",
    "                       'pm10Palas': 'PM$_{10.0}$',\n",
    "                       'pmTotalPalas': 'Total PM Concentration', \n",
    "                       'dCnPalas': 'Particle Count Density',\n",
    "                       'dateTime': 'Date Time',\n",
    "                       'P1_lpo_loRa':'> 1 μm LPO',\n",
    "                       'P1_ratio_loRa': '> 1 μm ratio', \n",
    "                       'P1_conc_loRa': '> 1 μm Concentration', \n",
    "                       'P2_lpo_loRa': '> 2.5 μm LPO', \n",
    "                       'P2_ratio_loRa': '> 2.5 μm ratio',\n",
    "                       'P2_conc_loRa': '> 2.5 μm Concentration' , \n",
    "                       'Temperature_loRa': 'Temperature', \n",
    "                       'Pressure_loRa': 'Pressure', \n",
    "                       'Humidity_loRa':'Humidity'}\n",
    "unit_regression = {'pm_conc':'($\\mu g/m^3$)',\n",
    "                   'dCn':'(#/cm$^{3}$)'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aef294c8-b29f-4fce-bab8-18a38eb8c41d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Linear_Regression.ipynb\n",
    "%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Decision_Tree_Regression.ipynb\n",
    "%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Ensemble_Bagging_Regression.ipynb\n",
    "%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/LGBM_Regression.ipynb\n",
    "%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Ridge_Regression.ipynb\n",
    "%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/KNN_Regression.ipynb\n",
    "%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Neural_Network_Regression.ipynb\n",
    "%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/XGBoost_Regression.ipynb\n",
    "%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Random_Forest_Regression.ipynb\n",
    "%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Stacking_Regression.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12e50a8-f5c5-4bd6-abd6-16afcbc8b392",
   "metadata": {},
   "source": [
    "## R^2 Score Without Hyperparmaeter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9e6cb88c-dd21-4476-91b7-7a076c003bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P1_lpo_loRa</th>\n",
       "      <th>P1_ratio_loRa</th>\n",
       "      <th>P1_conc_loRa</th>\n",
       "      <th>P2_lpo_loRa</th>\n",
       "      <th>P2_ratio_loRa</th>\n",
       "      <th>P2_conc_loRa</th>\n",
       "      <th>Temperature_loRa</th>\n",
       "      <th>Pressure_loRa</th>\n",
       "      <th>Humidity_loRa</th>\n",
       "      <th>pm10Palas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2556</td>\n",
       "      <td>0.02</td>\n",
       "      <td>9.48</td>\n",
       "      <td>151843</td>\n",
       "      <td>1.01</td>\n",
       "      <td>524.26</td>\n",
       "      <td>20.53</td>\n",
       "      <td>98866</td>\n",
       "      <td>67</td>\n",
       "      <td>13.516667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>67639</td>\n",
       "      <td>0.45</td>\n",
       "      <td>234.43</td>\n",
       "      <td>20.38</td>\n",
       "      <td>98866</td>\n",
       "      <td>68</td>\n",
       "      <td>13.506667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>86608</td>\n",
       "      <td>0.58</td>\n",
       "      <td>299.81</td>\n",
       "      <td>332245</td>\n",
       "      <td>2.21</td>\n",
       "      <td>1145.71</td>\n",
       "      <td>20.31</td>\n",
       "      <td>98869</td>\n",
       "      <td>68</td>\n",
       "      <td>13.493333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>30289</td>\n",
       "      <td>0.20</td>\n",
       "      <td>105.48</td>\n",
       "      <td>20.27</td>\n",
       "      <td>98868</td>\n",
       "      <td>68</td>\n",
       "      <td>13.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>153937</td>\n",
       "      <td>1.03</td>\n",
       "      <td>531.46</td>\n",
       "      <td>285021</td>\n",
       "      <td>1.90</td>\n",
       "      <td>982.52</td>\n",
       "      <td>20.20</td>\n",
       "      <td>98868</td>\n",
       "      <td>68</td>\n",
       "      <td>13.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2058</th>\n",
       "      <td>73639</td>\n",
       "      <td>0.49</td>\n",
       "      <td>255.12</td>\n",
       "      <td>162046</td>\n",
       "      <td>1.08</td>\n",
       "      <td>559.33</td>\n",
       "      <td>20.69</td>\n",
       "      <td>98121</td>\n",
       "      <td>52</td>\n",
       "      <td>12.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2059</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>7320</td>\n",
       "      <td>0.05</td>\n",
       "      <td>25.99</td>\n",
       "      <td>20.69</td>\n",
       "      <td>98125</td>\n",
       "      <td>52</td>\n",
       "      <td>12.610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2060</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>20.70</td>\n",
       "      <td>98123</td>\n",
       "      <td>52</td>\n",
       "      <td>12.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2061</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>15928</td>\n",
       "      <td>0.11</td>\n",
       "      <td>55.80</td>\n",
       "      <td>20.69</td>\n",
       "      <td>98122</td>\n",
       "      <td>52</td>\n",
       "      <td>12.476667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2062</th>\n",
       "      <td>47132</td>\n",
       "      <td>0.31</td>\n",
       "      <td>163.67</td>\n",
       "      <td>121788</td>\n",
       "      <td>0.81</td>\n",
       "      <td>420.90</td>\n",
       "      <td>20.69</td>\n",
       "      <td>98126</td>\n",
       "      <td>52</td>\n",
       "      <td>12.520000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2063 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      P1_lpo_loRa  P1_ratio_loRa  P1_conc_loRa  P2_lpo_loRa  P2_ratio_loRa  \\\n",
       "0            2556           0.02          9.48       151843           1.01   \n",
       "1               0           0.00          0.62        67639           0.45   \n",
       "2           86608           0.58        299.81       332245           2.21   \n",
       "3               0           0.00          0.62        30289           0.20   \n",
       "4          153937           1.03        531.46       285021           1.90   \n",
       "...           ...            ...           ...          ...            ...   \n",
       "2058        73639           0.49        255.12       162046           1.08   \n",
       "2059            0           0.00          0.62         7320           0.05   \n",
       "2060            0           0.00          0.62            0           0.00   \n",
       "2061            0           0.00          0.62        15928           0.11   \n",
       "2062        47132           0.31        163.67       121788           0.81   \n",
       "\n",
       "      P2_conc_loRa  Temperature_loRa  Pressure_loRa  Humidity_loRa  pm10Palas  \n",
       "0           524.26             20.53          98866             67  13.516667  \n",
       "1           234.43             20.38          98866             68  13.506667  \n",
       "2          1145.71             20.31          98869             68  13.493333  \n",
       "3           105.48             20.27          98868             68  13.480000  \n",
       "4           982.52             20.20          98868             68  13.466667  \n",
       "...            ...               ...            ...            ...        ...  \n",
       "2058        559.33             20.69          98121             52  12.833333  \n",
       "2059         25.99             20.69          98125             52  12.610000  \n",
       "2060          0.62             20.70          98123             52  12.440000  \n",
       "2061         55.80             20.69          98122             52  12.476667  \n",
       "2062        420.90             20.69          98126             52  12.520000  \n",
       "\n",
       "[2063 rows x 10 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Palas[\"pm10\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "08e327c4-0d14-4d01-8c67-973a1d28d84f",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 value of train data 0.48\n",
      "R2 value of test data 0.46\n",
      "r2 train 0.48\n",
      "r2 test 0.46\n",
      "r2 train 0.97\n",
      "r2 test 0.86\n",
      "R2 value of train data 1.0\n",
      "R2 value of test data 0.97\n",
      "R2 value of train data: 1.0\n",
      "R2 value of test data: 0.97\n",
      "r2 train 1.0\n",
      "r2 test 0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balag\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 train 0.95\n",
      "r2 test 0.88\n",
      "R2 value of train data 1.0\n",
      "R2 value of test data 0.97\n",
      "R2 value of train data 0.51\n",
      "R2 value of test data 0.5\n",
      "r2 train 0.51\n",
      "r2 test 0.5\n",
      "r2 train 0.96\n",
      "r2 test 0.86\n",
      "R2 value of train data 1.0\n",
      "R2 value of test data 0.98\n",
      "R2 value of train data: 1.0\n",
      "R2 value of test data: 0.98\n",
      "r2 train 1.0\n",
      "r2 test 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balag\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 train 0.94\n",
      "r2 test 0.88\n",
      "R2 value of train data 1.0\n",
      "R2 value of test data 0.98\n",
      "R2 value of train data 0.56\n",
      "R2 value of test data 0.55\n",
      "r2 train 0.56\n",
      "r2 test 0.55\n",
      "r2 train 0.96\n",
      "r2 test 0.87\n",
      "R2 value of train data 1.0\n",
      "R2 value of test data 0.96\n",
      "R2 value of train data: 1.0\n",
      "R2 value of test data: 0.96\n",
      "r2 train 1.0\n",
      "r2 test 0.97\n",
      "r2 train 0.93\n",
      "r2 test 0.88\n",
      "R2 value of train data 1.0\n",
      "R2 value of test data 0.97\n",
      "R2 value of train data 0.16\n",
      "R2 value of test data 0.17\n",
      "r2 train 0.15\n",
      "r2 test 0.17\n",
      "r2 train 0.82\n",
      "r2 test 0.59\n",
      "R2 value of train data 1.0\n",
      "R2 value of test data 0.81\n",
      "R2 value of train data: 0.99\n",
      "R2 value of test data: 0.87\n",
      "r2 train 0.98\n",
      "r2 test 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balag\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 train 0.63\n",
      "r2 test 0.59\n",
      "R2 value of train data 0.99\n",
      "R2 value of test data 0.89\n",
      "R2 value of train data 0.21\n",
      "R2 value of test data 0.21\n",
      "r2 train 0.21\n",
      "r2 test 0.21\n",
      "r2 train 0.73\n",
      "r2 test 0.46\n",
      "R2 value of train data 1.0\n",
      "R2 value of test data 0.67\n",
      "R2 value of train data: 0.99\n",
      "R2 value of test data: 0.71\n",
      "r2 train 0.96\n",
      "r2 test 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balag\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 train 0.45\n",
      "r2 test 0.4\n",
      "R2 value of train data 0.97\n",
      "R2 value of test data 0.8\n",
      "R2 value of train data 0.4\n",
      "R2 value of test data 0.38\n",
      "r2 train 0.4\n",
      "r2 test 0.38\n",
      "r2 train 0.96\n",
      "r2 test 0.9\n",
      "R2 value of train data 1.0\n",
      "R2 value of test data 0.99\n",
      "R2 value of train data: 1.0\n",
      "R2 value of test data: 0.99\n",
      "r2 train 1.0\n",
      "r2 test 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balag\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 train 0.95\n",
      "r2 test 0.92\n",
      "R2 value of train data 1.0\n",
      "R2 value of test data 0.99\n"
     ]
    }
   ],
   "source": [
    "r2_score_test_lr ={}\n",
    "r2_score_test_rr = {}\n",
    "r2_score_test_knn = {}\n",
    "r2_score_test_dt ={}\n",
    "r2_score_test_xgb = {}\n",
    "r2_score_test_lgbm ={}\n",
    "r2_score_test_br ={}\n",
    "r2_score_test_nn ={}\n",
    "r2_score_test_rf ={}\n",
    "# r2_score_test_sl = {}\n",
    "\n",
    "for k,v in enumerate(Palas):\n",
    "    X = Palas[v].drop([v+\"Palas\"],axis = 1)\n",
    "    y = Palas[v][v+\"Palas\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,random_state = 20)\n",
    "    X_train.to_csv(\"X_train.csv\")\n",
    "    X_test.to_csv(\"X_test.csv\")\n",
    "    y_train.to_csv(\"y_train.csv\")\n",
    "    y_test.to_csv(\"y_test.csv\")\n",
    "\n",
    "    r2_score_test_lr[v] =  Linear_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "    r2_score_test_rr[v] = Ridge_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "    r2_score_test_knn[v] = KNN_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "    r2_score_test_dt[v] = Decision_Tree_Regression(X_train,X_test,y_train,y_test,filtered_data)   \n",
    "    r2_score_test_xgb[v] = XGBoost_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "    r2_score_test_lgbm[v]  =  LGBM_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "    r2_score_test_br[v] = Ensemble_Bagging_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "    r2_score_test_nn[v] = Neural_Network_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "    r2_score_test_rf[v] = Random_Forest_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e695a7-4c42-41a2-8970-f4d939a78789",
   "metadata": {},
   "source": [
    "## R^2 Score Without Hyperparmaeter Tuning for all Models (Pre Super Learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "37ad6be6-bbb1-4163-ab3e-8000887c2e27",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Model   pm1  pm2_5   pm4  pm10  pmTotal   dCn\n",
      "5  Ensemble_Bagging_Regression  0.96   0.97  0.97  0.90     0.80  0.99\n",
      "1     Random_Forest_Regression  0.97   0.98  0.97  0.89     0.80  0.99\n",
      "2           XGBoost_Regression  0.97   0.98  0.96  0.87     0.71  0.99\n",
      "3     Decision_Tree_Regression  0.97   0.98  0.96  0.81     0.67  0.99\n",
      "4              LGBM_Regression  0.95   0.95  0.96  0.90     0.80  0.98\n",
      "6    Neural_Network_Regression  0.88   0.88  0.88  0.59     0.40  0.92\n",
      "8               KNN_Regression  0.86   0.86  0.87  0.59     0.46  0.90\n",
      "0            Linear_Regression  0.46   0.50  0.55  0.17     0.21  0.38\n",
      "7             Ridge_Regression  0.46   0.50  0.55  0.17     0.21  0.38\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "models = [\n",
    "    \"Linear_Regression\",\n",
    "    \"Random_Forest_Regression\",\n",
    "    \"XGBoost_Regression\",\n",
    "    \"Decision_Tree_Regression\",\n",
    "    \"LGBM_Regression\",\n",
    "    \"Ensemble_Bagging_Regression\",\n",
    "    \"Neural_Network_Regression\",\n",
    "    \"Ridge_Regression\",\n",
    "    \"KNN_Regression\"\n",
    "]\n",
    "\n",
    "v_values = [\"pm1\", \"pm2_5\", \"pm4\", \"pm10\", \"pmTotal\", \"dCn\"]\n",
    "\n",
    "# Initialize a DataFrame to store the R^2 scores\n",
    "data = {\n",
    "    \"Model\": models\n",
    "}\n",
    "\n",
    "for v in v_values:\n",
    "    data[v] = [\n",
    "        r2_score_test_lr[v],\n",
    "        r2_score_test_rf[v],\n",
    "        r2_score_test_xgb[v],\n",
    "        r2_score_test_dt[v],\n",
    "        r2_score_test_lgbm[v],\n",
    "        r2_score_test_br[v],\n",
    "        r2_score_test_nn[v],\n",
    "        r2_score_test_rr[v],\n",
    "        r2_score_test_knn[v]\n",
    "    ]\n",
    "\n",
    "# Create the DataFrame\n",
    "r2_score_table = pd.DataFrame(data)\n",
    "\n",
    "# Sort the table by decreasing order of R^2 scores for each column in v_values\n",
    "for v in v_values:\n",
    "    r2_score_table = r2_score_table.sort_values(by=v, ascending=False)\n",
    "\n",
    "# Save the sorted table to a CSV file\n",
    "r2_score_table.to_csv(\"sorted_r2_score_table.csv\", index=False)\n",
    "\n",
    "# Display the sorted table\n",
    "print(r2_score_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35fca5f-b27c-4df7-9356-f1a6c5eecd3c",
   "metadata": {},
   "source": [
    "## Training Super Learner without Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a507e5b-f37c-4658-aec9-6f3c101baf6b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 train 0.99\n",
      "r2 test 0.99\n",
      "r2 train 1.0\n",
      "r2 test 0.99\n",
      "r2 train 1.0\n",
      "r2 test 0.98\n",
      "r2 train 0.98\n",
      "r2 test 0.9\n",
      "r2 train 0.94\n",
      "r2 test 0.84\n",
      "r2 train 1.0\n",
      "r2 test 0.99\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pm1': 0.99,\n",
       " 'pm2_5': 0.99,\n",
       " 'pm4': 0.98,\n",
       " 'pm10': 0.9,\n",
       " 'pmTotal': 0.84,\n",
       " 'dCn': 0.99}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# r2_score_test_sl = {}\n",
    "\n",
    "# def Stacking_Regression(X_train,X_test,y_train,y_test,filtered_data):\n",
    "#     ml_type = 'SL'\n",
    "#     estimators = [\n",
    "#         ('rf', RandomForestRegressor(n_estimators = 50, random_state = 0)),\n",
    "#         ('br', BaggingRegressor(estimator = RandomForestRegressor(), n_estimators=10, random_state=42)),\n",
    "#         ('lgbm',LGBMRegressor(\n",
    "#         n_estimators=100,\n",
    "#         max_depth=-1,\n",
    "#         learning_rate=0.1,\n",
    "#         num_leaves=31,\n",
    "#         min_child_samples=20,\n",
    "#         random_state=1\n",
    "#         )),\n",
    "#         ('dt', DecisionTreeRegressor(random_state = 0))\n",
    "#     ]\n",
    "    \n",
    "#     model = StackingRegressor(\n",
    "#         estimators = estimators,\n",
    "#         final_estimator = LinearRegression())\n",
    "\n",
    "#     model.fit(X_train, y_train)\n",
    "    \n",
    "#     predict_train = model.predict(X_train)\n",
    "#     predict_test = model.predict(X_test)\n",
    "    \n",
    "#     train_df = pd.DataFrame({'Actual': y_train, 'Predicted': predict_train, 'Category': 'Training'}, index=y_train.index)\n",
    "#     test_df = pd.DataFrame({'Actual': y_test, 'Predicted': predict_test, 'Category': 'Testing'}, index=y_test.index)\n",
    "    \n",
    "#     # Concatenate the DataFrames\n",
    "#     combined_df = pd.concat([train_df, test_df])\n",
    "#     combined_df = combined_df.sort_index()\n",
    "#     # Print or use the combined DataFrame as needed\n",
    "#     # print(combined_df)\n",
    "    \n",
    "#     r2_score_train = round(metrics.r2_score(y_train, predict_train),2) \n",
    "#     r2_score_test = round(metrics.r2_score(y_test, predict_test),2)\n",
    "#     print('r2 train',r2_score_train)    \n",
    "#     print('r2 test',r2_score_test) \n",
    "    \n",
    "#     if (v == 'dCn'):\n",
    "#         unit = 'dCn'\n",
    "#     else:\n",
    "#         unit = 'pm_conc'\n",
    "    \n",
    "#     # Scatter_Plot(combined_df, train_df, test_df, r2_score_train, r2_score_test, v, unit,ml_type)\n",
    "#     # qq_plot(test_df, v, unit,ml_type)\n",
    "#     return r2_score_test\n",
    "\n",
    "# for k,v in enumerate(Palas):\n",
    "#     X = Palas[v].drop([v+\"Palas\"],axis = 1)\n",
    "#     y = Palas[v][v+\"Palas\"]\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,random_state = 42)\n",
    "#     X_train.to_csv(\"X_train.csv\")\n",
    "#     X_test.to_csv(\"X_test.csv\")\n",
    "#     y_train.to_csv(\"y_train.csv\")\n",
    "#     y_test.to_csv(\"y_test.csv\")\n",
    "#     r2_score_test_sl[v] = Stacking_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "# r2_score_test_sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce73e3fc-a1b8-4d37-bb1c-538dc33f8e1b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# models = [\n",
    "#     \"Stacking_Learner\",\n",
    "#     \"Random_Forest_Regression\",\n",
    "#     \"Decision_Tree_Regression\",\n",
    "#     \"Ensemble_Bagging_Regression\",\n",
    "#     \"LGBM_Regression\",\n",
    "#     \"XGBoost_Regression\",\n",
    "#     \"Neural_Network_Regression\",\n",
    "#     \"KNN_Regression\",\n",
    "#     \"Linear_Regression\",\n",
    "#     \"Ridge_Regression\"\n",
    "# ]\n",
    "\n",
    "# v_values = [\"pm1\", \"pm2_5\", \"pm4\", \"pm10\", \"pmTotal\", \"dCn\"]\n",
    "\n",
    "# # Initialize a DataFrame to store the R^2 scores\n",
    "# data = {\n",
    "#     \"Model\": models,\n",
    "#     \"pm1\": [0.99, 0.99, 0.98, 0.99, 0.99, 0.97, 0.96, 0.96, 0.48, 0.48],\n",
    "#     \"pm2_5\": [0.99, 0.97, 0.98, 0.96, 0.96, 0.97, 0.90, 0.85, 0.46, 0.45],\n",
    "#     \"pm4\": [0.99, 0.99, 0.98, 0.99, 0.99, 0.98, 0.93, 0.93, 0.60, 0.61],\n",
    "#     \"pm10\": [0.91, 0.91, 0.86, 0.91, 0.90, 0.89, 0.66, 0.57, 0.20, 0.19],\n",
    "#     \"pmTotal\": [0.86, 0.76, 0.75, 0.72, 0.72, 0.72, 0.44, 0.19, 0.16, 0.16],\n",
    "#     \"dCn\": [0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.97, 0.94, 0.43, 0.43]\n",
    "# }\n",
    "\n",
    "# # Create the DataFrame\n",
    "# r2_score_table = pd.DataFrame(data)\n",
    "\n",
    "# # Save the table to a CSV file\n",
    "# r2_score_table.to_csv(\"sorted_r2_score_table.csv\", index=False)\n",
    "\n",
    "# # Display the table\n",
    "# print(r2_score_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33310784-1ec0-4779-9e80-c4579aa2aaf9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Palas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Prepare data for modeling\u001b[39;00m\n\u001b[0;32m     46\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mPalas\u001b[49m\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(v)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpmTotal\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Palas' is not defined"
     ]
    }
   ],
   "source": [
    "# Suppress all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import itertools\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Define all learners\n",
    "learners = {\n",
    "    'lr': LinearRegression(),\n",
    "    'rr': Ridge(random_state=42),\n",
    "    'knn': KNeighborsRegressor(),\n",
    "    'dt': DecisionTreeRegressor(random_state=42),\n",
    "    'rf': RandomForestRegressor(random_state=42),\n",
    "    'br': BaggingRegressor(random_state=42),\n",
    "    'xgb': XGBRegressor(random_state=42),\n",
    "    'lgbm': LGBMRegressor(random_state=42),\n",
    "    'nn': MLPRegressor(random_state=42, max_iter=1000)  # Default Neural Network with random_state\n",
    "}\n",
    "\n",
    "# Function to evaluate a stacking model\n",
    "def evaluate_stacking(base_learners, meta_learner, X_train, X_test, y_train, y_test):\n",
    "    estimators = [(name, model) for name, model in base_learners.items()]\n",
    "    model = StackingRegressor(estimators=estimators, final_estimator=meta_learner)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions for training and test data\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Compute R2 scores\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    return train_r2, test_r2\n",
    "\n",
    "# Prepare data for modeling\n",
    "results = []\n",
    "\n",
    "for v in Palas.keys():\n",
    "    print(v)\n",
    "    if v == \"pmTotal\":\n",
    "        X = Palas[v].drop([v + \"Palas\"], axis=1)\n",
    "        y = Palas[v][v + \"Palas\"]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "        \n",
    "        # Generate all combinations of 4 base learners\n",
    "        base_combinations = list(itertools.combinations(learners.keys(), 4))\n",
    "        counter = 0\n",
    "        for base_comb in base_combinations:\n",
    "            for meta_learner_name in learners.keys():\n",
    "                # Ensure meta-learner can also be a base learner\n",
    "                base_learners = {name: learners[name] for name in base_comb}\n",
    "                meta_learner = learners[meta_learner_name]\n",
    "\n",
    "                # Evaluate the stacking model\n",
    "                counter += 1\n",
    "                print(\"comb:\", counter)\n",
    "                train_r2, test_r2 = evaluate_stacking(base_learners, meta_learner, X_train, X_test, y_train, y_test)\n",
    "\n",
    "                # Store the result\n",
    "                results.append({\n",
    "                    'Base Learners': ', '.join(base_comb),\n",
    "                    'Meta Learner': meta_learner_name,\n",
    "                    'Train R2 Score': train_r2,\n",
    "                    'Test R2 Score': test_r2,\n",
    "                    'Overfitting': 'Yes' if (train_r2 - test_r2) > 0.1 else 'No'\n",
    "                })\n",
    "\n",
    "# Convert results to a DataFrame and sort by Test R2 score\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.sort_values(by='Test R2 Score', ascending=False, inplace=True)\n",
    "results_df_pm10_fit = results_df\n",
    "results_df_pm10_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fda0c281-0025-4390-9d0e-7e17351a2c7f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pm1\n",
      "pm2_5\n",
      "pm4\n",
      "pm10\n",
      "pmTotal\n",
      "comb: 1\n",
      "comb: 2\n",
      "comb: 3\n",
      "comb: 4\n",
      "comb: 5\n",
      "comb: 6\n",
      "comb: 7\n",
      "comb: 8\n",
      "comb: 9\n",
      "comb: 10\n",
      "comb: 11\n",
      "comb: 12\n",
      "comb: 13\n",
      "comb: 14\n",
      "comb: 15\n",
      "comb: 16\n",
      "comb: 17\n",
      "comb: 18\n",
      "comb: 19\n",
      "comb: 20\n",
      "comb: 21\n",
      "comb: 22\n",
      "comb: 23\n",
      "comb: 24\n",
      "comb: 25\n",
      "comb: 26\n",
      "comb: 27\n",
      "comb: 28\n",
      "comb: 29\n",
      "comb: 30\n",
      "comb: 31\n",
      "comb: 32\n",
      "comb: 33\n",
      "comb: 34\n",
      "comb: 35\n",
      "comb: 36\n",
      "comb: 37\n",
      "comb: 38\n",
      "comb: 39\n",
      "comb: 40\n",
      "comb: 41\n",
      "comb: 42\n",
      "comb: 43\n",
      "comb: 44\n",
      "comb: 45\n",
      "comb: 46\n",
      "comb: 47\n",
      "comb: 48\n",
      "comb: 49\n",
      "comb: 50\n",
      "comb: 51\n",
      "comb: 52\n",
      "comb: 53\n",
      "comb: 54\n",
      "comb: 55\n",
      "comb: 56\n",
      "comb: 57\n",
      "comb: 58\n",
      "comb: 59\n",
      "comb: 60\n",
      "comb: 61\n",
      "comb: 62\n",
      "comb: 63\n",
      "comb: 64\n",
      "comb: 65\n",
      "comb: 66\n",
      "comb: 67\n",
      "comb: 68\n",
      "comb: 69\n",
      "comb: 70\n",
      "comb: 71\n",
      "comb: 72\n",
      "comb: 73\n",
      "comb: 74\n",
      "comb: 75\n",
      "comb: 76\n",
      "comb: 77\n",
      "comb: 78\n",
      "comb: 79\n",
      "comb: 80\n",
      "comb: 81\n",
      "comb: 82\n",
      "comb: 83\n",
      "comb: 84\n",
      "comb: 85\n",
      "comb: 86\n",
      "comb: 87\n",
      "comb: 88\n",
      "comb: 89\n",
      "comb: 90\n",
      "comb: 91\n",
      "comb: 92\n",
      "comb: 93\n",
      "comb: 94\n",
      "comb: 95\n",
      "comb: 96\n",
      "comb: 97\n",
      "comb: 98\n",
      "comb: 99\n",
      "comb: 100\n",
      "comb: 101\n",
      "comb: 102\n",
      "comb: 103\n",
      "comb: 104\n",
      "comb: 105\n",
      "comb: 106\n",
      "comb: 107\n",
      "comb: 108\n",
      "comb: 109\n",
      "comb: 110\n",
      "comb: 111\n",
      "comb: 112\n",
      "comb: 113\n",
      "comb: 114\n",
      "comb: 115\n",
      "comb: 116\n",
      "comb: 117\n",
      "comb: 118\n",
      "comb: 119\n",
      "comb: 120\n",
      "comb: 121\n",
      "comb: 122\n",
      "comb: 123\n",
      "comb: 124\n",
      "comb: 125\n",
      "comb: 126\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ace_tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 72\u001b[0m\n\u001b[0;32m     69\u001b[0m results_df\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR2 Score\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Display the top-performing combinations\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mace_tools\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtools\u001b[39;00m; tools\u001b[38;5;241m.\u001b[39mdisplay_dataframe_to_user(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStacking Model Performance\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataframe\u001b[38;5;241m=\u001b[39mresults_df)\n\u001b[0;32m     74\u001b[0m results_df\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ace_tools'"
     ]
    }
   ],
   "source": [
    "# Suppress all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import itertools\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "\n",
    "# Define all learners\n",
    "learners = {\n",
    "    'lr': LinearRegression(),\n",
    "    'rr': Ridge(random_state=42),\n",
    "    'knn': KNeighborsRegressor(),\n",
    "    'dt': DecisionTreeRegressor(random_state=42),\n",
    "    'rf': RandomForestRegressor(random_state=42),\n",
    "    'br': BaggingRegressor(random_state=42),\n",
    "    'xgb': XGBRegressor(random_state=42),\n",
    "    'lgbm': LGBMRegressor(random_state=42),\n",
    "    'nn': MLPRegressor(random_state=42,max_iter = 1000)  # Default Neural Network with random_state\n",
    "}\n",
    "\n",
    "# Function to evaluate a stacking model\n",
    "def evaluate_stacking(base_learners, meta_learner, X_train, X_test, y_train, y_test):\n",
    "    estimators = [(name, model) for name, model in base_learners.items()]\n",
    "    model = StackingRegressor(estimators=estimators, final_estimator=meta_learner)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return r2_score(y_test, y_pred)\n",
    "\n",
    "# Prepare data for modeling\n",
    "results = []\n",
    "\n",
    "for v in Palas.keys():\n",
    "    print(v)\n",
    "    if v == \"pmTotal\":\n",
    "        X = Palas[v].drop([v + \"Palas\"], axis=1)\n",
    "        y = Palas[v][v + \"Palas\"]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "        \n",
    "         # Generate all combinations of 4 base learners\n",
    "        base_combinations = list(itertools.combinations(learners.keys(), 4))\n",
    "        counter = 0\n",
    "        for base_comb in base_combinations:\n",
    "            counter+=1\n",
    "            print(\"comb:\",counter)\n",
    "            for meta_learner_name in learners.keys():\n",
    "                base_learners = {name: learners[name] for name in base_comb}\n",
    "                meta_learner = learners[meta_learner_name]\n",
    "                \n",
    "                 # Evaluate the stacking model\n",
    "                r2 = evaluate_stacking(base_learners, meta_learner, X_train, X_test, y_train, y_test)\n",
    "                \n",
    "                 # Store the result\n",
    "                results.append({\n",
    "                    'Base Learners': ', '.join(base_comb),\n",
    "                     'Meta Learner': meta_learner_name,\n",
    "                    'R2 Score': r2\n",
    "                 })\n",
    "        \n",
    "        # Convert results to a DataFrame and sort by R2 score\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.sort_values(by='R2 Score', ascending=False, inplace=True)\n",
    "        \n",
    "        # Display the top-performing combinations\n",
    "        # import ace_tools as tools; tools.display_dataframe_to_user(name=\"Stacking Model Performance\", dataframe=results_df)\n",
    "        \n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "293f9aa8-8283-4d83-a7ae-fa5281980043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Base Learners</th>\n",
       "      <th>Meta Learner</th>\n",
       "      <th>R2 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>knn, rf, br, xgb</td>\n",
       "      <td>nn</td>\n",
       "      <td>0.856981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>knn, rf, xgb, lgbm</td>\n",
       "      <td>nn</td>\n",
       "      <td>0.855798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>dt, rf, br, xgb</td>\n",
       "      <td>nn</td>\n",
       "      <td>0.852632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>dt, rf, xgb, lgbm</td>\n",
       "      <td>nn</td>\n",
       "      <td>0.851533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>knn, dt, rf, xgb</td>\n",
       "      <td>nn</td>\n",
       "      <td>0.851390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>lr, knn, br, nn</td>\n",
       "      <td>dt</td>\n",
       "      <td>0.246712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>lr, rr, knn, nn</td>\n",
       "      <td>xgb</td>\n",
       "      <td>0.192804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>lr, rr, knn, nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>0.192396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>lr, rr, knn, nn</td>\n",
       "      <td>knn</td>\n",
       "      <td>0.045884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>lr, rr, knn, nn</td>\n",
       "      <td>dt</td>\n",
       "      <td>-0.556203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1134 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Base Learners Meta Learner  R2 Score\n",
       "917     knn, rf, br, xgb           nn  0.856981\n",
       "944   knn, rf, xgb, lgbm           nn  0.855798\n",
       "1007     dt, rf, br, xgb           nn  0.852632\n",
       "1034   dt, rf, xgb, lgbm           nn  0.851533\n",
       "836     knn, dt, rf, xgb           nn  0.851390\n",
       "...                  ...          ...       ...\n",
       "291      lr, knn, br, nn           dt  0.246712\n",
       "51       lr, rr, knn, nn          xgb  0.192804\n",
       "53       lr, rr, knn, nn           nn  0.192396\n",
       "47       lr, rr, knn, nn          knn  0.045884\n",
       "48       lr, rr, knn, nn           dt -0.556203\n",
       "\n",
       "[1134 rows x 3 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df_pmTotal = results_df\n",
    "results_df_pmTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "83356051-9d8c-4223-b942-c14698fce00c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pm1\n",
      "pm2_5\n",
      "pm4\n",
      "pm10\n",
      "comb: 1\n",
      "comb: 2\n",
      "comb: 3\n",
      "comb: 4\n",
      "comb: 5\n",
      "comb: 6\n",
      "comb: 7\n",
      "comb: 8\n",
      "comb: 9\n",
      "comb: 10\n",
      "comb: 11\n",
      "comb: 12\n",
      "comb: 13\n",
      "comb: 14\n",
      "comb: 15\n",
      "comb: 16\n",
      "comb: 17\n",
      "comb: 18\n",
      "comb: 19\n",
      "comb: 20\n",
      "comb: 21\n",
      "comb: 22\n",
      "comb: 23\n",
      "comb: 24\n",
      "comb: 25\n",
      "comb: 26\n",
      "comb: 27\n",
      "comb: 28\n",
      "comb: 29\n",
      "comb: 30\n",
      "comb: 31\n",
      "comb: 32\n",
      "comb: 33\n",
      "comb: 34\n",
      "comb: 35\n",
      "comb: 36\n",
      "comb: 37\n",
      "comb: 38\n",
      "comb: 39\n",
      "comb: 40\n",
      "comb: 41\n",
      "comb: 42\n",
      "comb: 43\n",
      "comb: 44\n",
      "comb: 45\n",
      "comb: 46\n",
      "comb: 47\n",
      "comb: 48\n",
      "comb: 49\n",
      "comb: 50\n",
      "comb: 51\n",
      "comb: 52\n",
      "comb: 53\n",
      "comb: 54\n",
      "comb: 55\n",
      "comb: 56\n",
      "comb: 57\n",
      "comb: 58\n",
      "comb: 59\n",
      "comb: 60\n",
      "comb: 61\n",
      "comb: 62\n",
      "comb: 63\n",
      "comb: 64\n",
      "comb: 65\n",
      "comb: 66\n",
      "comb: 67\n",
      "comb: 68\n",
      "comb: 69\n",
      "comb: 70\n",
      "comb: 71\n",
      "comb: 72\n",
      "comb: 73\n",
      "comb: 74\n",
      "comb: 75\n",
      "comb: 76\n",
      "comb: 77\n",
      "comb: 78\n",
      "comb: 79\n",
      "comb: 80\n",
      "comb: 81\n",
      "comb: 82\n",
      "comb: 83\n",
      "comb: 84\n",
      "comb: 85\n",
      "comb: 86\n",
      "comb: 87\n",
      "comb: 88\n",
      "comb: 89\n",
      "comb: 90\n",
      "comb: 91\n",
      "comb: 92\n",
      "comb: 93\n",
      "comb: 94\n",
      "comb: 95\n",
      "comb: 96\n",
      "comb: 97\n",
      "comb: 98\n",
      "comb: 99\n",
      "comb: 100\n",
      "comb: 101\n",
      "comb: 102\n",
      "comb: 103\n",
      "comb: 104\n",
      "comb: 105\n",
      "comb: 106\n",
      "comb: 107\n",
      "comb: 108\n",
      "comb: 109\n",
      "comb: 110\n",
      "comb: 111\n",
      "comb: 112\n",
      "comb: 113\n",
      "comb: 114\n",
      "comb: 115\n",
      "comb: 116\n",
      "comb: 117\n",
      "comb: 118\n",
      "comb: 119\n",
      "comb: 120\n",
      "comb: 121\n",
      "comb: 122\n",
      "comb: 123\n",
      "comb: 124\n",
      "comb: 125\n",
      "comb: 126\n",
      "pmTotal\n",
      "dCn\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Base Learners</th>\n",
       "      <th>Meta Learner</th>\n",
       "      <th>Train R2 Score</th>\n",
       "      <th>Test R2 Score</th>\n",
       "      <th>Overfitting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>knn, rf, xgb, lgbm</td>\n",
       "      <td>nn</td>\n",
       "      <td>0.981864</td>\n",
       "      <td>0.905781</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>knn, rf, br, xgb</td>\n",
       "      <td>nn</td>\n",
       "      <td>0.988835</td>\n",
       "      <td>0.903832</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>lr, knn, rf, xgb</td>\n",
       "      <td>nn</td>\n",
       "      <td>0.988893</td>\n",
       "      <td>0.902452</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>knn, br, xgb, lgbm</td>\n",
       "      <td>nn</td>\n",
       "      <td>0.981745</td>\n",
       "      <td>0.902339</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>knn, rf, br, lgbm</td>\n",
       "      <td>nn</td>\n",
       "      <td>0.984255</td>\n",
       "      <td>0.901417</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>lr, rr, knn, nn</td>\n",
       "      <td>knn</td>\n",
       "      <td>0.267983</td>\n",
       "      <td>0.212317</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>lr, rr, knn, nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>0.161875</td>\n",
       "      <td>-0.094316</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>lr, rr, knn, nn</td>\n",
       "      <td>dt</td>\n",
       "      <td>-0.056566</td>\n",
       "      <td>-0.193913</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>lr, knn, xgb, nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>-0.359061</td>\n",
       "      <td>-0.753094</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>lr, rr, xgb, nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>-0.945290</td>\n",
       "      <td>-1.504876</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1134 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Base Learners Meta Learner  Train R2 Score  Test R2 Score  \\\n",
       "944  knn, rf, xgb, lgbm           nn        0.981864       0.905781   \n",
       "917    knn, rf, br, xgb           nn        0.988835       0.903832   \n",
       "251    lr, knn, rf, xgb           nn        0.988893       0.902452   \n",
       "971  knn, br, xgb, lgbm           nn        0.981745       0.902339   \n",
       "926   knn, rf, br, lgbm           nn        0.984255       0.901417   \n",
       "..                  ...          ...             ...            ...   \n",
       "47      lr, rr, knn, nn          knn        0.267983       0.212317   \n",
       "53      lr, rr, knn, nn           nn        0.161875      -0.094316   \n",
       "48      lr, rr, knn, nn           dt       -0.056566      -0.193913   \n",
       "314    lr, knn, xgb, nn           nn       -0.359061      -0.753094   \n",
       "179     lr, rr, xgb, nn           nn       -0.945290      -1.504876   \n",
       "\n",
       "    Overfitting  \n",
       "944         Yes  \n",
       "917         Yes  \n",
       "251         Yes  \n",
       "971         Yes  \n",
       "926         Yes  \n",
       "..          ...  \n",
       "47          Yes  \n",
       "53          Yes  \n",
       "48          Yes  \n",
       "314         Yes  \n",
       "179         Yes  \n",
       "\n",
       "[1134 rows x 5 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Suppress all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import itertools\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Define all learners\n",
    "learners = {\n",
    "    'lr': LinearRegression(),\n",
    "    'rr': Ridge(random_state=42),\n",
    "    'knn': KNeighborsRegressor(),\n",
    "    'dt': DecisionTreeRegressor(random_state=42),\n",
    "    'rf': RandomForestRegressor(random_state=42),\n",
    "    'br': BaggingRegressor(random_state=42),\n",
    "    'xgb': XGBRegressor(random_state=42),\n",
    "    'lgbm': LGBMRegressor(random_state=42),\n",
    "    'nn': MLPRegressor(random_state=42, max_iter=1000)  # Default Neural Network with random_state\n",
    "}\n",
    "\n",
    "# Function to evaluate a stacking model\n",
    "def evaluate_stacking(base_learners, meta_learner, X_train, X_test, y_train, y_test):\n",
    "    estimators = [(name, model) for name, model in base_learners.items()]\n",
    "    model = StackingRegressor(estimators=estimators, final_estimator=meta_learner)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions for training and test data\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Compute R2 scores\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    return train_r2, test_r2\n",
    "\n",
    "# Prepare data for modeling\n",
    "results = []\n",
    "\n",
    "for v in Palas.keys():\n",
    "    print(v)\n",
    "    if v == \"pm1\":\n",
    "        X = Palas[v].drop([v + \"Palas\"], axis=1)\n",
    "        y = Palas[v][v + \"Palas\"]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "        \n",
    "        # Generate all combinations of 4 base learners\n",
    "        base_combinations = list(itertools.combinations(learners.keys(), 4))\n",
    "        counter = 0\n",
    "        for base_comb in base_combinations:\n",
    "            counter += 1\n",
    "            print(\"comb:\", counter)\n",
    "            for meta_learner_name in learners.keys():\n",
    "                base_learners = {name: learners[name] for name in base_comb}\n",
    "                meta_learner = learners[meta_learner_name]\n",
    "\n",
    "                # Evaluate the stacking model\n",
    "                train_r2, test_r2 = evaluate_stacking(base_learners, meta_learner, X_train, X_test, y_train, y_test)\n",
    "\n",
    "                # Store the result\n",
    "                results.append({\n",
    "                    'Base Learners': ', '.join(base_comb),\n",
    "                    'Meta Learner': meta_learner_name,\n",
    "                    'Train R2 Score': train_r2,\n",
    "                    'Test R2 Score': test_r2,\n",
    "                    'Overfitting': 'Yes' if train_r2 > test_r2 else 'No'\n",
    "                })\n",
    "\n",
    "# Convert results to a DataFrame and sort by Test R2 score\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.sort_values(by='Test R2 Score', ascending=False, inplace=True)\n",
    "results_df_pm10_fit = results_df\n",
    "results_df_pm10_fit\n",
    "# # Display the top-performing combinations\n",
    "# import ace_tools as tools\n",
    "# tools.display_dataframe_to_user(name=\"Stacking Model Performance with Train/Test R2\", dataframe=results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41bd0b2-0588-496e-9716-35c6ec8842af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pm1\n",
      "pm2_5\n",
      "pm4\n",
      "pm10\n",
      "pmTotal\n",
      "comb: 1\n",
      "comb: 2\n",
      "comb: 3\n",
      "comb: 4\n",
      "comb: 5\n",
      "comb: 6\n",
      "comb: 7\n",
      "comb: 8\n",
      "comb: 9\n",
      "comb: 10\n",
      "comb: 11\n",
      "comb: 12\n",
      "comb: 13\n",
      "comb: 14\n",
      "comb: 15\n",
      "comb: 16\n",
      "comb: 17\n",
      "comb: 18\n",
      "comb: 19\n",
      "comb: 20\n",
      "comb: 21\n",
      "comb: 22\n",
      "comb: 23\n",
      "comb: 24\n",
      "comb: 25\n",
      "comb: 26\n",
      "comb: 27\n",
      "comb: 28\n",
      "comb: 29\n",
      "comb: 30\n",
      "comb: 31\n",
      "comb: 32\n",
      "comb: 33\n",
      "comb: 34\n",
      "comb: 35\n",
      "comb: 36\n",
      "comb: 37\n",
      "comb: 38\n",
      "comb: 39\n"
     ]
    }
   ],
   "source": [
    "# Suppress all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import itertools\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Define all learners\n",
    "learners = {\n",
    "    'lr': LinearRegression(),\n",
    "    'rr': Ridge(random_state=42),\n",
    "    'knn': KNeighborsRegressor(),\n",
    "    'dt': DecisionTreeRegressor(random_state=42),\n",
    "    'rf': RandomForestRegressor(random_state=42),\n",
    "    'br': BaggingRegressor(random_state=42),\n",
    "    'xgb': XGBRegressor(random_state=42),\n",
    "    'lgbm': LGBMRegressor(random_state=42),\n",
    "    'nn': MLPRegressor(random_state=42, max_iter=1000)  # Default Neural Network with random_state\n",
    "}\n",
    "\n",
    "# Function to evaluate a stacking model\n",
    "def evaluate_stacking(base_learners, meta_learner, X_train, X_test, y_train, y_test):\n",
    "    estimators = [(name, model) for name, model in base_learners.items()]\n",
    "    model = StackingRegressor(estimators=estimators, final_estimator=meta_learner)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions for training and test data\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Compute R2 scores\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    return train_r2, test_r2\n",
    "\n",
    "# Prepare data for modeling\n",
    "results = []\n",
    "\n",
    "for v in Palas.keys():\n",
    "    print(v)\n",
    "    if v == \"pmTotal\":\n",
    "        X = Palas[v].drop([v + \"Palas\"], axis=1)\n",
    "        y = Palas[v][v + \"Palas\"]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "        \n",
    "        # Generate all combinations of 4 base learners\n",
    "        base_combinations = list(itertools.combinations(learners.keys(), 4))\n",
    "        counter = 0\n",
    "        for base_comb in base_combinations:\n",
    "            counter += 1\n",
    "            print(\"comb:\", counter)\n",
    "            for meta_learner_name in learners.keys():\n",
    "                base_learners = {name: learners[name] for name in base_comb}\n",
    "                meta_learner = learners[meta_learner_name]\n",
    "\n",
    "                # Evaluate the stacking model\n",
    "                train_r2, test_r2 = evaluate_stacking(base_learners, meta_learner, X_train, X_test, y_train, y_test)\n",
    "\n",
    "                # Store the result\n",
    "                results.append({\n",
    "                    'Base Learners': ', '.join(base_comb),\n",
    "                    'Meta Learner': meta_learner_name,\n",
    "                    'Train R2 Score': train_r2,\n",
    "                    'Test R2 Score': test_r2,\n",
    "                    'Overfitting': 'Yes' if train_r2 > test_r2 else 'No'\n",
    "                })\n",
    "\n",
    "# Convert results to a DataFrame and sort by Test R2 score\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.sort_values(by='Test R2 Score', ascending=False, inplace=True)\n",
    "results_df_pmTotal_fit = results_df\n",
    "results_df_pmTotal_fit \n",
    "# # Display the top-performing combinations\n",
    "# import ace_tools as tools\n",
    "# tools.display_dataframe_to_user(name=\"Stacking Model Performance with Train/Test R2\", dataframe=results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "63efb57e-8d13-4168-a51a-b13dbe641ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pm1\n",
      "pm2_5\n",
      "pm4\n",
      "pm10\n",
      "comb: 1\n",
      "comb: 2\n",
      "comb: 3\n",
      "comb: 4\n",
      "comb: 5\n",
      "comb: 6\n",
      "comb: 7\n",
      "comb: 8\n",
      "comb: 9\n",
      "comb: 10\n",
      "comb: 11\n",
      "comb: 12\n",
      "comb: 13\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 58\u001b[0m\n\u001b[0;32m     55\u001b[0m meta_learner \u001b[38;5;241m=\u001b[39m learners[meta_learner_name]\n\u001b[0;32m     57\u001b[0m  \u001b[38;5;66;03m# Evaluate the stacking model\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m r2 \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_stacking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_learners\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta_learner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m  \u001b[38;5;66;03m# Store the result\u001b[39;00m\n\u001b[0;32m     61\u001b[0m results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBase Learners\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(base_comb),\n\u001b[0;32m     63\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMeta Learner\u001b[39m\u001b[38;5;124m'\u001b[39m: meta_learner_name,\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR2 Score\u001b[39m\u001b[38;5;124m'\u001b[39m: r2\n\u001b[0;32m     65\u001b[0m  })\n",
      "Cell \u001b[1;32mIn[79], line 33\u001b[0m, in \u001b[0;36mevaluate_stacking\u001b[1;34m(base_learners, meta_learner, X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[0;32m     31\u001b[0m estimators \u001b[38;5;241m=\u001b[39m [(name, model) \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m base_learners\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[0;32m     32\u001b[0m model \u001b[38;5;241m=\u001b[39m StackingRegressor(estimators\u001b[38;5;241m=\u001b[39mestimators, final_estimator\u001b[38;5;241m=\u001b[39mmeta_learner)\n\u001b[1;32m---> 33\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r2_score(y_test, y_pred)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_stacking.py:971\u001b[0m, in \u001b[0;36mStackingRegressor.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    969\u001b[0m _raise_for_unsupported_routing(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n\u001b[0;32m    970\u001b[0m y \u001b[38;5;241m=\u001b[39m column_or_1d(y, warn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 971\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_stacking.py:264\u001b[0m, in \u001b[0;36m_BaseStacking.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(cv, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m cv\u001b[38;5;241m.\u001b[39mrandom_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    262\u001b[0m         cv\u001b[38;5;241m.\u001b[39mrandom_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mRandomState()\n\u001b[1;32m--> 264\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcross_val_predict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m            \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mall_estimators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack_method_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdrop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;66;03m# Only not None or not 'drop' estimators will be used in transform.\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;66;03m# Remove the None from the method as well.\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    282\u001b[0m     meth\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (meth, est) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_, all_estimators)\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    285\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:1282\u001b[0m, in \u001b[0;36mcross_val_predict\u001b[1;34m(estimator, X, y, groups, cv, n_jobs, verbose, fit_params, params, pre_dispatch, method)\u001b[0m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m   1280\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m   1281\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m-> 1282\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_predict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1291\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1292\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplits\u001b[49m\n\u001b[0;32m   1293\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1295\u001b[0m inv_test_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mlen\u001b[39m(test_indices), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m   1296\u001b[0m inv_test_indices[test_indices] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(test_indices))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:1367\u001b[0m, in \u001b[0;36m_fit_and_predict\u001b[1;34m(estimator, X, y, train, test, fit_params, method)\u001b[0m\n\u001b[0;32m   1365\u001b[0m     estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1367\u001b[0m     estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m   1368\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(estimator, method)\n\u001b[0;32m   1369\u001b[0m predictions \u001b[38;5;241m=\u001b[39m func(X_test)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:478\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarm_start \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;66;03m# We draw from the random state to get the random state we\u001b[39;00m\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;66;03m# would have got if we hadn't used a warm_start.\u001b[39;00m\n\u001b[0;32m    476\u001b[0m     random_state\u001b[38;5;241m.\u001b[39mrandint(MAX_INT, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_))\n\u001b[1;32m--> 478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    481\u001b[0m ]\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m    489\u001b[0m trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[0;32m    490\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[0;32m    491\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[0;32m    508\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:479\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarm_start \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;66;03m# We draw from the random state to get the random state we\u001b[39;00m\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;66;03m# would have got if we hadn't used a warm_start.\u001b[39;00m\n\u001b[0;32m    476\u001b[0m     random_state\u001b[38;5;241m.\u001b[39mrandint(MAX_INT, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_))\n\u001b[0;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 479\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_estimator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mappend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    481\u001b[0m ]\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m    489\u001b[0m trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[0;32m    490\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[0;32m    491\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[0;32m    508\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_base.py:149\u001b[0m, in \u001b[0;36mBaseEnsemble._make_estimator\u001b[1;34m(self, append, random_state)\u001b[0m\n\u001b[0;32m    146\u001b[0m estimator\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{p: \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator_params})\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m random_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 149\u001b[0m     \u001b[43m_set_random_states\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m append:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mappend(estimator)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_base.py:79\u001b[0m, in \u001b[0;36m_set_random_states\u001b[1;34m(estimator, random_state)\u001b[0m\n\u001b[0;32m     76\u001b[0m         to_set[key] \u001b[38;5;241m=\u001b[39m random_state\u001b[38;5;241m.\u001b[39mrandint(np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m to_set:\n\u001b[1;32m---> 79\u001b[0m     estimator\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mto_set)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:272\u001b[0m, in \u001b[0;36mBaseEstimator.set_params\u001b[1;34m(self, **params)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m params:\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;66;03m# Simple optimization to gain speed (inspect is slow)\u001b[39;00m\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m--> 272\u001b[0m valid_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m nested_params \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mdict\u001b[39m)  \u001b[38;5;66;03m# grouped by prefix\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:243\u001b[0m, in \u001b[0;36mBaseEstimator.get_params\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03mGet parameters for this estimator.\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;124;03m    Parameter names mapped to their values.\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    242\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m--> 243\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_param_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    244\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, key)\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(value, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_params\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:208\u001b[0m, in \u001b[0;36mBaseEstimator._get_param_names\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[0;32m    206\u001b[0m \u001b[38;5;66;03m# introspect the constructor arguments to find the model parameters\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;66;03m# to represent\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m init_signature \u001b[38;5;241m=\u001b[39m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;66;03m# Consider the constructor parameters excluding 'self'\u001b[39;00m\n\u001b[0;32m    210\u001b[0m parameters \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    211\u001b[0m     p\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m init_signature\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m!=\u001b[39m p\u001b[38;5;241m.\u001b[39mVAR_KEYWORD\n\u001b[0;32m    214\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\inspect.py:3247\u001b[0m, in \u001b[0;36msignature\u001b[1;34m(obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[0;32m   3245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msignature\u001b[39m(obj, \u001b[38;5;241m*\u001b[39m, follow_wrapped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, eval_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   3246\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a signature object for the passed callable.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 3247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_wrapped\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_wrapped\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3248\u001b[0m \u001b[43m                                   \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\inspect.py:2995\u001b[0m, in \u001b[0;36mSignature.from_callable\u001b[1;34m(cls, obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[0;32m   2991\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   2992\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_callable\u001b[39m(\u001b[38;5;28mcls\u001b[39m, obj, \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   2993\u001b[0m                   follow_wrapped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, eval_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   2994\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Constructs Signature for the given callable object.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2995\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_signature_from_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigcls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2996\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mfollow_wrapper_chains\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_wrapped\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2997\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\inspect.py:2456\u001b[0m, in \u001b[0;36m_signature_from_callable\u001b[1;34m(obj, follow_wrapper_chains, skip_bound_arg, globals, locals, eval_str, sigcls)\u001b[0m\n\u001b[0;32m   2451\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m sig\u001b[38;5;241m.\u001b[39mreplace(parameters\u001b[38;5;241m=\u001b[39mnew_params)\n\u001b[0;32m   2453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isfunction(obj) \u001b[38;5;129;01mor\u001b[39;00m _signature_is_functionlike(obj):\n\u001b[0;32m   2454\u001b[0m     \u001b[38;5;66;03m# If it's a pure Python function, or an object that is duck type\u001b[39;00m\n\u001b[0;32m   2455\u001b[0m     \u001b[38;5;66;03m# of a Python function (Cython functions, for instance), then:\u001b[39;00m\n\u001b[1;32m-> 2456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_signature_from_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43msigcls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2457\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mskip_bound_arg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_bound_arg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2458\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _signature_is_builtin(obj):\n\u001b[0;32m   2461\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _signature_from_builtin(sigcls, obj,\n\u001b[0;32m   2462\u001b[0m                                    skip_bound_arg\u001b[38;5;241m=\u001b[39mskip_bound_arg)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\inspect.py:2321\u001b[0m, in \u001b[0;36m_signature_from_function\u001b[1;34m(cls, func, skip_bound_arg, globals, locals, eval_str)\u001b[0m\n\u001b[0;32m   2319\u001b[0m kind \u001b[38;5;241m=\u001b[39m _POSITIONAL_ONLY \u001b[38;5;28;01mif\u001b[39;00m posonly_left \u001b[38;5;28;01melse\u001b[39;00m _POSITIONAL_OR_KEYWORD\n\u001b[0;32m   2320\u001b[0m annotation \u001b[38;5;241m=\u001b[39m annotations\u001b[38;5;241m.\u001b[39mget(name, _empty)\n\u001b[1;32m-> 2321\u001b[0m parameters\u001b[38;5;241m.\u001b[39mappend(\u001b[43mParameter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mannotation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2322\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m posonly_left:\n\u001b[0;32m   2324\u001b[0m     posonly_left \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\inspect.py:2632\u001b[0m, in \u001b[0;36mParameter.__init__\u001b[1;34m(self, name, kind, default, annotation)\u001b[0m\n\u001b[0;32m   2630\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, kind, \u001b[38;5;241m*\u001b[39m, default\u001b[38;5;241m=\u001b[39m_empty, annotation\u001b[38;5;241m=\u001b[39m_empty):\n\u001b[0;32m   2631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kind \u001b[38;5;241m=\u001b[39m \u001b[43m_ParameterKind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkind\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2633\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m   2634\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkind\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m is not a valid Parameter.kind\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\enum.py:385\u001b[0m, in \u001b[0;36mEnumMeta.__call__\u001b[1;34m(cls, value, names, module, qualname, type, start)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03mEither returns an existing member, or creates a new enum class.\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;124;03m`type`, if set, will be mixed in as the first base class.\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# simple value lookup\u001b[39;00m\n\u001b[1;32m--> 385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__new__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;66;03m# otherwise, functional API: we're creating a new Enum type\u001b[39;00m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_create_(\n\u001b[0;32m    388\u001b[0m         value,\n\u001b[0;32m    389\u001b[0m         names,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         start\u001b[38;5;241m=\u001b[39mstart,\n\u001b[0;32m    394\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\enum.py:678\u001b[0m, in \u001b[0;36mEnum.__new__\u001b[1;34m(cls, value)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mEnum\u001b[39;00m(metaclass\u001b[38;5;241m=\u001b[39mEnumMeta):\n\u001b[0;32m    673\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;124;03m    Generic enumeration.\u001b[39;00m\n\u001b[0;32m    675\u001b[0m \n\u001b[0;32m    676\u001b[0m \u001b[38;5;124;03m    Derive from this class to define new enumerations.\u001b[39;00m\n\u001b[0;32m    677\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, value):\n\u001b[0;32m    679\u001b[0m         \u001b[38;5;66;03m# all enum instances are actually created during class construction\u001b[39;00m\n\u001b[0;32m    680\u001b[0m         \u001b[38;5;66;03m# without calling this method; this method is called by the metaclass'\u001b[39;00m\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;66;03m# __call__ (i.e. Color(3) ), and by pickle\u001b[39;00m\n\u001b[0;32m    682\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(value) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mcls\u001b[39m:\n\u001b[0;32m    683\u001b[0m             \u001b[38;5;66;03m# For lookups like Color(Color.RED)\u001b[39;00m\n\u001b[0;32m    684\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Suppress all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import itertools\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "\n",
    "# Define all learners\n",
    "learners = {\n",
    "    'lr': LinearRegression(),\n",
    "    'rr': Ridge(random_state=42),\n",
    "    'knn': KNeighborsRegressor(),\n",
    "    'dt': DecisionTreeRegressor(random_state=42),\n",
    "    'rf': RandomForestRegressor(random_state=42),\n",
    "    'br': BaggingRegressor(random_state=42),\n",
    "    'xgb': XGBRegressor(random_state=42),\n",
    "    'lgbm': LGBMRegressor(random_state=42),\n",
    "    'nn': MLPRegressor(random_state=42,max_iter = 1000)  # Default Neural Network with random_state\n",
    "}\n",
    "\n",
    "# Function to evaluate a stacking model\n",
    "def evaluate_stacking(base_learners, meta_learner, X_train, X_test, y_train, y_test):\n",
    "    estimators = [(name, model) for name, model in base_learners.items()]\n",
    "    model = StackingRegressor(estimators=estimators, final_estimator=meta_learner)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return r2_score(y_test, y_pred)\n",
    "\n",
    "# Prepare data for modeling\n",
    "results = []\n",
    "\n",
    "for v in Palas.keys():\n",
    "    print(v)\n",
    "    if v == \"pm10\":\n",
    "        X = Palas[v].drop([v + \"Palas\"], axis=1)\n",
    "        y = Palas[v][v + \"Palas\"]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "        \n",
    "         # Generate all combinations of 4 base learners\n",
    "        base_combinations = list(itertools.combinations(learners.keys(), 4))\n",
    "        counter = 0\n",
    "        for base_comb in base_combinations:\n",
    "            counter+=1\n",
    "            print(\"comb:\",counter)\n",
    "            for meta_learner_name in learners.keys():\n",
    "                base_learners = {name: learners[name] for name in base_comb}\n",
    "                meta_learner = learners[meta_learner_name]\n",
    "                \n",
    "                 # Evaluate the stacking model\n",
    "                r2 = evaluate_stacking(base_learners, meta_learner, X_train, X_test, y_train, y_test)\n",
    "                \n",
    "                 # Store the result\n",
    "                results.append({\n",
    "                    'Base Learners': ', '.join(base_comb),\n",
    "                     'Meta Learner': meta_learner_name,\n",
    "                    'R2 Score': r2\n",
    "                 })\n",
    "        \n",
    "        # Convert results to a DataFrame and sort by R2 score\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.sort_values(by='R2 Score', ascending=False, inplace=True)\n",
    "        \n",
    "        # Display the top-performing combinations\n",
    "        # import ace_tools as tools; tools.display_dataframe_to_user(name=\"Stacking Model Performance\", dataframe=results_df)\n",
    "        \n",
    "        # results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "87e73ee7-52ae-4cab-b22b-48f8abe94abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "r2 train 1.0\n",
      "r2 test 0.99\n",
      "1\n",
      "r2 train 1.0\n",
      "r2 test 0.96\n",
      "2\n",
      "r2 train 1.0\n",
      "r2 test 0.98\n",
      "3\n",
      "r2 train 0.99\n",
      "r2 test 0.88\n",
      "4\n",
      "r2 train 0.96\n",
      "r2 test 0.85\n",
      "5\n",
      "r2 train 1.0\n",
      "r2 test 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pm1': 0.99,\n",
       " 'pm2_5': 0.96,\n",
       " 'pm4': 0.98,\n",
       " 'pm10': 0.88,\n",
       " 'pmTotal': 0.85,\n",
       " 'dCn': 1.0}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score_test_sl = {}\n",
    "\n",
    "def Stacking_Regression(X_train,X_test,y_train,y_test,filtered_data):\n",
    "    ml_type = 'SL'\n",
    "    estimators = [\n",
    "        ('rf', RandomForestRegressor(random_state = 42)),\n",
    "        ('dt', DecisionTreeRegressor(random_state=42)),\n",
    "        ('br',BaggingRegressor(random_state=42)),\n",
    "        ('lgbm', LGBMRegressor(random_state=42)\n",
    ")\n",
    "    ]\n",
    "    \n",
    "    model = StackingRegressor(\n",
    "        estimators = estimators,\n",
    "        final_estimator = Ridge(random_state=42))\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    predict_train = model.predict(X_train)\n",
    "    predict_test = model.predict(X_test)\n",
    "    \n",
    "    train_df = pd.DataFrame({'Actual': y_train, 'Predicted': predict_train, 'Category': 'Training'}, index=y_train.index)\n",
    "    test_df = pd.DataFrame({'Actual': y_test, 'Predicted': predict_test, 'Category': 'Testing'}, index=y_test.index)\n",
    "    \n",
    "    # Concatenate the DataFrames\n",
    "    combined_df = pd.concat([train_df, test_df])\n",
    "    combined_df = combined_df.sort_index()\n",
    "    # Print or use the combined DataFrame as needed\n",
    "    # print(combined_df)\n",
    "    \n",
    "    r2_score_train = round(metrics.r2_score(y_train, predict_train),2) \n",
    "    r2_score_test = round(metrics.r2_score(y_test, predict_test),2)\n",
    "    print('r2 train',r2_score_train)    \n",
    "    print('r2 test',r2_score_test) \n",
    "    \n",
    "    if (v == 'dCn'):\n",
    "        unit = 'dCn'\n",
    "    else:\n",
    "        unit = 'pm_conc'\n",
    "    \n",
    "    # Scatter_Plot(combined_df, train_df, test_df, r2_score_train, r2_score_test, v, unit,ml_type)\n",
    "    # qq_plot(test_df, v, unit,ml_type)\n",
    "    # Feature_Importance_Random_Forest_Regressor(model, filtered_data, v, unit,ml_type)\n",
    "    return r2_score_test,model\n",
    "\n",
    "for k,v in enumerate(Palas):\n",
    "    print(k)\n",
    "    X = Palas[v].drop([v+\"Palas\"],axis = 1)\n",
    "    y = Palas[v][v+\"Palas\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "    X_train.to_csv(\"X_train.csv\")\n",
    "    X_test.to_csv(\"X_test.csv\")\n",
    "    y_train.to_csv(\"y_train.csv\")\n",
    "    y_test.to_csv(\"y_test.csv\")\n",
    "    r2_score_test_sl[v],model = Stacking_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "r2_score_test_sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7e050ac0-cfb3-4920-8148-2294716bd830",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "r2 train 1.0\n",
      "r2 test 0.99\n",
      "1\n",
      "r2 train 1.0\n",
      "r2 test 0.99\n",
      "2\n",
      "r2 train 1.0\n",
      "r2 test 0.99\n",
      "3\n",
      "r2 train 0.99\n",
      "r2 test 0.92\n",
      "4\n",
      "r2 train 0.97\n",
      "r2 test 0.8\n",
      "5\n",
      "r2 train 1.0\n",
      "r2 test 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pm1': 0.99,\n",
       " 'pm2_5': 0.99,\n",
       " 'pm4': 0.99,\n",
       " 'pm10': 0.92,\n",
       " 'pmTotal': 0.8,\n",
       " 'dCn': 1.0}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score_test_sl = {}\n",
    "\n",
    "def Stacking_Regression(X_train,X_test,y_train,y_test,filtered_data):\n",
    "    ml_type = 'SL'\n",
    "    estimators = [\n",
    "        ('rf', RandomForestRegressor(random_state = 42)),\n",
    "        ('knn', KNeighborsRegressor()),\n",
    "        ('lr',LinearRegression()),\n",
    "        ('nn',     MLPRegressor(random_state=42,max_iter = 1000)\n",
    ")\n",
    "    ]\n",
    "    \n",
    "    model = StackingRegressor(\n",
    "        estimators = estimators,\n",
    "        final_estimator = Ridge(random_state=42))\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    predict_train = model.predict(X_train)\n",
    "    predict_test = model.predict(X_test)\n",
    "    \n",
    "    train_df = pd.DataFrame({'Actual': y_train, 'Predicted': predict_train, 'Category': 'Training'}, index=y_train.index)\n",
    "    test_df = pd.DataFrame({'Actual': y_test, 'Predicted': predict_test, 'Category': 'Testing'}, index=y_test.index)\n",
    "    \n",
    "    # Concatenate the DataFrames\n",
    "    combined_df = pd.concat([train_df, test_df])\n",
    "    combined_df = combined_df.sort_index()\n",
    "    # Print or use the combined DataFrame as needed\n",
    "    # print(combined_df)\n",
    "    \n",
    "    r2_score_train = round(metrics.r2_score(y_train, predict_train),2) \n",
    "    r2_score_test = round(metrics.r2_score(y_test, predict_test),2)\n",
    "    print('r2 train',r2_score_train)    \n",
    "    print('r2 test',r2_score_test) \n",
    "    \n",
    "    if (v == 'dCn'):\n",
    "        unit = 'dCn'\n",
    "    else:\n",
    "        unit = 'pm_conc'\n",
    "    \n",
    "    # Scatter_Plot(combined_df, train_df, test_df, r2_score_train, r2_score_test, v, unit,ml_type)\n",
    "    # qq_plot(test_df, v, unit,ml_type)\n",
    "    # Feature_Importance_Random_Forest_Regressor(model, filtered_data, v, unit,ml_type)\n",
    "    return r2_score_test,model\n",
    "\n",
    "for k,v in enumerate(Palas):\n",
    "    print(k)\n",
    "    X = Palas[v].drop([v+\"Palas\"],axis = 1)\n",
    "    y = Palas[v][v+\"Palas\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "    X_train.to_csv(\"X_train.csv\")\n",
    "    X_test.to_csv(\"X_test.csv\")\n",
    "    y_train.to_csv(\"y_train.csv\")\n",
    "    y_test.to_csv(\"y_test.csv\")\n",
    "    r2_score_test_sl[v],model = Stacking_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "r2_score_test_sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e7f56a08-4bf6-4c79-8ee3-10ba40a8eec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "r2 train 1.0\n",
      "r2 test 0.98\n",
      "1\n",
      "r2 train 1.0\n",
      "r2 test 0.99\n",
      "2\n",
      "r2 train 1.0\n",
      "r2 test 0.97\n",
      "3\n",
      "r2 train 0.98\n",
      "r2 test 0.92\n",
      "4\n",
      "r2 train 0.96\n",
      "r2 test 0.84\n",
      "5\n",
      "r2 train 1.0\n",
      "r2 test 0.99\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pm1': 0.98,\n",
       " 'pm2_5': 0.99,\n",
       " 'pm4': 0.97,\n",
       " 'pm10': 0.92,\n",
       " 'pmTotal': 0.84,\n",
       " 'dCn': 0.99}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score_test_sl = {}\n",
    "\n",
    "def Stacking_Regression(X_train,X_test,y_train,y_test,filtered_data):\n",
    "    ml_type = 'SL'\n",
    "    estimators = [\n",
    "        ('rf', RandomForestRegressor(random_state = 42)),\n",
    "        ('knn', KNeighborsRegressor()),\n",
    "        ('lgbm',LGBMRegressor()),\n",
    "        ('nn',     MLPRegressor(random_state=42,max_iter = 1000)\n",
    ")\n",
    "    ]\n",
    "    \n",
    "    model = StackingRegressor(\n",
    "        estimators = estimators,\n",
    "        final_estimator = Ridge(random_state=42))\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    predict_train = model.predict(X_train)\n",
    "    predict_test = model.predict(X_test)\n",
    "    \n",
    "    train_df = pd.DataFrame({'Actual': y_train, 'Predicted': predict_train, 'Category': 'Training'}, index=y_train.index)\n",
    "    test_df = pd.DataFrame({'Actual': y_test, 'Predicted': predict_test, 'Category': 'Testing'}, index=y_test.index)\n",
    "    \n",
    "    # Concatenate the DataFrames\n",
    "    combined_df = pd.concat([train_df, test_df])\n",
    "    combined_df = combined_df.sort_index()\n",
    "    # Print or use the combined DataFrame as needed\n",
    "    # print(combined_df)\n",
    "    \n",
    "    r2_score_train = round(metrics.r2_score(y_train, predict_train),2) \n",
    "    r2_score_test = round(metrics.r2_score(y_test, predict_test),2)\n",
    "    print('r2 train',r2_score_train)    \n",
    "    print('r2 test',r2_score_test) \n",
    "    \n",
    "    if (v == 'dCn'):\n",
    "        unit = 'dCn'\n",
    "    else:\n",
    "        unit = 'pm_conc'\n",
    "    \n",
    "    # Scatter_Plot(combined_df, train_df, test_df, r2_score_train, r2_score_test, v, unit,ml_type)\n",
    "    # qq_plot(test_df, v, unit,ml_type)\n",
    "    # Feature_Importance_Random_Forest_Regressor(model, filtered_data, v, unit,ml_type)\n",
    "    return r2_score_test,model\n",
    "\n",
    "for k,v in enumerate(Palas):\n",
    "    print(k)\n",
    "    X = Palas[v].drop([v+\"Palas\"],axis = 1)\n",
    "    y = Palas[v][v+\"Palas\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "    X_train.to_csv(\"X_train.csv\")\n",
    "    X_test.to_csv(\"X_test.csv\")\n",
    "    y_train.to_csv(\"y_train.csv\")\n",
    "    y_test.to_csv(\"y_test.csv\")\n",
    "    r2_score_test_sl[v],model = Stacking_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "r2_score_test_sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e0b143-8838-4dc4-b775-055eae56c8d6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "r2_score_test_sl = {}\n",
    "\n",
    "def Stacking_Regression(X_train,X_test,y_train,y_test,filtered_data):\n",
    "    ml_type = 'SL'\n",
    "    estimators = [\n",
    "        ('rf', RandomForestRegressor(random_state = 42)),\n",
    "        ('br', BaggingRegressor(random_state=42)),\n",
    "        ('lgbm',LGBMRegressor(random_state=42)),\n",
    "        ('nn',     MLPRegressor(random_state=42)\n",
    ")\n",
    "    ]\n",
    "    \n",
    "    model = StackingRegressor(\n",
    "        estimators = estimators,\n",
    "        final_estimator = Ridge(random_state=42))\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    predict_train = model.predict(X_train)\n",
    "    predict_test = model.predict(X_test)\n",
    "    \n",
    "    train_df = pd.DataFrame({'Actual': y_train, 'Predicted': predict_train, 'Category': 'Training'}, index=y_train.index)\n",
    "    test_df = pd.DataFrame({'Actual': y_test, 'Predicted': predict_test, 'Category': 'Testing'}, index=y_test.index)\n",
    "    \n",
    "    # Concatenate the DataFrames\n",
    "    combined_df = pd.concat([train_df, test_df])\n",
    "    combined_df = combined_df.sort_index()\n",
    "    # Print or use the combined DataFrame as needed\n",
    "    # print(combined_df)\n",
    "    \n",
    "    r2_score_train = round(metrics.r2_score(y_train, predict_train),2) \n",
    "    r2_score_test = round(metrics.r2_score(y_test, predict_test),2)\n",
    "    print('r2 train',r2_score_train)    \n",
    "    print('r2 test',r2_score_test) \n",
    "    \n",
    "    if (v == 'dCn'):\n",
    "        unit = 'dCn'\n",
    "    else:\n",
    "        unit = 'pm_conc'\n",
    "    \n",
    "    Scatter_Plot(combined_df, train_df, test_df, r2_score_train, r2_score_test, v, unit,ml_type)\n",
    "    qq_plot(test_df, v, unit,ml_type)\n",
    "    # Feature_Importance_Random_Forest_Regressor(model, filtered_data, v, unit,ml_type)\n",
    "    return r2_score_test,model\n",
    "\n",
    "for k,v in enumerate(Palas):\n",
    "    print(k)\n",
    "    X = Palas[v].drop([v+\"Palas\"],axis = 1)\n",
    "    y = Palas[v][v+\"Palas\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "    X_train.to_csv(\"X_train.csv\")\n",
    "    X_test.to_csv(\"X_test.csv\")\n",
    "    y_train.to_csv(\"y_train.csv\")\n",
    "    y_test.to_csv(\"y_test.csv\")\n",
    "    r2_score_test_sl[v],model = Stacking_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "r2_score_test_sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a53aeb1-7fcf-4c60-a5b3-096c0ff7e346",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/LGBM_Regression_Optimized.ipynb\n",
    "%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Ridge_Regression_Optimized.ipynb\n",
    "%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/KNN_Regression_Optimized.ipynb\n",
    "%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Neural_Network_Regression_Optimized.ipynb\n",
    "%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Random_Forest_Regression_Optimized.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bf6907b8-5c12-4223-a3ce-cf6cf60195d4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pmTotal\n",
      "|   iter    |  target   | max_depth | min_sa... | min_sa... | n_esti... |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.9152   \u001b[0m | \u001b[0m21.85    \u001b[0m | \u001b[0m3.852    \u001b[0m | \u001b[0m7.856    \u001b[0m | \u001b[0m199.7    \u001b[0m |\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m0.9695   \u001b[0m | \u001b[95m12.02    \u001b[0m | \u001b[95m1.468    \u001b[0m | \u001b[95m2.465    \u001b[0m | \u001b[95m266.5    \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.9183   \u001b[0m | \u001b[0m32.05    \u001b[0m | \u001b[0m3.124    \u001b[0m | \u001b[0m2.165    \u001b[0m | \u001b[0m292.5    \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.9671   \u001b[0m | \u001b[0m42.46    \u001b[0m | \u001b[0m1.637    \u001b[0m | \u001b[0m3.455    \u001b[0m | \u001b[0m95.85    \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.9338   \u001b[0m | \u001b[0m18.69    \u001b[0m | \u001b[0m2.574    \u001b[0m | \u001b[0m5.456    \u001b[0m | \u001b[0m122.8    \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.9367   \u001b[0m | \u001b[0m11.73    \u001b[0m | \u001b[0m2.153    \u001b[0m | \u001b[0m3.048    \u001b[0m | \u001b[0m267.7    \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.8756   \u001b[0m | \u001b[0m7.215    \u001b[0m | \u001b[0m1.055    \u001b[0m | \u001b[0m6.804    \u001b[0m | \u001b[0m231.0    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.9398   \u001b[0m | \u001b[0m12.58    \u001b[0m | \u001b[0m2.19     \u001b[0m | \u001b[0m3.009    \u001b[0m | \u001b[0m265.8    \u001b[0m |\n",
      "| \u001b[95m9        \u001b[0m | \u001b[95m0.9719   \u001b[0m | \u001b[95m42.33    \u001b[0m | \u001b[95m1.273    \u001b[0m | \u001b[95m2.209    \u001b[0m | \u001b[95m95.64    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.9176   \u001b[0m | \u001b[0m41.28    \u001b[0m | \u001b[0m3.427    \u001b[0m | \u001b[0m2.582    \u001b[0m | \u001b[0m95.82    \u001b[0m |\n",
      "| \u001b[95m11       \u001b[0m | \u001b[95m0.9724   \u001b[0m | \u001b[95m43.56    \u001b[0m | \u001b[95m1.532    \u001b[0m | \u001b[95m2.198    \u001b[0m | \u001b[95m96.35    \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.948    \u001b[0m | \u001b[0m44.74    \u001b[0m | \u001b[0m1.993    \u001b[0m | \u001b[0m5.338    \u001b[0m | \u001b[0m96.87    \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.9681   \u001b[0m | \u001b[0m43.3     \u001b[0m | \u001b[0m1.057    \u001b[0m | \u001b[0m3.524    \u001b[0m | \u001b[0m98.22    \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.9719   \u001b[0m | \u001b[0m43.92    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.822    \u001b[0m | \u001b[0m94.43    \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.9461   \u001b[0m | \u001b[0m9.745    \u001b[0m | \u001b[0m1.187    \u001b[0m | \u001b[0m2.281    \u001b[0m | \u001b[0m265.6    \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m0.9496   \u001b[0m | \u001b[0m42.83    \u001b[0m | \u001b[0m1.015    \u001b[0m | \u001b[0m5.24     \u001b[0m | \u001b[0m94.14    \u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m0.9662   \u001b[0m | \u001b[0m45.03    \u001b[0m | \u001b[0m1.323    \u001b[0m | \u001b[0m3.375    \u001b[0m | \u001b[0m100.5    \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m0.9176   \u001b[0m | \u001b[0m44.37    \u001b[0m | \u001b[0m3.407    \u001b[0m | \u001b[0m2.4      \u001b[0m | \u001b[0m100.5    \u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m0.9368   \u001b[0m | \u001b[0m45.71    \u001b[0m | \u001b[0m1.203    \u001b[0m | \u001b[0m6.057    \u001b[0m | \u001b[0m99.52    \u001b[0m |\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.9721   \u001b[0m | \u001b[0m44.83    \u001b[0m | \u001b[0m1.008    \u001b[0m | \u001b[0m2.707    \u001b[0m | \u001b[0m96.6     \u001b[0m |\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m0.9721   \u001b[0m | \u001b[0m46.18    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.214    \u001b[0m | \u001b[0m98.72    \u001b[0m |\n",
      "| \u001b[0m22       \u001b[0m | \u001b[0m0.9413   \u001b[0m | \u001b[0m48.58    \u001b[0m | \u001b[0m2.202    \u001b[0m | \u001b[0m3.472    \u001b[0m | \u001b[0m97.82    \u001b[0m |\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m0.9419   \u001b[0m | \u001b[0m46.08    \u001b[0m | \u001b[0m2.468    \u001b[0m | \u001b[0m2.147    \u001b[0m | \u001b[0m93.21    \u001b[0m |\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m0.951    \u001b[0m | \u001b[0m39.93    \u001b[0m | \u001b[0m1.018    \u001b[0m | \u001b[0m5.468    \u001b[0m | \u001b[0m97.82    \u001b[0m |\n",
      "| \u001b[0m25       \u001b[0m | \u001b[0m0.9588   \u001b[0m | \u001b[0m45.92    \u001b[0m | \u001b[0m1.283    \u001b[0m | \u001b[0m4.038    \u001b[0m | \u001b[0m103.8    \u001b[0m |\n",
      "| \u001b[0m26       \u001b[0m | \u001b[0m0.9634   \u001b[0m | \u001b[0m49.01    \u001b[0m | \u001b[0m1.753    \u001b[0m | \u001b[0m4.129    \u001b[0m | \u001b[0m104.5    \u001b[0m |\n",
      "| \u001b[95m27       \u001b[0m | \u001b[95m0.975    \u001b[0m | \u001b[95m47.68    \u001b[0m | \u001b[95m1.059    \u001b[0m | \u001b[95m2.032    \u001b[0m | \u001b[95m102.5    \u001b[0m |\n",
      "| \u001b[0m28       \u001b[0m | \u001b[0m0.971    \u001b[0m | \u001b[0m49.71    \u001b[0m | \u001b[0m1.423    \u001b[0m | \u001b[0m2.653    \u001b[0m | \u001b[0m103.5    \u001b[0m |\n",
      "| \u001b[0m29       \u001b[0m | \u001b[0m0.9158   \u001b[0m | \u001b[0m49.56    \u001b[0m | \u001b[0m3.791    \u001b[0m | \u001b[0m2.554    \u001b[0m | \u001b[0m106.4    \u001b[0m |\n",
      "| \u001b[0m30       \u001b[0m | \u001b[0m0.924    \u001b[0m | \u001b[0m49.63    \u001b[0m | \u001b[0m1.122    \u001b[0m | \u001b[0m8.069    \u001b[0m | \u001b[0m104.8    \u001b[0m |\n",
      "| \u001b[0m31       \u001b[0m | \u001b[0m0.9691   \u001b[0m | \u001b[0m48.77    \u001b[0m | \u001b[0m1.783    \u001b[0m | \u001b[0m3.805    \u001b[0m | \u001b[0m102.5    \u001b[0m |\n",
      "| \u001b[0m32       \u001b[0m | \u001b[0m0.9226   \u001b[0m | \u001b[0m36.2     \u001b[0m | \u001b[0m1.297    \u001b[0m | \u001b[0m8.883    \u001b[0m | \u001b[0m100.8    \u001b[0m |\n",
      "| \u001b[0m33       \u001b[0m | \u001b[0m0.9744   \u001b[0m | \u001b[0m48.11    \u001b[0m | \u001b[0m1.422    \u001b[0m | \u001b[0m2.418    \u001b[0m | \u001b[0m100.4    \u001b[0m |\n",
      "| \u001b[0m34       \u001b[0m | \u001b[0m0.9391   \u001b[0m | \u001b[0m49.61    \u001b[0m | \u001b[0m2.65     \u001b[0m | \u001b[0m2.509    \u001b[0m | \u001b[0m52.85    \u001b[0m |\n",
      "| \u001b[0m35       \u001b[0m | \u001b[0m0.9165   \u001b[0m | \u001b[0m46.33    \u001b[0m | \u001b[0m3.39     \u001b[0m | \u001b[0m3.457    \u001b[0m | \u001b[0m149.0    \u001b[0m |\n",
      "| \u001b[0m36       \u001b[0m | \u001b[0m0.8669   \u001b[0m | \u001b[0m7.351    \u001b[0m | \u001b[0m3.026    \u001b[0m | \u001b[0m2.211    \u001b[0m | \u001b[0m66.18    \u001b[0m |\n",
      "| \u001b[0m37       \u001b[0m | \u001b[0m0.9292   \u001b[0m | \u001b[0m49.95    \u001b[0m | \u001b[0m1.027    \u001b[0m | \u001b[0m7.733    \u001b[0m | \u001b[0m242.9    \u001b[0m |\n",
      "| \u001b[0m38       \u001b[0m | \u001b[0m0.7868   \u001b[0m | \u001b[0m5.0      \u001b[0m | \u001b[0m2.723    \u001b[0m | \u001b[0m2.709    \u001b[0m | \u001b[0m162.7    \u001b[0m |\n",
      "| \u001b[0m39       \u001b[0m | \u001b[0m0.9742   \u001b[0m | \u001b[0m18.86    \u001b[0m | \u001b[0m1.111    \u001b[0m | \u001b[0m2.313    \u001b[0m | \u001b[0m270.9    \u001b[0m |\n",
      "| \u001b[0m40       \u001b[0m | \u001b[0m0.9679   \u001b[0m | \u001b[0m24.6     \u001b[0m | \u001b[0m1.281    \u001b[0m | \u001b[0m3.591    \u001b[0m | \u001b[0m273.2    \u001b[0m |\n",
      "| \u001b[0m41       \u001b[0m | \u001b[0m0.9742   \u001b[0m | \u001b[0m23.17    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m266.8    \u001b[0m |\n",
      "| \u001b[0m42       \u001b[0m | \u001b[0m0.9227   \u001b[0m | \u001b[0m25.49    \u001b[0m | \u001b[0m1.586    \u001b[0m | \u001b[0m8.984    \u001b[0m | \u001b[0m267.1    \u001b[0m |\n",
      "| \u001b[0m43       \u001b[0m | \u001b[0m0.9732   \u001b[0m | \u001b[0m19.84    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m277.5    \u001b[0m |\n",
      "| \u001b[0m44       \u001b[0m | \u001b[0m0.9028   \u001b[0m | \u001b[0m21.34    \u001b[0m | \u001b[0m4.0      \u001b[0m | \u001b[0m7.046    \u001b[0m | \u001b[0m278.0    \u001b[0m |\n",
      "| \u001b[0m45       \u001b[0m | \u001b[0m0.9739   \u001b[0m | \u001b[0m29.42    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m269.5    \u001b[0m |\n",
      "| \u001b[0m46       \u001b[0m | \u001b[0m0.9747   \u001b[0m | \u001b[0m34.98    \u001b[0m | \u001b[0m1.663    \u001b[0m | \u001b[0m2.256    \u001b[0m | \u001b[0m274.0    \u001b[0m |\n",
      "| \u001b[0m47       \u001b[0m | \u001b[0m0.9722   \u001b[0m | \u001b[0m36.12    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m268.1    \u001b[0m |\n",
      "| \u001b[0m48       \u001b[0m | \u001b[0m0.9264   \u001b[0m | \u001b[0m38.38    \u001b[0m | \u001b[0m2.447    \u001b[0m | \u001b[0m6.904    \u001b[0m | \u001b[0m272.1    \u001b[0m |\n",
      "| \u001b[0m49       \u001b[0m | \u001b[0m0.94     \u001b[0m | \u001b[0m30.85    \u001b[0m | \u001b[0m2.923    \u001b[0m | \u001b[0m2.947    \u001b[0m | \u001b[0m276.5    \u001b[0m |\n",
      "| \u001b[0m50       \u001b[0m | \u001b[0m0.9734   \u001b[0m | \u001b[0m31.41    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m263.5    \u001b[0m |\n",
      "| \u001b[0m51       \u001b[0m | \u001b[0m0.9188   \u001b[0m | \u001b[0m37.36    \u001b[0m | \u001b[0m3.035    \u001b[0m | \u001b[0m2.921    \u001b[0m | \u001b[0m261.4    \u001b[0m |\n",
      "| \u001b[0m52       \u001b[0m | \u001b[0m0.9183   \u001b[0m | \u001b[0m26.4     \u001b[0m | \u001b[0m3.416    \u001b[0m | \u001b[0m2.507    \u001b[0m | \u001b[0m262.3    \u001b[0m |\n",
      "| \u001b[0m53       \u001b[0m | \u001b[0m0.9181   \u001b[0m | \u001b[0m32.86    \u001b[0m | \u001b[0m3.906    \u001b[0m | \u001b[0m2.849    \u001b[0m | \u001b[0m267.4    \u001b[0m |\n",
      "| \u001b[0m54       \u001b[0m | \u001b[0m0.9169   \u001b[0m | \u001b[0m21.64    \u001b[0m | \u001b[0m3.875    \u001b[0m | \u001b[0m2.375    \u001b[0m | \u001b[0m270.4    \u001b[0m |\n",
      "| \u001b[0m55       \u001b[0m | \u001b[0m0.9747   \u001b[0m | \u001b[0m17.15    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m266.9    \u001b[0m |\n",
      "=========================================================================\n",
      "|   iter    |  target   | n_neig... |\n",
      "-------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.2516   \u001b[0m | \u001b[0m13.11    \u001b[0m |\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m0.1771   \u001b[0m | \u001b[0m28.67    \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.2002   \u001b[0m | \u001b[0m22.76    \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.2095   \u001b[0m | \u001b[0m19.16    \u001b[0m |\n",
      "| \u001b[95m5        \u001b[0m | \u001b[95m0.3186   \u001b[0m | \u001b[95m7.213    \u001b[0m |\n",
      "| \u001b[95m6        \u001b[0m | \u001b[95m0.4923   \u001b[0m | \u001b[95m3.0      \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.669    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.335    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.4201   \u001b[0m | \u001b[0m4.457    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.3002   \u001b[0m | \u001b[0m9.921    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.2251   \u001b[0m | \u001b[0m16.04    \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.1925   \u001b[0m | \u001b[0m25.63    \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.522    \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.135    \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.603    \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.235    \u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.07     \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.425    \u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.597    \u001b[0m |\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.0      \u001b[0m |\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.271    \u001b[0m |\n",
      "| \u001b[0m22       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.715    \u001b[0m |\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.656    \u001b[0m |\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.391    \u001b[0m |\n",
      "| \u001b[0m25       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.171    \u001b[0m |\n",
      "| \u001b[0m26       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.07     \u001b[0m |\n",
      "| \u001b[0m27       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.477    \u001b[0m |\n",
      "| \u001b[0m28       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.0      \u001b[0m |\n",
      "| \u001b[0m29       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.657    \u001b[0m |\n",
      "| \u001b[0m30       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.323    \u001b[0m |\n",
      "| \u001b[0m31       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.737    \u001b[0m |\n",
      "| \u001b[0m32       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.689    \u001b[0m |\n",
      "| \u001b[0m33       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.201    \u001b[0m |\n",
      "| \u001b[0m34       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.069    \u001b[0m |\n",
      "| \u001b[0m35       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.46     \u001b[0m |\n",
      "| \u001b[0m36       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.378    \u001b[0m |\n",
      "| \u001b[0m37       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.0      \u001b[0m |\n",
      "| \u001b[0m38       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.687    \u001b[0m |\n",
      "| \u001b[0m39       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.533    \u001b[0m |\n",
      "| \u001b[0m40       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.272    \u001b[0m |\n",
      "| \u001b[0m41       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.141    \u001b[0m |\n",
      "| \u001b[0m42       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.058    \u001b[0m |\n",
      "| \u001b[0m43       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.685    \u001b[0m |\n",
      "| \u001b[0m44       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.749    \u001b[0m |\n",
      "| \u001b[0m45       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.447    \u001b[0m |\n",
      "| \u001b[0m46       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.21     \u001b[0m |\n",
      "| \u001b[0m47       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.71     \u001b[0m |\n",
      "| \u001b[0m48       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.537    \u001b[0m |\n",
      "| \u001b[0m49       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.331    \u001b[0m |\n",
      "\u001b[91mData point [3.] is not unique. 1 duplicates registered. Continuing ...\u001b[0m\n",
      "| \u001b[0m50       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.0      \u001b[0m |\n",
      "| \u001b[0m51       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.13     \u001b[0m |\n",
      "| \u001b[0m52       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.403    \u001b[0m |\n",
      "| \u001b[0m53       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.706    \u001b[0m |\n",
      "| \u001b[0m54       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.269    \u001b[0m |\n",
      "| \u001b[0m55       \u001b[0m | \u001b[0m0.4923   \u001b[0m | \u001b[0m3.051    \u001b[0m |\n",
      "=====================================\n",
      "|   iter    |  target   | learni... | max_depth | n_esti... | num_le... |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.9499   \u001b[0m | \u001b[0m0.1186   \u001b[0m | \u001b[0m14.41    \u001b[0m | \u001b[0m159.8    \u001b[0m | \u001b[0m67.89    \u001b[0m |\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m0.7571   \u001b[0m | \u001b[0m0.05525  \u001b[0m | \u001b[0m4.872    \u001b[0m | \u001b[0m58.71    \u001b[0m | \u001b[0m89.29    \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.919    \u001b[0m | \u001b[0m0.1843   \u001b[0m | \u001b[0m11.5     \u001b[0m | \u001b[0m53.09    \u001b[0m | \u001b[0m97.59    \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.8993   \u001b[0m | \u001b[0m0.2514   \u001b[0m | \u001b[0m5.548    \u001b[0m | \u001b[0m77.27    \u001b[0m | \u001b[0m34.67    \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.9149   \u001b[0m | \u001b[0m0.09823  \u001b[0m | \u001b[0m9.297    \u001b[0m | \u001b[0m114.8    \u001b[0m | \u001b[0m43.3     \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.9279   \u001b[0m | \u001b[0m0.1287   \u001b[0m | \u001b[0m9.625    \u001b[0m | \u001b[0m115.4    \u001b[0m | \u001b[0m43.56    \u001b[0m |\n",
      "| \u001b[95m7        \u001b[0m | \u001b[95m0.9686   \u001b[0m | \u001b[95m0.3      \u001b[0m | \u001b[95m12.54    \u001b[0m | \u001b[95m121.0    \u001b[0m | \u001b[95m45.84    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.9307   \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m5.543    \u001b[0m | \u001b[0m122.7    \u001b[0m | \u001b[0m49.13    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.8848   \u001b[0m | \u001b[0m0.04357  \u001b[0m | \u001b[0m10.69    \u001b[0m | \u001b[0m123.5    \u001b[0m | \u001b[0m38.06    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.93     \u001b[0m | \u001b[0m0.09886  \u001b[0m | \u001b[0m14.84    \u001b[0m | \u001b[0m117.6    \u001b[0m | \u001b[0m51.53    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.9064   \u001b[0m | \u001b[0m0.06692  \u001b[0m | \u001b[0m8.978    \u001b[0m | \u001b[0m154.9    \u001b[0m | \u001b[0m64.0     \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.9681   \u001b[0m | \u001b[0m0.1854   \u001b[0m | \u001b[0m15.0     \u001b[0m | \u001b[0m165.7    \u001b[0m | \u001b[0m72.86    \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.8629   \u001b[0m | \u001b[0m0.02054  \u001b[0m | \u001b[0m14.43    \u001b[0m | \u001b[0m169.9    \u001b[0m | \u001b[0m66.58    \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.9599   \u001b[0m | \u001b[0m0.1586   \u001b[0m | \u001b[0m14.0     \u001b[0m | \u001b[0m159.7    \u001b[0m | \u001b[0m74.63    \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.9586   \u001b[0m | \u001b[0m0.1444   \u001b[0m | \u001b[0m14.52    \u001b[0m | \u001b[0m165.4    \u001b[0m | \u001b[0m80.35    \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m0.8493   \u001b[0m | \u001b[0m0.0199   \u001b[0m | \u001b[0m8.393    \u001b[0m | \u001b[0m164.1    \u001b[0m | \u001b[0m76.34    \u001b[0m |\n",
      "| \u001b[95m17       \u001b[0m | \u001b[95m0.98     \u001b[0m | \u001b[95m0.3      \u001b[0m | \u001b[95m15.0     \u001b[0m | \u001b[95m159.5    \u001b[0m | \u001b[95m80.8     \u001b[0m |\n",
      "| \u001b[95m18       \u001b[0m | \u001b[95m0.9802   \u001b[0m | \u001b[95m0.3      \u001b[0m | \u001b[95m15.0     \u001b[0m | \u001b[95m161.3    \u001b[0m | \u001b[95m86.37    \u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m0.9735   \u001b[0m | \u001b[0m0.2717   \u001b[0m | \u001b[0m13.51    \u001b[0m | \u001b[0m155.4    \u001b[0m | \u001b[0m86.82    \u001b[0m |\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.9787   \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m15.0     \u001b[0m | \u001b[0m151.3    \u001b[0m | \u001b[0m79.75    \u001b[0m |\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m0.9781   \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m15.0     \u001b[0m | \u001b[0m145.8    \u001b[0m | \u001b[0m86.19    \u001b[0m |\n",
      "| \u001b[0m22       \u001b[0m | \u001b[0m0.9558   \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m7.398    \u001b[0m | \u001b[0m146.6    \u001b[0m | \u001b[0m83.06    \u001b[0m |\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m0.9776   \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m15.0     \u001b[0m | \u001b[0m141.6    \u001b[0m | \u001b[0m78.26    \u001b[0m |\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m0.9465   \u001b[0m | \u001b[0m0.1241   \u001b[0m | \u001b[0m15.0     \u001b[0m | \u001b[0m135.8    \u001b[0m | \u001b[0m85.8     \u001b[0m |\n",
      "| \u001b[0m25       \u001b[0m | \u001b[0m0.9598   \u001b[0m | \u001b[0m0.1792   \u001b[0m | \u001b[0m13.78    \u001b[0m | \u001b[0m149.7    \u001b[0m | \u001b[0m95.39    \u001b[0m |\n",
      "| \u001b[0m26       \u001b[0m | \u001b[0m0.9802   \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m15.0     \u001b[0m | \u001b[0m161.5    \u001b[0m | \u001b[0m96.41    \u001b[0m |\n",
      "| \u001b[0m27       \u001b[0m | \u001b[0m0.95     \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m6.105    \u001b[0m | \u001b[0m159.5    \u001b[0m | \u001b[0m96.7     \u001b[0m |\n",
      "| \u001b[0m28       \u001b[0m | \u001b[0m0.9577   \u001b[0m | \u001b[0m0.1436   \u001b[0m | \u001b[0m13.97    \u001b[0m | \u001b[0m171.3    \u001b[0m | \u001b[0m96.39    \u001b[0m |\n",
      "| \u001b[0m29       \u001b[0m | \u001b[0m0.959    \u001b[0m | \u001b[0m0.1649   \u001b[0m | \u001b[0m15.0     \u001b[0m | \u001b[0m136.3    \u001b[0m | \u001b[0m69.63    \u001b[0m |\n",
      "| \u001b[0m30       \u001b[0m | \u001b[0m0.8158   \u001b[0m | \u001b[0m0.04944  \u001b[0m | \u001b[0m4.62     \u001b[0m | \u001b[0m138.3    \u001b[0m | \u001b[0m73.62    \u001b[0m |\n",
      "| \u001b[0m31       \u001b[0m | \u001b[0m0.8838   \u001b[0m | \u001b[0m0.1363   \u001b[0m | \u001b[0m4.077    \u001b[0m | \u001b[0m144.2    \u001b[0m | \u001b[0m95.04    \u001b[0m |\n",
      "| \u001b[0m32       \u001b[0m | \u001b[0m0.9784   \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m15.0     \u001b[0m | \u001b[0m147.4    \u001b[0m | \u001b[0m71.94    \u001b[0m |\n",
      "| \u001b[0m33       \u001b[0m | \u001b[0m0.9136   \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m3.0      \u001b[0m | \u001b[0m172.4    \u001b[0m | \u001b[0m100.0    \u001b[0m |\n",
      "| \u001b[0m34       \u001b[0m | \u001b[0m0.9798   \u001b[0m | \u001b[0m0.2917   \u001b[0m | \u001b[0m14.69    \u001b[0m | \u001b[0m174.7    \u001b[0m | \u001b[0m86.83    \u001b[0m |\n",
      "| \u001b[95m35       \u001b[0m | \u001b[95m0.9824   \u001b[0m | \u001b[95m0.3      \u001b[0m | \u001b[95m15.0     \u001b[0m | \u001b[95m183.5    \u001b[0m | \u001b[95m89.8     \u001b[0m |\n",
      "| \u001b[0m36       \u001b[0m | \u001b[0m0.9824   \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m15.0     \u001b[0m | \u001b[0m183.1    \u001b[0m | \u001b[0m81.22    \u001b[0m |\n",
      "| \u001b[0m37       \u001b[0m | \u001b[0m0.9542   \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m6.835    \u001b[0m | \u001b[0m184.0    \u001b[0m | \u001b[0m85.57    \u001b[0m |\n",
      "| \u001b[0m38       \u001b[0m | \u001b[0m0.9539   \u001b[0m | \u001b[0m0.1084   \u001b[0m | \u001b[0m14.69    \u001b[0m | \u001b[0m193.8    \u001b[0m | \u001b[0m84.02    \u001b[0m |\n",
      "| \u001b[0m39       \u001b[0m | \u001b[0m0.9339   \u001b[0m | \u001b[0m0.06863  \u001b[0m | \u001b[0m13.73    \u001b[0m | \u001b[0m191.5    \u001b[0m | \u001b[0m98.23    \u001b[0m |\n",
      "| \u001b[95m40       \u001b[0m | \u001b[95m0.983    \u001b[0m | \u001b[95m0.3      \u001b[0m | \u001b[95m15.0     \u001b[0m | \u001b[95m190.0    \u001b[0m | \u001b[95m72.88    \u001b[0m |\n",
      "| \u001b[0m41       \u001b[0m | \u001b[0m0.9551   \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m6.155    \u001b[0m | \u001b[0m192.2    \u001b[0m | \u001b[0m73.45    \u001b[0m |\n",
      "| \u001b[0m42       \u001b[0m | \u001b[0m0.8922   \u001b[0m | \u001b[0m0.02803  \u001b[0m | \u001b[0m13.63    \u001b[0m | \u001b[0m196.4    \u001b[0m | \u001b[0m66.62    \u001b[0m |\n",
      "| \u001b[0m43       \u001b[0m | \u001b[0m0.9523   \u001b[0m | \u001b[0m0.1295   \u001b[0m | \u001b[0m11.74    \u001b[0m | \u001b[0m183.3    \u001b[0m | \u001b[0m72.39    \u001b[0m |\n",
      "| \u001b[0m44       \u001b[0m | \u001b[0m0.9772   \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m15.0     \u001b[0m | \u001b[0m138.4    \u001b[0m | \u001b[0m57.96    \u001b[0m |\n",
      "| \u001b[0m45       \u001b[0m | \u001b[0m0.9766   \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m15.0     \u001b[0m | \u001b[0m142.2    \u001b[0m | \u001b[0m49.38    \u001b[0m |\n",
      "| \u001b[0m46       \u001b[0m | \u001b[0m0.842    \u001b[0m | \u001b[0m0.02322  \u001b[0m | \u001b[0m7.053    \u001b[0m | \u001b[0m139.2    \u001b[0m | \u001b[0m52.02    \u001b[0m |\n",
      "| \u001b[0m47       \u001b[0m | \u001b[0m0.9737   \u001b[0m | \u001b[0m0.2612   \u001b[0m | \u001b[0m14.99    \u001b[0m | \u001b[0m151.6    \u001b[0m | \u001b[0m47.44    \u001b[0m |\n",
      "| \u001b[0m48       \u001b[0m | \u001b[0m0.894    \u001b[0m | \u001b[0m0.03946  \u001b[0m | \u001b[0m14.4     \u001b[0m | \u001b[0m147.0    \u001b[0m | \u001b[0m39.94    \u001b[0m |\n",
      "| \u001b[0m49       \u001b[0m | \u001b[0m0.977    \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m15.0     \u001b[0m | \u001b[0m147.5    \u001b[0m | \u001b[0m54.67    \u001b[0m |\n",
      "| \u001b[0m50       \u001b[0m | \u001b[0m0.9398   \u001b[0m | \u001b[0m0.09464  \u001b[0m | \u001b[0m11.09    \u001b[0m | \u001b[0m188.2    \u001b[0m | \u001b[0m78.85    \u001b[0m |\n",
      "| \u001b[0m51       \u001b[0m | \u001b[0m0.9764   \u001b[0m | \u001b[0m0.2549   \u001b[0m | \u001b[0m14.63    \u001b[0m | \u001b[0m176.3    \u001b[0m | \u001b[0m79.12    \u001b[0m |\n",
      "| \u001b[0m52       \u001b[0m | \u001b[0m0.8684   \u001b[0m | \u001b[0m0.02344  \u001b[0m | \u001b[0m14.39    \u001b[0m | \u001b[0m159.7    \u001b[0m | \u001b[0m49.06    \u001b[0m |\n",
      "| \u001b[0m53       \u001b[0m | \u001b[0m0.976    \u001b[0m | \u001b[0m0.2664   \u001b[0m | \u001b[0m14.75    \u001b[0m | \u001b[0m166.5    \u001b[0m | \u001b[0m89.96    \u001b[0m |\n",
      "| \u001b[0m54       \u001b[0m | \u001b[0m0.9779   \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m15.0     \u001b[0m | \u001b[0m143.7    \u001b[0m | \u001b[0m63.6     \u001b[0m |\n",
      "| \u001b[0m55       \u001b[0m | \u001b[0m0.9764   \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m15.0     \u001b[0m | \u001b[0m129.6    \u001b[0m | \u001b[0m61.94    \u001b[0m |\n",
      "=========================================================================\n",
      "|   iter    |  target   | hidden... | learni... |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.5569   \u001b[0m | \u001b[0m87.97    \u001b[0m | \u001b[0m0.009512 \u001b[0m |\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m0.5416   \u001b[0m | \u001b[0m110.8    \u001b[0m | \u001b[0m0.006027 \u001b[0m |\n",
      "| \u001b[95m3        \u001b[0m | \u001b[95m0.5576   \u001b[0m | \u001b[95m73.99    \u001b[0m | \u001b[95m0.001644 \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.5554   \u001b[0m | \u001b[0m67.72    \u001b[0m | \u001b[0m0.008675 \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.5186   \u001b[0m | \u001b[0m102.5    \u001b[0m | \u001b[0m0.00711  \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balag\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.3829   \u001b[0m | \u001b[0m81.26    \u001b[0m | \u001b[0m0.0001   \u001b[0m |\n",
      "| \u001b[95m7        \u001b[0m | \u001b[95m0.5771   \u001b[0m | \u001b[95m74.1     \u001b[0m | \u001b[95m0.006968 \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.4607   \u001b[0m | \u001b[0m74.74    \u001b[0m | \u001b[0m0.01     \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.5146   \u001b[0m | \u001b[0m88.43    \u001b[0m | \u001b[0m0.004167 \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.5529   \u001b[0m | \u001b[0m87.54    \u001b[0m | \u001b[0m0.008146 \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.5544   \u001b[0m | \u001b[0m68.21    \u001b[0m | \u001b[0m0.01     \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.5708   \u001b[0m | \u001b[0m67.16    \u001b[0m | \u001b[0m0.00568  \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.5394   \u001b[0m | \u001b[0m66.69    \u001b[0m | \u001b[0m0.005955 \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.5509   \u001b[0m | \u001b[0m67.18    \u001b[0m | \u001b[0m0.005105 \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.555    \u001b[0m | \u001b[0m67.1     \u001b[0m | \u001b[0m0.004853 \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m0.533    \u001b[0m | \u001b[0m67.16    \u001b[0m | \u001b[0m0.004649 \u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m0.535    \u001b[0m | \u001b[0m67.17    \u001b[0m | \u001b[0m0.005163 \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m0.5515   \u001b[0m | \u001b[0m67.16    \u001b[0m | \u001b[0m0.006942 \u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m0.4064   \u001b[0m | \u001b[0m71.36    \u001b[0m | \u001b[0m0.002065 \u001b[0m |\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.5427   \u001b[0m | \u001b[0m67.16    \u001b[0m | \u001b[0m0.004964 \u001b[0m |\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m0.5092   \u001b[0m | \u001b[0m87.97    \u001b[0m | \u001b[0m0.007873 \u001b[0m |\n",
      "| \u001b[0m22       \u001b[0m | \u001b[0m0.5356   \u001b[0m | \u001b[0m77.33    \u001b[0m | \u001b[0m0.007773 \u001b[0m |\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m0.5577   \u001b[0m | \u001b[0m116.4    \u001b[0m | \u001b[0m0.00684  \u001b[0m |\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m0.574    \u001b[0m | \u001b[0m74.1     \u001b[0m | \u001b[0m0.00745  \u001b[0m |\n",
      "| \u001b[0m25       \u001b[0m | \u001b[0m0.5656   \u001b[0m | \u001b[0m104.6    \u001b[0m | \u001b[0m0.007677 \u001b[0m |\n",
      "| \u001b[95m26       \u001b[0m | \u001b[95m0.5799   \u001b[0m | \u001b[95m90.21    \u001b[0m | \u001b[95m0.008034 \u001b[0m |\n",
      "| \u001b[0m27       \u001b[0m | \u001b[0m0.4637   \u001b[0m | \u001b[0m68.35    \u001b[0m | \u001b[0m0.004171 \u001b[0m |\n",
      "| \u001b[0m28       \u001b[0m | \u001b[0m0.5462   \u001b[0m | \u001b[0m90.21    \u001b[0m | \u001b[0m0.005336 \u001b[0m |\n",
      "| \u001b[0m29       \u001b[0m | \u001b[0m0.5661   \u001b[0m | \u001b[0m104.6    \u001b[0m | \u001b[0m0.004589 \u001b[0m |\n",
      "| \u001b[95m30       \u001b[0m | \u001b[95m0.5928   \u001b[0m | \u001b[95m90.21    \u001b[0m | \u001b[95m0.008757 \u001b[0m |\n",
      "| \u001b[0m31       \u001b[0m | \u001b[0m0.535    \u001b[0m | \u001b[0m67.16    \u001b[0m | \u001b[0m0.007734 \u001b[0m |\n",
      "| \u001b[0m32       \u001b[0m | \u001b[0m0.5329   \u001b[0m | \u001b[0m74.11    \u001b[0m | \u001b[0m0.003355 \u001b[0m |\n",
      "| \u001b[0m33       \u001b[0m | \u001b[0m0.5561   \u001b[0m | \u001b[0m74.09    \u001b[0m | \u001b[0m0.005684 \u001b[0m |\n",
      "| \u001b[0m34       \u001b[0m | \u001b[0m0.5466   \u001b[0m | \u001b[0m122.9    \u001b[0m | \u001b[0m0.008012 \u001b[0m |\n",
      "| \u001b[0m35       \u001b[0m | \u001b[0m0.5878   \u001b[0m | \u001b[0m114.6    \u001b[0m | \u001b[0m0.0008379\u001b[0m |\n",
      "| \u001b[0m36       \u001b[0m | \u001b[0m0.5656   \u001b[0m | \u001b[0m77.78    \u001b[0m | \u001b[0m0.008431 \u001b[0m |\n",
      "| \u001b[0m37       \u001b[0m | \u001b[0m0.5586   \u001b[0m | \u001b[0m114.6    \u001b[0m | \u001b[0m0.00449  \u001b[0m |\n",
      "| \u001b[0m38       \u001b[0m | \u001b[0m0.5431   \u001b[0m | \u001b[0m90.21    \u001b[0m | \u001b[0m0.006247 \u001b[0m |\n",
      "| \u001b[0m39       \u001b[0m | \u001b[0m0.5415   \u001b[0m | \u001b[0m120.8    \u001b[0m | \u001b[0m0.004851 \u001b[0m |\n",
      "| \u001b[0m40       \u001b[0m | \u001b[0m0.5738   \u001b[0m | \u001b[0m73.58    \u001b[0m | \u001b[0m0.004564 \u001b[0m |\n",
      "| \u001b[0m41       \u001b[0m | \u001b[0m0.5385   \u001b[0m | \u001b[0m88.59    \u001b[0m | \u001b[0m0.005103 \u001b[0m |\n",
      "| \u001b[0m42       \u001b[0m | \u001b[0m0.5501   \u001b[0m | \u001b[0m120.7    \u001b[0m | \u001b[0m0.001585 \u001b[0m |\n",
      "| \u001b[0m43       \u001b[0m | \u001b[0m0.5567   \u001b[0m | \u001b[0m74.1     \u001b[0m | \u001b[0m0.003795 \u001b[0m |\n",
      "| \u001b[0m44       \u001b[0m | \u001b[0m0.5589   \u001b[0m | \u001b[0m127.9    \u001b[0m | \u001b[0m0.004908 \u001b[0m |\n",
      "| \u001b[0m45       \u001b[0m | \u001b[0m0.536    \u001b[0m | \u001b[0m119.0    \u001b[0m | \u001b[0m0.002498 \u001b[0m |\n",
      "| \u001b[0m46       \u001b[0m | \u001b[0m0.5784   \u001b[0m | \u001b[0m114.6    \u001b[0m | \u001b[0m0.001378 \u001b[0m |\n",
      "| \u001b[95m47       \u001b[0m | \u001b[95m0.605    \u001b[0m | \u001b[95m73.57    \u001b[0m | \u001b[95m0.006271 \u001b[0m |\n",
      "| \u001b[0m48       \u001b[0m | \u001b[0m0.4254   \u001b[0m | \u001b[0m88.28    \u001b[0m | \u001b[0m0.009416 \u001b[0m |\n",
      "| \u001b[0m49       \u001b[0m | \u001b[0m0.5357   \u001b[0m | \u001b[0m106.0    \u001b[0m | \u001b[0m0.008698 \u001b[0m |\n",
      "| \u001b[0m50       \u001b[0m | \u001b[0m0.5157   \u001b[0m | \u001b[0m73.58    \u001b[0m | \u001b[0m0.008462 \u001b[0m |\n",
      "| \u001b[0m51       \u001b[0m | \u001b[0m0.5056   \u001b[0m | \u001b[0m71.53    \u001b[0m | \u001b[0m0.008972 \u001b[0m |\n",
      "| \u001b[0m52       \u001b[0m | \u001b[0m0.5616   \u001b[0m | \u001b[0m117.4    \u001b[0m | \u001b[0m0.005856 \u001b[0m |\n",
      "| \u001b[0m53       \u001b[0m | \u001b[0m0.3567   \u001b[0m | \u001b[0m72.68    \u001b[0m | \u001b[0m0.008902 \u001b[0m |\n",
      "| \u001b[0m54       \u001b[0m | \u001b[0m0.3677   \u001b[0m | \u001b[0m73.57    \u001b[0m | \u001b[0m0.003841 \u001b[0m |\n",
      "| \u001b[0m55       \u001b[0m | \u001b[0m0.5392   \u001b[0m | \u001b[0m74.1     \u001b[0m | \u001b[0m0.00331  \u001b[0m |\n",
      "=================================================\n",
      "|   iter    |  target   |   alpha   |\n",
      "-------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m3.808    \u001b[0m |\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m9.512    \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m7.347    \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m6.027    \u001b[0m |\n",
      "| \u001b[95m5        \u001b[0m | \u001b[95m0.2107   \u001b[0m | \u001b[95m1.645    \u001b[0m |\n",
      "| \u001b[95m6        \u001b[0m | \u001b[95m0.2108   \u001b[0m | \u001b[95m0.1001   \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m0.3405   \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m0.1735   \u001b[0m |\n",
      "| \u001b[95m9        \u001b[0m | \u001b[95m0.2108   \u001b[0m | \u001b[95m0.1001   \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m4.916    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m0.7785   \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m2.72     \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m8.429    \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m6.687    \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m5.47     \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m4.361    \u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m2.177    \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m3.262    \u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m0.2108   \u001b[0m | \u001b[0m0.116    \u001b[0m |\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m0.5182   \u001b[0m |\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m8.97     \u001b[0m |\n",
      "| \u001b[0m22       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m7.888    \u001b[0m |\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m10.0     \u001b[0m |\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m1.195    \u001b[0m |\n",
      "| \u001b[0m25       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m0.2494   \u001b[0m |\n",
      "| \u001b[0m26       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m6.357    \u001b[0m |\n",
      "| \u001b[0m27       \u001b[0m | \u001b[0m0.2108   \u001b[0m | \u001b[0m0.1003   \u001b[0m |\n",
      "| \u001b[0m28       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m7.016    \u001b[0m |\n",
      "| \u001b[0m29       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m2.448    \u001b[0m |\n",
      "| \u001b[0m30       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m4.084    \u001b[0m |\n",
      "| \u001b[0m31       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m4.637    \u001b[0m |\n",
      "| \u001b[0m32       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m1.909    \u001b[0m |\n",
      "| \u001b[0m33       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m5.193    \u001b[0m |\n",
      "| \u001b[0m34       \u001b[0m | \u001b[0m0.2108   \u001b[0m | \u001b[0m0.1025   \u001b[0m |\n",
      "| \u001b[0m35       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m5.748    \u001b[0m |\n",
      "| \u001b[0m36       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m3.535    \u001b[0m |\n",
      "| \u001b[0m37       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m2.99     \u001b[0m |\n",
      "| \u001b[0m38       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m7.617    \u001b[0m |\n",
      "| \u001b[0m39       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m8.157    \u001b[0m |\n",
      "| \u001b[0m40       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m8.697    \u001b[0m |\n",
      "| \u001b[0m41       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m9.239    \u001b[0m |\n",
      "| \u001b[0m42       \u001b[0m | \u001b[0m0.2108   \u001b[0m | \u001b[0m0.101    \u001b[0m |\n",
      "| \u001b[0m43       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m1.417    \u001b[0m |\n",
      "| \u001b[0m44       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m9.755    \u001b[0m |\n",
      "| \u001b[0m45       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m0.9826   \u001b[0m |\n",
      "| \u001b[0m46       \u001b[0m | \u001b[0m0.2108   \u001b[0m | \u001b[0m0.1014   \u001b[0m |\n",
      "| \u001b[95m47       \u001b[0m | \u001b[95m0.2108   \u001b[0m | \u001b[95m0.1      \u001b[0m |\n",
      "| \u001b[0m48       \u001b[0m | \u001b[0m0.2108   \u001b[0m | \u001b[0m0.1008   \u001b[0m |\n",
      "\u001b[91mData point [0.1] is not unique. 1 duplicates registered. Continuing ...\u001b[0m\n",
      "| \u001b[0m49       \u001b[0m | \u001b[0m0.2108   \u001b[0m | \u001b[0m0.1      \u001b[0m |\n",
      "| \u001b[0m50       \u001b[0m | \u001b[0m0.2108   \u001b[0m | \u001b[0m0.1034   \u001b[0m |\n",
      "| \u001b[0m51       \u001b[0m | \u001b[0m0.2108   \u001b[0m | \u001b[0m0.1005   \u001b[0m |\n",
      "| \u001b[0m52       \u001b[0m | \u001b[0m0.2108   \u001b[0m | \u001b[0m0.1002   \u001b[0m |\n",
      "| \u001b[0m53       \u001b[0m | \u001b[0m0.2108   \u001b[0m | \u001b[0m0.1003   \u001b[0m |\n",
      "\u001b[91mData point [0.1] is not unique. 2 duplicates registered. Continuing ...\u001b[0m\n",
      "| \u001b[0m54       \u001b[0m | \u001b[0m0.2108   \u001b[0m | \u001b[0m0.1      \u001b[0m |\n",
      "| \u001b[0m55       \u001b[0m | \u001b[0m0.2108   \u001b[0m | \u001b[0m0.1021   \u001b[0m |\n",
      "=====================================\n",
      "R2 Train: 0.97\n",
      "R2 Test: 0.86\n",
      "{'pmTotal': 0.86}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import StackingRegressor, RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Define Bayesian optimization function for each model\n",
    "def bayesian_optimization(model_func, param_space, X_train, y_train):\n",
    "    def objective_function(**params):\n",
    "        # Cast parameters to int where required\n",
    "        params = {k: int(v) if k in ['n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'n_neighbors', 'num_leaves'] else v for k, v in params.items()}\n",
    "        if 'hidden_layer_sizes' in params:\n",
    "            params['hidden_layer_sizes'] = tuple([int(params['hidden_layer_sizes']) for _ in range(2)])\n",
    "        model = model_func(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        return model.score(X_train, y_train)\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=objective_function,\n",
    "        pbounds=param_space,\n",
    "        random_state=42,\n",
    "        verbose=2,\n",
    "        allow_duplicate_points=True\n",
    "    )\n",
    "    optimizer.maximize(init_points=5, n_iter=50)\n",
    "\n",
    "    best_params = optimizer.max['params']\n",
    "    best_params = {k: int(v) if k in ['n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'n_neighbors', 'num_leaves'] else v for k, v in best_params.items()}\n",
    "    if 'hidden_layer_sizes' in best_params:\n",
    "        best_params['hidden_layer_sizes'] = tuple([int(best_params['hidden_layer_sizes']) for _ in range(2)])\n",
    "    return model_func(**best_params)\n",
    "\n",
    "# Define parameter spaces for base learners and meta-learner\n",
    "param_spaces = {\n",
    "    'rf': {\n",
    "        'n_estimators': (50, 300),\n",
    "        'max_depth': (5, 50),\n",
    "        'min_samples_split': (2, 10),\n",
    "        'min_samples_leaf': (1, 4),\n",
    "    },\n",
    "    'knn': {\n",
    "        'n_neighbors': (3, 30),\n",
    "    },\n",
    "    'lgbm': {\n",
    "        'num_leaves': (20, 100),\n",
    "        'max_depth': (3, 15),\n",
    "        'learning_rate': (0.01, 0.3),\n",
    "        'n_estimators': (50, 200),\n",
    "    },\n",
    "    'nn': {\n",
    "        'hidden_layer_sizes': (64, 128),\n",
    "        'learning_rate_init': (0.0001, 0.01),\n",
    "    },\n",
    "    'ridge': {\n",
    "        'alpha': (0.1, 10.0),\n",
    "    }\n",
    "}\n",
    "\n",
    "# Stacking Regressor with Bayesian Optimization\n",
    "def Stacking_Regression(X_train, X_test, y_train, y_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Tune base learners\n",
    "    rf = bayesian_optimization(RandomForestRegressor, param_spaces['rf'], X_train, y_train)\n",
    "    knn = bayesian_optimization(KNeighborsRegressor, param_spaces['knn'], X_train, y_train)\n",
    "    lgbm = bayesian_optimization(LGBMRegressor, param_spaces['lgbm'], X_train, y_train)\n",
    "    nn = bayesian_optimization(lambda **kwargs: MLPRegressor(random_state=42, max_iter=1000, **kwargs), param_spaces['nn'], X_train_scaled, y_train)\n",
    "\n",
    "    # Tune meta-learner\n",
    "    ridge = bayesian_optimization(Ridge, param_spaces['ridge'], X_train, y_train)\n",
    "\n",
    "    # Define stacking model\n",
    "    estimators = [\n",
    "        ('rf', rf),\n",
    "        ('knn', knn),\n",
    "        ('lgbm', lgbm),\n",
    "        ('nn', nn),\n",
    "    ]\n",
    "    model = StackingRegressor(estimators=estimators, final_estimator=ridge)\n",
    "\n",
    "    # Fit and predict\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    predict_train = model.predict(X_train_scaled)\n",
    "    predict_test = model.predict(X_test_scaled)\n",
    "\n",
    "    r2_score_train = round(r2_score(y_train, predict_train), 2)\n",
    "    r2_score_test = round(r2_score(y_test, predict_test), 2)\n",
    "    print('R2 Train:', r2_score_train)\n",
    "    print('R2 Test:', r2_score_test)\n",
    "\n",
    "    return r2_score_test, model\n",
    "\n",
    "# Iterate over Palas keys\n",
    "r2_score_test_sl = {}\n",
    "for k in Palas.keys():\n",
    "    if k == \"pmTotal\":\n",
    "        print(k)\n",
    "        X = Palas[k].drop([k + \"Palas\"], axis=1)\n",
    "        y = Palas[k][k + \"Palas\"]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "        r2_score_test_sl[k], model = Stacking_Regression(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Results\n",
    "print(r2_score_test_sl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1b39d9a6-1f98-45b3-9afd-45894744fa6e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# r2_score_test_sl = {}\n",
    "\n",
    "# def Stacking_Regression(X_train,X_test,y_train,y_test,filtered_data):\n",
    "#     ml_type = 'SL'\n",
    "#     estimators = [\n",
    "#         ('rf', RandomForestRegressor(n_estimators = 50, random_state = 0)),\n",
    "#         ('br', BaggingRegressor(estimator = RandomForestRegressor(), n_estimators=10, random_state=42)),\n",
    "#         ('lgbm',LGBMRegressor(\n",
    "#         n_estimators=100,\n",
    "#         max_depth=-1,\n",
    "#         learning_rate=0.1,\n",
    "#         num_leaves=31,\n",
    "#         min_child_samples=20,\n",
    "#         random_state=1\n",
    "#         )),\n",
    "#         ('nn',     MLPRegressor(hidden_layer_sizes=(64, 64), activation='relu', random_state=1, max_iter=500)\n",
    "# )\n",
    "#     ]\n",
    "    \n",
    "#     model = StackingRegressor(\n",
    "#         estimators = estimators,\n",
    "#         final_estimator = Ridge(alpha=1.0, random_state=42))\n",
    "\n",
    "#     model.fit(X_train, y_train)\n",
    "    \n",
    "#     predict_train = model.predict(X_train)\n",
    "#     predict_test = model.predict(X_test)\n",
    "    \n",
    "#     train_df = pd.DataFrame({'Actual': y_train, 'Predicted': predict_train, 'Category': 'Training'}, index=y_train.index)\n",
    "#     test_df = pd.DataFrame({'Actual': y_test, 'Predicted': predict_test, 'Category': 'Testing'}, index=y_test.index)\n",
    "    \n",
    "#     # Concatenate the DataFrames\n",
    "#     combined_df = pd.concat([train_df, test_df])\n",
    "#     combined_df = combined_df.sort_index()\n",
    "#     # Print or use the combined DataFrame as needed\n",
    "#     # print(combined_df)\n",
    "    \n",
    "#     r2_score_train = round(metrics.r2_score(y_train, predict_train),2) \n",
    "#     r2_score_test = round(metrics.r2_score(y_test, predict_test),2)\n",
    "#     print('r2 train',r2_score_train)    \n",
    "#     print('r2 test',r2_score_test) \n",
    "    \n",
    "#     if (v == 'dCn'):\n",
    "#         unit = 'dCn'\n",
    "#     else:\n",
    "#         unit = 'pm_conc'\n",
    "    \n",
    "#     Scatter_Plot(combined_df, train_df, test_df, r2_score_train, r2_score_test, v, unit,ml_type)\n",
    "#     qq_plot(test_df, v, unit,ml_type)\n",
    "#     # Feature_Importance_Random_Forest_Regressor(model, filtered_data, v, unit,ml_type)\n",
    "#     return r2_score_test,model\n",
    "\n",
    "# for k,v in enumerate(Palas):\n",
    "#     X = Palas[v].drop([v+\"Palas\"],axis = 1)\n",
    "#     y = Palas[v][v+\"Palas\"]\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,ra)\n",
    "#     X_train.to_csv(\"X_train.csv\")\n",
    "#     X_test.to_csv(\"X_test.csv\")\n",
    "#     y_train.to_csv(\"y_train.csv\")\n",
    "#     y_test.to_csv(\"y_test.csv\")\n",
    "#     r2_score_test_sl[v],model = Stacking_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "# r2_score_test_sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cc28e675-b13e-4eb0-9770-2ff50028a7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pmTotal\n",
      "|   iter    |  target   | max_depth | min_sa... | min_sa... | n_esti... |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.9134   \u001b[0m | \u001b[0m21.85    \u001b[0m | \u001b[0m3.852    \u001b[0m | \u001b[0m7.856    \u001b[0m | \u001b[0m199.7    \u001b[0m |\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m0.9689   \u001b[0m | \u001b[95m12.02    \u001b[0m | \u001b[95m1.468    \u001b[0m | \u001b[95m2.465    \u001b[0m | \u001b[95m266.5    \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.9181   \u001b[0m | \u001b[0m32.05    \u001b[0m | \u001b[0m3.124    \u001b[0m | \u001b[0m2.165    \u001b[0m | \u001b[0m292.5    \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.968    \u001b[0m | \u001b[0m42.46    \u001b[0m | \u001b[0m1.637    \u001b[0m | \u001b[0m3.455    \u001b[0m | \u001b[0m95.85    \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.9357   \u001b[0m | \u001b[0m18.69    \u001b[0m | \u001b[0m2.574    \u001b[0m | \u001b[0m5.456    \u001b[0m | \u001b[0m122.8    \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.9367   \u001b[0m | \u001b[0m11.73    \u001b[0m | \u001b[0m2.153    \u001b[0m | \u001b[0m3.048    \u001b[0m | \u001b[0m267.7    \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.8778   \u001b[0m | \u001b[0m7.215    \u001b[0m | \u001b[0m1.055    \u001b[0m | \u001b[0m6.804    \u001b[0m | \u001b[0m231.0    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.9389   \u001b[0m | \u001b[0m12.58    \u001b[0m | \u001b[0m2.19     \u001b[0m | \u001b[0m3.009    \u001b[0m | \u001b[0m265.8    \u001b[0m |\n",
      "| \u001b[95m9        \u001b[0m | \u001b[95m0.9732   \u001b[0m | \u001b[95m42.33    \u001b[0m | \u001b[95m1.273    \u001b[0m | \u001b[95m2.209    \u001b[0m | \u001b[95m95.64    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.919    \u001b[0m | \u001b[0m41.28    \u001b[0m | \u001b[0m3.427    \u001b[0m | \u001b[0m2.582    \u001b[0m | \u001b[0m95.82    \u001b[0m |\n",
      "| \u001b[95m11       \u001b[0m | \u001b[95m0.9746   \u001b[0m | \u001b[95m43.56    \u001b[0m | \u001b[95m1.532    \u001b[0m | \u001b[95m2.198    \u001b[0m | \u001b[95m96.35    \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.9475   \u001b[0m | \u001b[0m44.74    \u001b[0m | \u001b[0m1.993    \u001b[0m | \u001b[0m5.338    \u001b[0m | \u001b[0m96.87    \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.9675   \u001b[0m | \u001b[0m43.3     \u001b[0m | \u001b[0m1.057    \u001b[0m | \u001b[0m3.524    \u001b[0m | \u001b[0m98.22    \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.9727   \u001b[0m | \u001b[0m43.94    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.768    \u001b[0m | \u001b[0m94.49    \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.9468   \u001b[0m | \u001b[0m9.745    \u001b[0m | \u001b[0m1.187    \u001b[0m | \u001b[0m2.281    \u001b[0m | \u001b[0m265.6    \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m0.9729   \u001b[0m | \u001b[0m43.09    \u001b[0m | \u001b[0m1.222    \u001b[0m | \u001b[0m2.032    \u001b[0m | \u001b[0m92.75    \u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m0.9446   \u001b[0m | \u001b[0m42.21    \u001b[0m | \u001b[0m1.113    \u001b[0m | \u001b[0m5.142    \u001b[0m | \u001b[0m93.19    \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m0.9186   \u001b[0m | \u001b[0m46.43    \u001b[0m | \u001b[0m3.52     \u001b[0m | \u001b[0m2.296    \u001b[0m | \u001b[0m92.98    \u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m0.9743   \u001b[0m | \u001b[0m39.93    \u001b[0m | \u001b[0m1.873    \u001b[0m | \u001b[0m2.442    \u001b[0m | \u001b[0m91.69    \u001b[0m |\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.9175   \u001b[0m | \u001b[0m37.98    \u001b[0m | \u001b[0m3.21     \u001b[0m | \u001b[0m3.365    \u001b[0m | \u001b[0m90.56    \u001b[0m |\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m0.9658   \u001b[0m | \u001b[0m42.81    \u001b[0m | \u001b[0m1.825    \u001b[0m | \u001b[0m3.016    \u001b[0m | \u001b[0m90.27    \u001b[0m |\n",
      "| \u001b[0m22       \u001b[0m | \u001b[0m0.938    \u001b[0m | \u001b[0m41.5     \u001b[0m | \u001b[0m2.872    \u001b[0m | \u001b[0m3.367    \u001b[0m | \u001b[0m91.69    \u001b[0m |\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m0.9661   \u001b[0m | \u001b[0m41.86    \u001b[0m | \u001b[0m1.652    \u001b[0m | \u001b[0m3.105    \u001b[0m | \u001b[0m87.66    \u001b[0m |\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m0.9393   \u001b[0m | \u001b[0m42.06    \u001b[0m | \u001b[0m2.793    \u001b[0m | \u001b[0m2.974    \u001b[0m | \u001b[0m85.61    \u001b[0m |\n",
      "| \u001b[0m25       \u001b[0m | \u001b[0m0.9597   \u001b[0m | \u001b[0m41.32    \u001b[0m | \u001b[0m1.454    \u001b[0m | \u001b[0m4.182    \u001b[0m | \u001b[0m88.68    \u001b[0m |\n",
      "| \u001b[0m26       \u001b[0m | \u001b[0m0.9187   \u001b[0m | \u001b[0m44.52    \u001b[0m | \u001b[0m3.817    \u001b[0m | \u001b[0m2.393    \u001b[0m | \u001b[0m99.38    \u001b[0m |\n",
      "| \u001b[0m27       \u001b[0m | \u001b[0m0.9707   \u001b[0m | \u001b[0m45.5     \u001b[0m | \u001b[0m1.865    \u001b[0m | \u001b[0m2.24     \u001b[0m | \u001b[0m89.23    \u001b[0m |\n",
      "| \u001b[0m28       \u001b[0m | \u001b[0m0.9391   \u001b[0m | \u001b[0m44.63    \u001b[0m | \u001b[0m2.011    \u001b[0m | \u001b[0m4.672    \u001b[0m | \u001b[0m88.01    \u001b[0m |\n",
      "| \u001b[0m29       \u001b[0m | \u001b[0m0.9673   \u001b[0m | \u001b[0m43.99    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m3.506    \u001b[0m | \u001b[0m96.24    \u001b[0m |\n",
      "| \u001b[0m30       \u001b[0m | \u001b[0m0.9179   \u001b[0m | \u001b[0m43.13    \u001b[0m | \u001b[0m3.648    \u001b[0m | \u001b[0m2.08     \u001b[0m | \u001b[0m88.19    \u001b[0m |\n",
      "| \u001b[0m31       \u001b[0m | \u001b[0m0.9743   \u001b[0m | \u001b[0m40.25    \u001b[0m | \u001b[0m1.325    \u001b[0m | \u001b[0m2.561    \u001b[0m | \u001b[0m93.66    \u001b[0m |\n",
      "| \u001b[0m32       \u001b[0m | \u001b[0m0.9676   \u001b[0m | \u001b[0m48.52    \u001b[0m | \u001b[0m1.89     \u001b[0m | \u001b[0m3.246    \u001b[0m | \u001b[0m89.6     \u001b[0m |\n",
      "| \u001b[0m33       \u001b[0m | \u001b[0m0.9742   \u001b[0m | \u001b[0m47.3     \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m88.41    \u001b[0m |\n",
      "| \u001b[0m34       \u001b[0m | \u001b[0m0.973    \u001b[0m | \u001b[0m38.63    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m92.78    \u001b[0m |\n",
      "| \u001b[0m35       \u001b[0m | \u001b[0m0.9745   \u001b[0m | \u001b[0m50.0     \u001b[0m | \u001b[0m1.356    \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m87.32    \u001b[0m |\n",
      "| \u001b[0m36       \u001b[0m | \u001b[0m0.9182   \u001b[0m | \u001b[0m47.86    \u001b[0m | \u001b[0m3.395    \u001b[0m | \u001b[0m2.027    \u001b[0m | \u001b[0m86.93    \u001b[0m |\n",
      "| \u001b[0m37       \u001b[0m | \u001b[0m0.9374   \u001b[0m | \u001b[0m48.96    \u001b[0m | \u001b[0m1.326    \u001b[0m | \u001b[0m6.273    \u001b[0m | \u001b[0m89.01    \u001b[0m |\n",
      "| \u001b[0m38       \u001b[0m | \u001b[0m0.9735   \u001b[0m | \u001b[0m41.62    \u001b[0m | \u001b[0m1.046    \u001b[0m | \u001b[0m2.131    \u001b[0m | \u001b[0m97.76    \u001b[0m |\n",
      "| \u001b[0m39       \u001b[0m | \u001b[0m0.9567   \u001b[0m | \u001b[0m40.33    \u001b[0m | \u001b[0m1.002    \u001b[0m | \u001b[0m4.587    \u001b[0m | \u001b[0m99.86    \u001b[0m |\n",
      "| \u001b[0m40       \u001b[0m | \u001b[0m0.9729   \u001b[0m | \u001b[0m35.59    \u001b[0m | \u001b[0m1.125    \u001b[0m | \u001b[0m2.342    \u001b[0m | \u001b[0m96.27    \u001b[0m |\n",
      "| \u001b[0m41       \u001b[0m | \u001b[0m0.971    \u001b[0m | \u001b[0m33.51    \u001b[0m | \u001b[0m1.215    \u001b[0m | \u001b[0m2.193    \u001b[0m | \u001b[0m98.14    \u001b[0m |\n",
      "| \u001b[0m42       \u001b[0m | \u001b[0m0.9588   \u001b[0m | \u001b[0m35.64    \u001b[0m | \u001b[0m1.335    \u001b[0m | \u001b[0m4.641    \u001b[0m | \u001b[0m98.24    \u001b[0m |\n",
      "| \u001b[0m43       \u001b[0m | \u001b[0m0.9411   \u001b[0m | \u001b[0m32.84    \u001b[0m | \u001b[0m2.627    \u001b[0m | \u001b[0m3.519    \u001b[0m | \u001b[0m95.37    \u001b[0m |\n",
      "| \u001b[0m44       \u001b[0m | \u001b[0m0.9733   \u001b[0m | \u001b[0m35.45    \u001b[0m | \u001b[0m1.557    \u001b[0m | \u001b[0m2.166    \u001b[0m | \u001b[0m98.76    \u001b[0m |\n",
      "| \u001b[0m45       \u001b[0m | \u001b[0m0.9687   \u001b[0m | \u001b[0m34.79    \u001b[0m | \u001b[0m1.225    \u001b[0m | \u001b[0m3.034    \u001b[0m | \u001b[0m101.8    \u001b[0m |\n",
      "| \u001b[0m46       \u001b[0m | \u001b[0m0.9734   \u001b[0m | \u001b[0m32.14    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m101.0    \u001b[0m |\n",
      "| \u001b[0m47       \u001b[0m | \u001b[0m0.9178   \u001b[0m | \u001b[0m33.47    \u001b[0m | \u001b[0m3.618    \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m100.5    \u001b[0m |\n",
      "| \u001b[0m48       \u001b[0m | \u001b[0m0.9737   \u001b[0m | \u001b[0m30.19    \u001b[0m | \u001b[0m1.399    \u001b[0m | \u001b[0m2.205    \u001b[0m | \u001b[0m99.04    \u001b[0m |\n",
      "| \u001b[0m49       \u001b[0m | \u001b[0m0.9689   \u001b[0m | \u001b[0m31.53    \u001b[0m | \u001b[0m1.179    \u001b[0m | \u001b[0m3.877    \u001b[0m | \u001b[0m100.3    \u001b[0m |\n",
      "| \u001b[0m50       \u001b[0m | \u001b[0m0.9725   \u001b[0m | \u001b[0m29.4     \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.112    \u001b[0m | \u001b[0m101.5    \u001b[0m |\n",
      "| \u001b[0m51       \u001b[0m | \u001b[0m0.9404   \u001b[0m | \u001b[0m27.27    \u001b[0m | \u001b[0m2.623    \u001b[0m | \u001b[0m2.124    \u001b[0m | \u001b[0m98.82    \u001b[0m |\n",
      "| \u001b[0m52       \u001b[0m | \u001b[0m0.9451   \u001b[0m | \u001b[0m36.32    \u001b[0m | \u001b[0m1.227    \u001b[0m | \u001b[0m5.399    \u001b[0m | \u001b[0m103.0    \u001b[0m |\n",
      "| \u001b[0m53       \u001b[0m | \u001b[0m0.9602   \u001b[0m | \u001b[0m30.88    \u001b[0m | \u001b[0m1.833    \u001b[0m | \u001b[0m4.82     \u001b[0m | \u001b[0m103.4    \u001b[0m |\n",
      "| \u001b[0m54       \u001b[0m | \u001b[0m0.9734   \u001b[0m | \u001b[0m37.75    \u001b[0m | \u001b[0m1.658    \u001b[0m | \u001b[0m2.529    \u001b[0m | \u001b[0m95.19    \u001b[0m |\n",
      "| \u001b[0m55       \u001b[0m | \u001b[0m0.9717   \u001b[0m | \u001b[0m31.62    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m103.7    \u001b[0m |\n",
      "=========================================================================\n",
      "|   iter    |  target   | max_depth | min_sa... | min_sa... |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.9201   \u001b[0m | \u001b[0m21.85    \u001b[0m | \u001b[0m3.852    \u001b[0m | \u001b[0m7.856    \u001b[0m |\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m0.9949   \u001b[0m | \u001b[95m31.94    \u001b[0m | \u001b[95m1.468    \u001b[0m | \u001b[95m3.248    \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.8628   \u001b[0m | \u001b[0m7.614    \u001b[0m | \u001b[0m3.599    \u001b[0m | \u001b[0m6.809    \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.9304   \u001b[0m | \u001b[0m36.86    \u001b[0m | \u001b[0m1.062    \u001b[0m | \u001b[0m9.759    \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.9949   \u001b[0m | \u001b[0m42.46    \u001b[0m | \u001b[0m1.637    \u001b[0m | \u001b[0m3.455    \u001b[0m |\n",
      "| \u001b[95m6        \u001b[0m | \u001b[95m0.9949   \u001b[0m | \u001b[95m42.57    \u001b[0m | \u001b[95m1.533    \u001b[0m | \u001b[95m3.273    \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.9123   \u001b[0m | \u001b[0m50.0     \u001b[0m | \u001b[0m4.0      \u001b[0m | \u001b[0m7.246    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.9123   \u001b[0m | \u001b[0m37.22    \u001b[0m | \u001b[0m4.0      \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "| \u001b[95m9        \u001b[0m | \u001b[95m0.9999   \u001b[0m | \u001b[95m28.78    \u001b[0m | \u001b[95m1.0      \u001b[0m | \u001b[95m2.0      \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.9584   \u001b[0m | \u001b[0m29.09    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m6.289    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.9123   \u001b[0m | \u001b[0m29.59    \u001b[0m | \u001b[0m4.0      \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.9709   \u001b[0m | \u001b[0m44.44    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m5.029    \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.9123   \u001b[0m | \u001b[0m43.9     \u001b[0m | \u001b[0m4.0      \u001b[0m | \u001b[0m2.646    \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.9949   \u001b[0m | \u001b[0m29.99    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m3.548    \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.9999   \u001b[0m | \u001b[0m25.89    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.415    \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m0.9999   \u001b[0m | \u001b[0m22.45    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m0.9997   \u001b[0m | \u001b[0m18.48    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m0.9303   \u001b[0m | \u001b[0m19.12    \u001b[0m | \u001b[0m3.992    \u001b[0m | \u001b[0m2.072    \u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m0.9971   \u001b[0m | \u001b[0m15.11    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.9707   \u001b[0m | \u001b[0m16.43    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m5.138    \u001b[0m |\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m0.9112   \u001b[0m | \u001b[0m13.0     \u001b[0m | \u001b[0m4.0      \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "| \u001b[0m22       \u001b[0m | \u001b[0m0.9709   \u001b[0m | \u001b[0m40.57    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m5.942    \u001b[0m |\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m0.9881   \u001b[0m | \u001b[0m24.01    \u001b[0m | \u001b[0m1.003    \u001b[0m | \u001b[0m4.813    \u001b[0m |\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m0.9098   \u001b[0m | \u001b[0m43.01    \u001b[0m | \u001b[0m4.0      \u001b[0m | \u001b[0m10.0     \u001b[0m |\n",
      "| \u001b[0m25       \u001b[0m | \u001b[0m0.9999   \u001b[0m | \u001b[0m40.34    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "| \u001b[0m26       \u001b[0m | \u001b[0m0.9709   \u001b[0m | \u001b[0m34.16    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m5.132    \u001b[0m |\n",
      "| \u001b[0m27       \u001b[0m | \u001b[0m0.9253   \u001b[0m | \u001b[0m15.41    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m10.0     \u001b[0m |\n",
      "| \u001b[0m28       \u001b[0m | \u001b[0m0.9999   \u001b[0m | \u001b[0m34.51    \u001b[0m | \u001b[0m1.029    \u001b[0m | \u001b[0m2.237    \u001b[0m |\n",
      "| \u001b[0m29       \u001b[0m | \u001b[0m0.9999   \u001b[0m | \u001b[0m31.94    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "| \u001b[0m30       \u001b[0m | \u001b[0m0.9999   \u001b[0m | \u001b[0m50.0     \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "| \u001b[0m31       \u001b[0m | \u001b[0m0.9881   \u001b[0m | \u001b[0m20.56    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m4.149    \u001b[0m |\n",
      "| \u001b[0m32       \u001b[0m | \u001b[0m0.9303   \u001b[0m | \u001b[0m49.98    \u001b[0m | \u001b[0m3.726    \u001b[0m | \u001b[0m2.051    \u001b[0m |\n",
      "| \u001b[0m33       \u001b[0m | \u001b[0m0.9881   \u001b[0m | \u001b[0m50.0     \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m4.323    \u001b[0m |\n",
      "| \u001b[0m34       \u001b[0m | \u001b[0m0.9999   \u001b[0m | \u001b[0m47.62    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "| \u001b[0m35       \u001b[0m | \u001b[0m0.9256   \u001b[0m | \u001b[0m25.53    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m10.0     \u001b[0m |\n",
      "| \u001b[0m36       \u001b[0m | \u001b[0m0.9949   \u001b[0m | \u001b[0m40.95    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m3.562    \u001b[0m |\n",
      "| \u001b[0m37       \u001b[0m | \u001b[0m0.7947   \u001b[0m | \u001b[0m5.049    \u001b[0m | \u001b[0m1.377    \u001b[0m | \u001b[0m2.205    \u001b[0m |\n",
      "| \u001b[0m38       \u001b[0m | \u001b[0m0.9995   \u001b[0m | \u001b[0m16.71    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "| \u001b[0m39       \u001b[0m | \u001b[0m0.9256   \u001b[0m | \u001b[0m50.0     \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m10.0     \u001b[0m |\n",
      "| \u001b[0m40       \u001b[0m | \u001b[0m0.9999   \u001b[0m | \u001b[0m37.46    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "| \u001b[0m41       \u001b[0m | \u001b[0m0.9949   \u001b[0m | \u001b[0m48.59    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m3.064    \u001b[0m |\n",
      "| \u001b[0m42       \u001b[0m | \u001b[0m0.9999   \u001b[0m | \u001b[0m24.11    \u001b[0m | \u001b[0m1.027    \u001b[0m | \u001b[0m2.147    \u001b[0m |\n",
      "| \u001b[0m43       \u001b[0m | \u001b[0m0.9098   \u001b[0m | \u001b[0m31.86    \u001b[0m | \u001b[0m4.0      \u001b[0m | \u001b[0m10.0     \u001b[0m |\n",
      "| \u001b[0m44       \u001b[0m | \u001b[0m0.9949   \u001b[0m | \u001b[0m32.89    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m3.082    \u001b[0m |\n",
      "| \u001b[0m45       \u001b[0m | \u001b[0m0.9999   \u001b[0m | \u001b[0m20.44    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "| \u001b[0m46       \u001b[0m | \u001b[0m0.9999   \u001b[0m | \u001b[0m35.91    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "| \u001b[0m47       \u001b[0m | \u001b[0m0.9999   \u001b[0m | \u001b[0m45.32    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "| \u001b[0m48       \u001b[0m | \u001b[0m0.9949   \u001b[0m | \u001b[0m37.31    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m3.549    \u001b[0m |\n",
      "| \u001b[0m49       \u001b[0m | \u001b[0m0.9949   \u001b[0m | \u001b[0m22.93    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m3.257    \u001b[0m |\n",
      "| \u001b[0m50       \u001b[0m | \u001b[0m0.9999   \u001b[0m | \u001b[0m42.15    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "| \u001b[0m51       \u001b[0m | \u001b[0m0.9999   \u001b[0m | \u001b[0m30.58    \u001b[0m | \u001b[0m1.009    \u001b[0m | \u001b[0m2.047    \u001b[0m |\n",
      "| \u001b[0m52       \u001b[0m | \u001b[0m0.9949   \u001b[0m | \u001b[0m26.92    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m3.754    \u001b[0m |\n",
      "| \u001b[0m53       \u001b[0m | \u001b[0m0.9947   \u001b[0m | \u001b[0m18.18    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m3.136    \u001b[0m |\n",
      "| \u001b[0m54       \u001b[0m | \u001b[0m0.9999   \u001b[0m | \u001b[0m38.85    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.539    \u001b[0m |\n",
      "| \u001b[0m55       \u001b[0m | \u001b[0m0.9999   \u001b[0m | \u001b[0m48.93    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "=============================================================\n",
      "|   iter    |  target   | learni... | max_depth | n_esti... | num_le... |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.9499   \u001b[0m | \u001b[0m0.1186   \u001b[0m | \u001b[0m14.41    \u001b[0m | \u001b[0m159.8    \u001b[0m | \u001b[0m67.89    \u001b[0m |\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m0.7571   \u001b[0m | \u001b[0m0.05525  \u001b[0m | \u001b[0m4.872    \u001b[0m | \u001b[0m58.71    \u001b[0m | \u001b[0m89.29    \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.919    \u001b[0m | \u001b[0m0.1843   \u001b[0m | \u001b[0m11.5     \u001b[0m | \u001b[0m53.09    \u001b[0m | \u001b[0m97.59    \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.8993   \u001b[0m | \u001b[0m0.2514   \u001b[0m | \u001b[0m5.548    \u001b[0m | \u001b[0m77.27    \u001b[0m | \u001b[0m34.67    \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.9149   \u001b[0m | \u001b[0m0.09823  \u001b[0m | \u001b[0m9.297    \u001b[0m | \u001b[0m114.8    \u001b[0m | \u001b[0m43.3     \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.9279   \u001b[0m | \u001b[0m0.1287   \u001b[0m | \u001b[0m9.625    \u001b[0m | \u001b[0m115.4    \u001b[0m | \u001b[0m43.56    \u001b[0m |\n",
      "| \u001b[95m7        \u001b[0m | \u001b[95m0.9686   \u001b[0m | \u001b[95m0.3      \u001b[0m | \u001b[95m12.54    \u001b[0m | \u001b[95m121.0    \u001b[0m | \u001b[95m45.84    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.9307   \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m5.543    \u001b[0m | \u001b[0m122.7    \u001b[0m | \u001b[0m49.13    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.8848   \u001b[0m | \u001b[0m0.04357  \u001b[0m | \u001b[0m10.69    \u001b[0m | \u001b[0m123.5    \u001b[0m | \u001b[0m38.06    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.93     \u001b[0m | \u001b[0m0.09886  \u001b[0m | \u001b[0m14.84    \u001b[0m | \u001b[0m117.6    \u001b[0m | \u001b[0m51.53    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.9064   \u001b[0m | \u001b[0m0.06692  \u001b[0m | \u001b[0m8.978    \u001b[0m | \u001b[0m154.9    \u001b[0m | \u001b[0m64.0     \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.9681   \u001b[0m | \u001b[0m0.1854   \u001b[0m | \u001b[0m15.0     \u001b[0m | \u001b[0m165.7    \u001b[0m | \u001b[0m72.86    \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.8629   \u001b[0m | \u001b[0m0.02054  \u001b[0m | \u001b[0m14.43    \u001b[0m | \u001b[0m169.9    \u001b[0m | \u001b[0m66.58    \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.9599   \u001b[0m | \u001b[0m0.1586   \u001b[0m | \u001b[0m14.0     \u001b[0m | \u001b[0m159.7    \u001b[0m | \u001b[0m74.63    \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.9586   \u001b[0m | \u001b[0m0.1444   \u001b[0m | \u001b[0m14.52    \u001b[0m | \u001b[0m165.4    \u001b[0m | \u001b[0m80.35    \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m0.8493   \u001b[0m | \u001b[0m0.0199   \u001b[0m | \u001b[0m8.393    \u001b[0m | \u001b[0m164.1    \u001b[0m | \u001b[0m76.34    \u001b[0m |\n",
      "| \u001b[95m17       \u001b[0m | \u001b[95m0.98     \u001b[0m | \u001b[95m0.3      \u001b[0m | \u001b[95m15.0     \u001b[0m | \u001b[95m159.5    \u001b[0m | \u001b[95m80.8     \u001b[0m |\n",
      "| \u001b[95m18       \u001b[0m | \u001b[95m0.9802   \u001b[0m | \u001b[95m0.3      \u001b[0m | \u001b[95m15.0     \u001b[0m | \u001b[95m161.3    \u001b[0m | \u001b[95m86.37    \u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m0.9735   \u001b[0m | \u001b[0m0.2717   \u001b[0m | \u001b[0m13.51    \u001b[0m | \u001b[0m155.4    \u001b[0m | \u001b[0m86.82    \u001b[0m |\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.9787   \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m15.0     \u001b[0m | \u001b[0m151.3    \u001b[0m | \u001b[0m79.75    \u001b[0m |\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m0.9781   \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m15.0     \u001b[0m | \u001b[0m145.8    \u001b[0m | \u001b[0m86.19    \u001b[0m |\n",
      "| \u001b[0m22       \u001b[0m | \u001b[0m0.9558   \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m7.398    \u001b[0m | \u001b[0m146.6    \u001b[0m | \u001b[0m83.06    \u001b[0m |\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m0.9776   \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m15.0     \u001b[0m | \u001b[0m141.6    \u001b[0m | \u001b[0m78.26    \u001b[0m |\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m0.9465   \u001b[0m | \u001b[0m0.1241   \u001b[0m | \u001b[0m15.0     \u001b[0m | \u001b[0m135.8    \u001b[0m | \u001b[0m85.8     \u001b[0m |\n",
      "| \u001b[0m25       \u001b[0m | \u001b[0m0.9598   \u001b[0m | \u001b[0m0.1792   \u001b[0m | \u001b[0m13.78    \u001b[0m | \u001b[0m149.7    \u001b[0m | \u001b[0m95.39    \u001b[0m |\n",
      "| \u001b[0m26       \u001b[0m | \u001b[0m0.9802   \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m15.0     \u001b[0m | \u001b[0m161.5    \u001b[0m | \u001b[0m96.41    \u001b[0m |\n",
      "| \u001b[0m27       \u001b[0m | \u001b[0m0.95     \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m6.105    \u001b[0m | \u001b[0m159.5    \u001b[0m | \u001b[0m96.7     \u001b[0m |\n",
      "| \u001b[0m28       \u001b[0m | \u001b[0m0.9577   \u001b[0m | \u001b[0m0.1436   \u001b[0m | \u001b[0m13.97    \u001b[0m | \u001b[0m171.3    \u001b[0m | \u001b[0m96.39    \u001b[0m |\n",
      "| \u001b[0m29       \u001b[0m | \u001b[0m0.959    \u001b[0m | \u001b[0m0.1649   \u001b[0m | \u001b[0m15.0     \u001b[0m | \u001b[0m136.3    \u001b[0m | \u001b[0m69.63    \u001b[0m |\n",
      "| \u001b[0m30       \u001b[0m | \u001b[0m0.8158   \u001b[0m | \u001b[0m0.04944  \u001b[0m | \u001b[0m4.62     \u001b[0m | \u001b[0m138.3    \u001b[0m | \u001b[0m73.62    \u001b[0m |\n",
      "| \u001b[0m31       \u001b[0m | \u001b[0m0.8838   \u001b[0m | \u001b[0m0.1363   \u001b[0m | \u001b[0m4.077    \u001b[0m | \u001b[0m144.2    \u001b[0m | \u001b[0m95.04    \u001b[0m |\n",
      "| \u001b[0m32       \u001b[0m | \u001b[0m0.9784   \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m15.0     \u001b[0m | \u001b[0m147.4    \u001b[0m | \u001b[0m71.94    \u001b[0m |\n",
      "| \u001b[0m33       \u001b[0m | \u001b[0m0.9136   \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m3.0      \u001b[0m | \u001b[0m172.4    \u001b[0m | \u001b[0m100.0    \u001b[0m |\n",
      "| \u001b[0m34       \u001b[0m | \u001b[0m0.9798   \u001b[0m | \u001b[0m0.2917   \u001b[0m | \u001b[0m14.69    \u001b[0m | \u001b[0m174.7    \u001b[0m | \u001b[0m86.83    \u001b[0m |\n",
      "| \u001b[95m35       \u001b[0m | \u001b[95m0.9824   \u001b[0m | \u001b[95m0.3      \u001b[0m | \u001b[95m15.0     \u001b[0m | \u001b[95m183.5    \u001b[0m | \u001b[95m89.8     \u001b[0m |\n",
      "| \u001b[0m36       \u001b[0m | \u001b[0m0.9824   \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m15.0     \u001b[0m | \u001b[0m183.1    \u001b[0m | \u001b[0m81.22    \u001b[0m |\n",
      "| \u001b[0m37       \u001b[0m | \u001b[0m0.9542   \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m6.835    \u001b[0m | \u001b[0m184.0    \u001b[0m | \u001b[0m85.57    \u001b[0m |\n",
      "| \u001b[0m38       \u001b[0m | \u001b[0m0.9539   \u001b[0m | \u001b[0m0.1084   \u001b[0m | \u001b[0m14.69    \u001b[0m | \u001b[0m193.8    \u001b[0m | \u001b[0m84.02    \u001b[0m |\n",
      "| \u001b[0m39       \u001b[0m | \u001b[0m0.9339   \u001b[0m | \u001b[0m0.06863  \u001b[0m | \u001b[0m13.73    \u001b[0m | \u001b[0m191.5    \u001b[0m | \u001b[0m98.23    \u001b[0m |\n",
      "| \u001b[95m40       \u001b[0m | \u001b[95m0.983    \u001b[0m | \u001b[95m0.3      \u001b[0m | \u001b[95m15.0     \u001b[0m | \u001b[95m190.0    \u001b[0m | \u001b[95m72.88    \u001b[0m |\n",
      "| \u001b[0m41       \u001b[0m | \u001b[0m0.9551   \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m6.155    \u001b[0m | \u001b[0m192.2    \u001b[0m | \u001b[0m73.45    \u001b[0m |\n",
      "| \u001b[0m42       \u001b[0m | \u001b[0m0.8922   \u001b[0m | \u001b[0m0.02803  \u001b[0m | \u001b[0m13.63    \u001b[0m | \u001b[0m196.4    \u001b[0m | \u001b[0m66.62    \u001b[0m |\n",
      "| \u001b[0m43       \u001b[0m | \u001b[0m0.9523   \u001b[0m | \u001b[0m0.1295   \u001b[0m | \u001b[0m11.74    \u001b[0m | \u001b[0m183.3    \u001b[0m | \u001b[0m72.39    \u001b[0m |\n",
      "| \u001b[0m44       \u001b[0m | \u001b[0m0.9772   \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m15.0     \u001b[0m | \u001b[0m138.4    \u001b[0m | \u001b[0m57.96    \u001b[0m |\n",
      "| \u001b[0m45       \u001b[0m | \u001b[0m0.9766   \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m15.0     \u001b[0m | \u001b[0m142.2    \u001b[0m | \u001b[0m49.38    \u001b[0m |\n",
      "| \u001b[0m46       \u001b[0m | \u001b[0m0.842    \u001b[0m | \u001b[0m0.02322  \u001b[0m | \u001b[0m7.053    \u001b[0m | \u001b[0m139.2    \u001b[0m | \u001b[0m52.02    \u001b[0m |\n",
      "| \u001b[0m47       \u001b[0m | \u001b[0m0.9737   \u001b[0m | \u001b[0m0.2612   \u001b[0m | \u001b[0m14.99    \u001b[0m | \u001b[0m151.6    \u001b[0m | \u001b[0m47.44    \u001b[0m |\n",
      "| \u001b[0m48       \u001b[0m | \u001b[0m0.894    \u001b[0m | \u001b[0m0.03946  \u001b[0m | \u001b[0m14.4     \u001b[0m | \u001b[0m147.0    \u001b[0m | \u001b[0m39.94    \u001b[0m |\n",
      "| \u001b[0m49       \u001b[0m | \u001b[0m0.977    \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m15.0     \u001b[0m | \u001b[0m147.5    \u001b[0m | \u001b[0m54.67    \u001b[0m |\n",
      "| \u001b[0m50       \u001b[0m | \u001b[0m0.9398   \u001b[0m | \u001b[0m0.09464  \u001b[0m | \u001b[0m11.09    \u001b[0m | \u001b[0m188.2    \u001b[0m | \u001b[0m78.85    \u001b[0m |\n",
      "| \u001b[0m51       \u001b[0m | \u001b[0m0.9764   \u001b[0m | \u001b[0m0.2549   \u001b[0m | \u001b[0m14.63    \u001b[0m | \u001b[0m176.3    \u001b[0m | \u001b[0m79.12    \u001b[0m |\n",
      "| \u001b[0m52       \u001b[0m | \u001b[0m0.8684   \u001b[0m | \u001b[0m0.02344  \u001b[0m | \u001b[0m14.39    \u001b[0m | \u001b[0m159.7    \u001b[0m | \u001b[0m49.06    \u001b[0m |\n",
      "| \u001b[0m53       \u001b[0m | \u001b[0m0.976    \u001b[0m | \u001b[0m0.2664   \u001b[0m | \u001b[0m14.75    \u001b[0m | \u001b[0m166.5    \u001b[0m | \u001b[0m89.96    \u001b[0m |\n",
      "| \u001b[0m54       \u001b[0m | \u001b[0m0.9779   \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m15.0     \u001b[0m | \u001b[0m143.7    \u001b[0m | \u001b[0m63.6     \u001b[0m |\n",
      "| \u001b[0m55       \u001b[0m | \u001b[0m0.9764   \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m15.0     \u001b[0m | \u001b[0m129.6    \u001b[0m | \u001b[0m61.94    \u001b[0m |\n",
      "=========================================================================\n",
      "|   iter    |  target   | hidden... | learni... |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.5569   \u001b[0m | \u001b[0m87.97    \u001b[0m | \u001b[0m0.009512 \u001b[0m |\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m0.5416   \u001b[0m | \u001b[0m110.8    \u001b[0m | \u001b[0m0.006027 \u001b[0m |\n",
      "| \u001b[95m3        \u001b[0m | \u001b[95m0.5576   \u001b[0m | \u001b[95m73.99    \u001b[0m | \u001b[95m0.001644 \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.5554   \u001b[0m | \u001b[0m67.72    \u001b[0m | \u001b[0m0.008675 \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.5186   \u001b[0m | \u001b[0m102.5    \u001b[0m | \u001b[0m0.00711  \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balag\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.3829   \u001b[0m | \u001b[0m81.26    \u001b[0m | \u001b[0m0.0001   \u001b[0m |\n",
      "| \u001b[95m7        \u001b[0m | \u001b[95m0.5771   \u001b[0m | \u001b[95m74.1     \u001b[0m | \u001b[95m0.006968 \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.4607   \u001b[0m | \u001b[0m74.74    \u001b[0m | \u001b[0m0.01     \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.5146   \u001b[0m | \u001b[0m88.43    \u001b[0m | \u001b[0m0.004167 \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.5529   \u001b[0m | \u001b[0m87.54    \u001b[0m | \u001b[0m0.008146 \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.5544   \u001b[0m | \u001b[0m68.21    \u001b[0m | \u001b[0m0.01     \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.5708   \u001b[0m | \u001b[0m67.16    \u001b[0m | \u001b[0m0.00568  \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.5394   \u001b[0m | \u001b[0m66.69    \u001b[0m | \u001b[0m0.005955 \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.5509   \u001b[0m | \u001b[0m67.18    \u001b[0m | \u001b[0m0.005105 \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.555    \u001b[0m | \u001b[0m67.1     \u001b[0m | \u001b[0m0.004853 \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m0.533    \u001b[0m | \u001b[0m67.16    \u001b[0m | \u001b[0m0.004649 \u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m0.535    \u001b[0m | \u001b[0m67.17    \u001b[0m | \u001b[0m0.005163 \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m0.5515   \u001b[0m | \u001b[0m67.16    \u001b[0m | \u001b[0m0.006942 \u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m0.4064   \u001b[0m | \u001b[0m71.36    \u001b[0m | \u001b[0m0.002065 \u001b[0m |\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.5427   \u001b[0m | \u001b[0m67.16    \u001b[0m | \u001b[0m0.004964 \u001b[0m |\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m0.5092   \u001b[0m | \u001b[0m87.97    \u001b[0m | \u001b[0m0.007873 \u001b[0m |\n",
      "| \u001b[0m22       \u001b[0m | \u001b[0m0.5356   \u001b[0m | \u001b[0m77.33    \u001b[0m | \u001b[0m0.007773 \u001b[0m |\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m0.5577   \u001b[0m | \u001b[0m116.4    \u001b[0m | \u001b[0m0.00684  \u001b[0m |\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m0.574    \u001b[0m | \u001b[0m74.1     \u001b[0m | \u001b[0m0.00745  \u001b[0m |\n",
      "| \u001b[0m25       \u001b[0m | \u001b[0m0.5656   \u001b[0m | \u001b[0m104.6    \u001b[0m | \u001b[0m0.007677 \u001b[0m |\n",
      "| \u001b[95m26       \u001b[0m | \u001b[95m0.5799   \u001b[0m | \u001b[95m90.21    \u001b[0m | \u001b[95m0.008034 \u001b[0m |\n",
      "| \u001b[0m27       \u001b[0m | \u001b[0m0.4637   \u001b[0m | \u001b[0m68.35    \u001b[0m | \u001b[0m0.004171 \u001b[0m |\n",
      "| \u001b[0m28       \u001b[0m | \u001b[0m0.5462   \u001b[0m | \u001b[0m90.21    \u001b[0m | \u001b[0m0.005336 \u001b[0m |\n",
      "| \u001b[0m29       \u001b[0m | \u001b[0m0.5661   \u001b[0m | \u001b[0m104.6    \u001b[0m | \u001b[0m0.004589 \u001b[0m |\n",
      "| \u001b[95m30       \u001b[0m | \u001b[95m0.5928   \u001b[0m | \u001b[95m90.21    \u001b[0m | \u001b[95m0.008757 \u001b[0m |\n",
      "| \u001b[0m31       \u001b[0m | \u001b[0m0.535    \u001b[0m | \u001b[0m67.16    \u001b[0m | \u001b[0m0.007734 \u001b[0m |\n",
      "| \u001b[0m32       \u001b[0m | \u001b[0m0.5329   \u001b[0m | \u001b[0m74.11    \u001b[0m | \u001b[0m0.003355 \u001b[0m |\n",
      "| \u001b[0m33       \u001b[0m | \u001b[0m0.5561   \u001b[0m | \u001b[0m74.09    \u001b[0m | \u001b[0m0.005684 \u001b[0m |\n",
      "| \u001b[0m34       \u001b[0m | \u001b[0m0.5466   \u001b[0m | \u001b[0m122.9    \u001b[0m | \u001b[0m0.008012 \u001b[0m |\n",
      "| \u001b[0m35       \u001b[0m | \u001b[0m0.5878   \u001b[0m | \u001b[0m114.6    \u001b[0m | \u001b[0m0.0008379\u001b[0m |\n",
      "| \u001b[0m36       \u001b[0m | \u001b[0m0.5656   \u001b[0m | \u001b[0m77.78    \u001b[0m | \u001b[0m0.008431 \u001b[0m |\n",
      "| \u001b[0m37       \u001b[0m | \u001b[0m0.5586   \u001b[0m | \u001b[0m114.6    \u001b[0m | \u001b[0m0.00449  \u001b[0m |\n",
      "| \u001b[0m38       \u001b[0m | \u001b[0m0.5431   \u001b[0m | \u001b[0m90.21    \u001b[0m | \u001b[0m0.006247 \u001b[0m |\n",
      "| \u001b[0m39       \u001b[0m | \u001b[0m0.5415   \u001b[0m | \u001b[0m120.8    \u001b[0m | \u001b[0m0.004851 \u001b[0m |\n",
      "| \u001b[0m40       \u001b[0m | \u001b[0m0.5738   \u001b[0m | \u001b[0m73.58    \u001b[0m | \u001b[0m0.004564 \u001b[0m |\n",
      "| \u001b[0m41       \u001b[0m | \u001b[0m0.5385   \u001b[0m | \u001b[0m88.59    \u001b[0m | \u001b[0m0.005103 \u001b[0m |\n",
      "| \u001b[0m42       \u001b[0m | \u001b[0m0.5501   \u001b[0m | \u001b[0m120.7    \u001b[0m | \u001b[0m0.001585 \u001b[0m |\n",
      "| \u001b[0m43       \u001b[0m | \u001b[0m0.5567   \u001b[0m | \u001b[0m74.1     \u001b[0m | \u001b[0m0.003795 \u001b[0m |\n",
      "| \u001b[0m44       \u001b[0m | \u001b[0m0.5589   \u001b[0m | \u001b[0m127.9    \u001b[0m | \u001b[0m0.004908 \u001b[0m |\n",
      "| \u001b[0m45       \u001b[0m | \u001b[0m0.536    \u001b[0m | \u001b[0m119.0    \u001b[0m | \u001b[0m0.002498 \u001b[0m |\n",
      "| \u001b[0m46       \u001b[0m | \u001b[0m0.5784   \u001b[0m | \u001b[0m114.6    \u001b[0m | \u001b[0m0.001378 \u001b[0m |\n",
      "| \u001b[95m47       \u001b[0m | \u001b[95m0.605    \u001b[0m | \u001b[95m73.57    \u001b[0m | \u001b[95m0.006271 \u001b[0m |\n",
      "| \u001b[0m48       \u001b[0m | \u001b[0m0.4254   \u001b[0m | \u001b[0m88.28    \u001b[0m | \u001b[0m0.009416 \u001b[0m |\n",
      "| \u001b[0m49       \u001b[0m | \u001b[0m0.5357   \u001b[0m | \u001b[0m106.0    \u001b[0m | \u001b[0m0.008698 \u001b[0m |\n",
      "| \u001b[0m50       \u001b[0m | \u001b[0m0.5157   \u001b[0m | \u001b[0m73.58    \u001b[0m | \u001b[0m0.008462 \u001b[0m |\n",
      "| \u001b[0m51       \u001b[0m | \u001b[0m0.5056   \u001b[0m | \u001b[0m71.53    \u001b[0m | \u001b[0m0.008972 \u001b[0m |\n",
      "| \u001b[0m52       \u001b[0m | \u001b[0m0.5616   \u001b[0m | \u001b[0m117.4    \u001b[0m | \u001b[0m0.005856 \u001b[0m |\n",
      "| \u001b[0m53       \u001b[0m | \u001b[0m0.3567   \u001b[0m | \u001b[0m72.68    \u001b[0m | \u001b[0m0.008902 \u001b[0m |\n",
      "| \u001b[0m54       \u001b[0m | \u001b[0m0.3677   \u001b[0m | \u001b[0m73.57    \u001b[0m | \u001b[0m0.003841 \u001b[0m |\n",
      "| \u001b[0m55       \u001b[0m | \u001b[0m0.5392   \u001b[0m | \u001b[0m74.1     \u001b[0m | \u001b[0m0.00331  \u001b[0m |\n",
      "=================================================\n",
      "|   iter    |  target   |   alpha   |\n",
      "-------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m3.808    \u001b[0m |\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m9.512    \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m7.347    \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m6.027    \u001b[0m |\n",
      "| \u001b[95m5        \u001b[0m | \u001b[95m0.2107   \u001b[0m | \u001b[95m1.645    \u001b[0m |\n",
      "| \u001b[95m6        \u001b[0m | \u001b[95m0.2108   \u001b[0m | \u001b[95m0.1001   \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m0.3405   \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m0.1735   \u001b[0m |\n",
      "| \u001b[95m9        \u001b[0m | \u001b[95m0.2108   \u001b[0m | \u001b[95m0.1001   \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m4.916    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m0.7785   \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m2.72     \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m8.429    \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m6.687    \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m5.47     \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m4.361    \u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m2.177    \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m3.262    \u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m0.2108   \u001b[0m | \u001b[0m0.116    \u001b[0m |\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m0.5182   \u001b[0m |\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m8.97     \u001b[0m |\n",
      "| \u001b[0m22       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m7.888    \u001b[0m |\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m10.0     \u001b[0m |\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m1.195    \u001b[0m |\n",
      "| \u001b[0m25       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m0.2494   \u001b[0m |\n",
      "| \u001b[0m26       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m6.357    \u001b[0m |\n",
      "| \u001b[0m27       \u001b[0m | \u001b[0m0.2108   \u001b[0m | \u001b[0m0.1003   \u001b[0m |\n",
      "| \u001b[0m28       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m7.016    \u001b[0m |\n",
      "| \u001b[0m29       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m2.448    \u001b[0m |\n",
      "| \u001b[0m30       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m4.084    \u001b[0m |\n",
      "| \u001b[0m31       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m4.637    \u001b[0m |\n",
      "| \u001b[0m32       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m1.909    \u001b[0m |\n",
      "| \u001b[0m33       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m5.193    \u001b[0m |\n",
      "| \u001b[0m34       \u001b[0m | \u001b[0m0.2108   \u001b[0m | \u001b[0m0.1025   \u001b[0m |\n",
      "| \u001b[0m35       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m5.748    \u001b[0m |\n",
      "| \u001b[0m36       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m3.535    \u001b[0m |\n",
      "| \u001b[0m37       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m2.99     \u001b[0m |\n",
      "| \u001b[0m38       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m7.617    \u001b[0m |\n",
      "| \u001b[0m39       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m8.157    \u001b[0m |\n",
      "| \u001b[0m40       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m8.697    \u001b[0m |\n",
      "| \u001b[0m41       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m9.239    \u001b[0m |\n",
      "| \u001b[0m42       \u001b[0m | \u001b[0m0.2108   \u001b[0m | \u001b[0m0.101    \u001b[0m |\n",
      "| \u001b[0m43       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m1.417    \u001b[0m |\n",
      "| \u001b[0m44       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m9.755    \u001b[0m |\n",
      "| \u001b[0m45       \u001b[0m | \u001b[0m0.2107   \u001b[0m | \u001b[0m0.9826   \u001b[0m |\n",
      "| \u001b[0m46       \u001b[0m | \u001b[0m0.2108   \u001b[0m | \u001b[0m0.1014   \u001b[0m |\n",
      "| \u001b[95m47       \u001b[0m | \u001b[95m0.2108   \u001b[0m | \u001b[95m0.1      \u001b[0m |\n",
      "| \u001b[0m48       \u001b[0m | \u001b[0m0.2108   \u001b[0m | \u001b[0m0.1008   \u001b[0m |\n",
      "\u001b[91mData point [0.1] is not unique. 1 duplicates registered. Continuing ...\u001b[0m\n",
      "| \u001b[0m49       \u001b[0m | \u001b[0m0.2108   \u001b[0m | \u001b[0m0.1      \u001b[0m |\n",
      "| \u001b[0m50       \u001b[0m | \u001b[0m0.2108   \u001b[0m | \u001b[0m0.1034   \u001b[0m |\n",
      "| \u001b[0m51       \u001b[0m | \u001b[0m0.2108   \u001b[0m | \u001b[0m0.1005   \u001b[0m |\n",
      "| \u001b[0m52       \u001b[0m | \u001b[0m0.2108   \u001b[0m | \u001b[0m0.1002   \u001b[0m |\n",
      "| \u001b[0m53       \u001b[0m | \u001b[0m0.2108   \u001b[0m | \u001b[0m0.1003   \u001b[0m |\n",
      "\u001b[91mData point [0.1] is not unique. 2 duplicates registered. Continuing ...\u001b[0m\n",
      "| \u001b[0m54       \u001b[0m | \u001b[0m0.2108   \u001b[0m | \u001b[0m0.1      \u001b[0m |\n",
      "| \u001b[0m55       \u001b[0m | \u001b[0m0.2108   \u001b[0m | \u001b[0m0.1021   \u001b[0m |\n",
      "=====================================\n",
      "R2 Train: 0.96\n",
      "R2 Test: 0.85\n",
      "{'pmTotal': 0.85}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import StackingRegressor, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Define Bayesian optimization function for each model\n",
    "def bayesian_optimization(model_func, param_space, X_train, y_train):\n",
    "    def objective_function(**params):\n",
    "        # Cast parameters to int where required\n",
    "        params = {k: int(v) if k in ['n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'n_neighbors', 'num_leaves'] else v for k, v in params.items()}\n",
    "        if 'hidden_layer_sizes' in params:\n",
    "            params['hidden_layer_sizes'] = tuple([int(params['hidden_layer_sizes']) for _ in range(2)])\n",
    "        model = model_func(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        return model.score(X_train, y_train)\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=objective_function,\n",
    "        pbounds=param_space,\n",
    "        random_state=42,\n",
    "        verbose=2,\n",
    "        allow_duplicate_points=True\n",
    "    )\n",
    "    optimizer.maximize(init_points=5, n_iter=50)\n",
    "\n",
    "    best_params = optimizer.max['params']\n",
    "    best_params = {k: int(v) if k in ['n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'n_neighbors', 'num_leaves'] else v for k, v in best_params.items()}\n",
    "    if 'hidden_layer_sizes' in best_params:\n",
    "        best_params['hidden_layer_sizes'] = tuple([int(best_params['hidden_layer_sizes']) for _ in range(2)])\n",
    "    return model_func(**best_params)\n",
    "\n",
    "# Define parameter spaces for base learners and meta-learner\n",
    "param_spaces = {\n",
    "    'rf': {\n",
    "        'n_estimators': (50, 300),\n",
    "        'max_depth': (5, 50),\n",
    "        'min_samples_split': (2, 10),\n",
    "        'min_samples_leaf': (1, 4),\n",
    "    },\n",
    "    'dt': {\n",
    "        'max_depth': (5, 50),\n",
    "        'min_samples_split': (2, 10),\n",
    "        'min_samples_leaf': (1, 4),\n",
    "    },\n",
    "    'lgbm': {\n",
    "        'num_leaves': (20, 100),\n",
    "        'max_depth': (3, 15),\n",
    "        'learning_rate': (0.01, 0.3),\n",
    "        'n_estimators': (50, 200),\n",
    "    },\n",
    "    'nn': {\n",
    "        'hidden_layer_sizes': (64, 128),\n",
    "        'learning_rate_init': (0.0001, 0.01),\n",
    "    },\n",
    "    'ridge': {\n",
    "        'alpha': (0.1, 10.0),\n",
    "    }\n",
    "}\n",
    "\n",
    "# Stacking Regressor with Bayesian Optimization\n",
    "def Stacking_Regression(X_train, X_test, y_train, y_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Tune base learners\n",
    "    rf = bayesian_optimization(RandomForestRegressor, param_spaces['rf'], X_train, y_train)\n",
    "    dt = bayesian_optimization(DecisionTreeRegressor, param_spaces['dt'], X_train, y_train)\n",
    "    lgbm = bayesian_optimization(LGBMRegressor, param_spaces['lgbm'], X_train, y_train)\n",
    "    nn = bayesian_optimization(lambda **kwargs: MLPRegressor(random_state=42, max_iter=1000, **kwargs), param_spaces['nn'], X_train_scaled, y_train)\n",
    "\n",
    "    # Tune meta-learner\n",
    "    ridge = bayesian_optimization(Ridge, param_spaces['ridge'], X_train, y_train)\n",
    "\n",
    "    # Define stacking model\n",
    "    estimators = [\n",
    "        ('rf', rf),\n",
    "        ('dt', dt),\n",
    "        ('lgbm', lgbm),\n",
    "        ('nn', nn),\n",
    "    ]\n",
    "    model = StackingRegressor(estimators=estimators, final_estimator=ridge)\n",
    "\n",
    "    # Fit and predict\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    predict_train = model.predict(X_train_scaled)\n",
    "    predict_test = model.predict(X_test_scaled)\n",
    "\n",
    "    r2_score_train = round(r2_score(y_train, predict_train), 2)\n",
    "    r2_score_test = round(r2_score(y_test, predict_test), 2)\n",
    "    print('R2 Train:', r2_score_train)\n",
    "    print('R2 Test:', r2_score_test)\n",
    "\n",
    "    return r2_score_test, model\n",
    "\n",
    "# Iterate over Palas keys\n",
    "r2_score_test_sl = {}\n",
    "for k in Palas.keys():\n",
    "    if k == \"pmTotal\":\n",
    "        print(k)\n",
    "        X = Palas[k].drop([k + \"Palas\"], axis=1)\n",
    "        y = Palas[k][k + \"Palas\"]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "        r2_score_test_sl[k], model = Stacking_Regression(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Results\n",
    "print(r2_score_test_sl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5974b33e-f928-4afd-9293-98ae41f5a2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def Permutation_Importance_Stack_Regressor(model, X_test, y_test, v, unit, ml_type, dict_col_regression):\n",
    "    # Calculate permutation importance\n",
    "    result = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42, scoring='r2')\n",
    "\n",
    "    # Extract importance values and their corresponding feature names\n",
    "    importance_values = result.importances_mean\n",
    "    importance_std = result.importances_std\n",
    "    feature_names = X_test.columns\n",
    "\n",
    "    # Sort by importance values\n",
    "    sorted_idx = importance_values.argsort()\n",
    "    sorted_importance_values = importance_values[sorted_idx]\n",
    "    sorted_importance_std = importance_std[sorted_idx]\n",
    "    sorted_feature_names = [dict_col_regression[n] for n in feature_names[sorted_idx]]\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(sorted_feature_names, sorted_importance_values, xerr=sorted_importance_std, color='#1f77b4', capsize=4)\n",
    "    plt.xlabel('Permutation Importance (R²)', fontsize=14)\n",
    "    plt.ylabel('Features', fontsize=14)\n",
    "    plt.title(f'{dict_col_regression[v + \"Palas\"]} Permutation Importance', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Adjust layout for automatic spacing\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save and show the plot\n",
    "    plt.savefig(f'D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/plots/Permutation Importance/permutation_importance_plot_{ml_type}_{v}.png', format='png')\n",
    "    plt.show()\n",
    "r2_score_test_sl[v] = Stacking_Regression(X_train, X_test, y_train, y_test, filtered_data)\n",
    "Permutation_Importance_Stack_Regressor(model, X_test, y_test, v, unit, ml_type, dict_col_regression)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d877f00c-7da7-442b-bede-7b5ab025bcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score_test_stacking_optim = {}\n",
    "print(\"========================= Stacking  Regressor  = Ridge  Regressor ================================\")\n",
    "%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Stacking_Regression_Ridge_Regression.ipynb\n",
    "\n",
    "for k,v in enumerate(Palas):\n",
    "    X = Palas[v].drop([v+\"Palas\"],axis = 1)\n",
    "    y = Palas[v][v+\"Palas\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state= 40)\n",
    "    # print(\"x train\",X_train.shape)\n",
    "    # print(\"x test\",X_test.shape)\n",
    "    # print(\"y train\",y_train.shape)\n",
    "    # print(\"y test\",y_test.shape)\n",
    "    \n",
    "    r2_score_test  =  Stacking_Regression(X_train,X_test,y_train,y_test,filtered_data,hyperparameter_dict_before_filtering[v])\n",
    "    r2_score_test_stacking_optim[v] = r2_score_test\n",
    "r2_score_test_stacking_optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f42685-5c11-4d35-80f8-6896d0ef0246",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
