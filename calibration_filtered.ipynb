{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d00ea010-c7e0-4618-a89e-912bbb72ad80",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install numpy \n",
    "# %pip install pandas\n",
    "# %pip install matplotlib\n",
    "# %pip install scikit-learn\n",
    "# %pip install seaborn\n",
    "# %pip install plotly-express\n",
    "# %pip install lightgbm\n",
    "# %pip install statsmodels\n",
    "# %pip install scikit-optimize\n",
    "# %pip install nbformat\n",
    "# %pip install bayesian-optimization==1.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07904906-7455-45ab-a7e1-574337b4a43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import pickle\n",
    "# from statsmodels.graphics.gofplots import qqplot\n",
    "from statsmodels.graphics.gofplots import qqplot_2samples\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "import sklearn.gaussian_process as gp\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "import matplotlib.dates as mdates\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "from skopt import gp_minimize\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from lightgbm import LGBMRegressor\n",
    "import nbformat\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cae7aff0-53b8-4153-baa2-1c19a1edd830",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run models/Linear_Regression.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b1c6ed7-7b3f-470e-9e4b-1fefeb1ba59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Null values are already removed from this data set ##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0cda90ae-dd92-4503-98bc-50ba4035ac91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pm1Palas</th>\n",
       "      <th>pm2_5Palas</th>\n",
       "      <th>pm4Palas</th>\n",
       "      <th>pm10Palas</th>\n",
       "      <th>pmTotalPalas</th>\n",
       "      <th>dCnPalas</th>\n",
       "      <th>dateTime</th>\n",
       "      <th>NH3_loRa</th>\n",
       "      <th>CO2_loRa</th>\n",
       "      <th>NO2_loRa</th>\n",
       "      <th>...</th>\n",
       "      <th>C2H5OH_loRa</th>\n",
       "      <th>P1_lpo_loRa</th>\n",
       "      <th>P1_ratio_loRa</th>\n",
       "      <th>P1_conc_loRa</th>\n",
       "      <th>P2_lpo_loRa</th>\n",
       "      <th>P2_ratio_loRa</th>\n",
       "      <th>P2_conc_loRa</th>\n",
       "      <th>Temperature_loRa</th>\n",
       "      <th>Pressure_loRa</th>\n",
       "      <th>Humidity_loRa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.718542</td>\n",
       "      <td>2.820000</td>\n",
       "      <td>3.085417</td>\n",
       "      <td>3.112500</td>\n",
       "      <td>3.148958</td>\n",
       "      <td>97.440417</td>\n",
       "      <td>2020-06-17T16:32:00.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>724.0</td>\n",
       "      <td>5.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.94</td>\n",
       "      <td>2051025.0</td>\n",
       "      <td>13.67</td>\n",
       "      <td>9212.48</td>\n",
       "      <td>3820665.0</td>\n",
       "      <td>25.47</td>\n",
       "      <td>28957.81</td>\n",
       "      <td>32.78</td>\n",
       "      <td>99243.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.067500</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>2.232708</td>\n",
       "      <td>2.231875</td>\n",
       "      <td>2.232708</td>\n",
       "      <td>73.388750</td>\n",
       "      <td>2020-06-17T16:33:00.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>663.0</td>\n",
       "      <td>5.11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.92</td>\n",
       "      <td>2175028.0</td>\n",
       "      <td>14.50</td>\n",
       "      <td>10095.36</td>\n",
       "      <td>4235407.0</td>\n",
       "      <td>28.24</td>\n",
       "      <td>36416.78</td>\n",
       "      <td>32.90</td>\n",
       "      <td>99247.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.904375</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.135833</td>\n",
       "      <td>2.132083</td>\n",
       "      <td>2.135833</td>\n",
       "      <td>66.655000</td>\n",
       "      <td>2020-06-17T16:33:30.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>698.0</td>\n",
       "      <td>5.11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1672212.0</td>\n",
       "      <td>11.15</td>\n",
       "      <td>6849.39</td>\n",
       "      <td>3967298.0</td>\n",
       "      <td>26.45</td>\n",
       "      <td>31447.53</td>\n",
       "      <td>32.89</td>\n",
       "      <td>99246.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.681458</td>\n",
       "      <td>1.876042</td>\n",
       "      <td>2.078333</td>\n",
       "      <td>2.072917</td>\n",
       "      <td>2.086667</td>\n",
       "      <td>58.758958</td>\n",
       "      <td>2020-06-17T16:35:30.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>681.0</td>\n",
       "      <td>4.85</td>\n",
       "      <td>...</td>\n",
       "      <td>0.85</td>\n",
       "      <td>2295499.0</td>\n",
       "      <td>15.30</td>\n",
       "      <td>11010.73</td>\n",
       "      <td>3623442.0</td>\n",
       "      <td>24.16</td>\n",
       "      <td>25849.88</td>\n",
       "      <td>33.07</td>\n",
       "      <td>99248.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.540208</td>\n",
       "      <td>1.661458</td>\n",
       "      <td>1.834792</td>\n",
       "      <td>1.824792</td>\n",
       "      <td>1.834792</td>\n",
       "      <td>57.921875</td>\n",
       "      <td>2020-06-17T16:37:00.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>686.0</td>\n",
       "      <td>4.88</td>\n",
       "      <td>...</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1544578.0</td>\n",
       "      <td>10.30</td>\n",
       "      <td>6153.25</td>\n",
       "      <td>3452410.0</td>\n",
       "      <td>23.02</td>\n",
       "      <td>23367.73</td>\n",
       "      <td>33.12</td>\n",
       "      <td>99246.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8326</th>\n",
       "      <td>4.982917</td>\n",
       "      <td>12.055000</td>\n",
       "      <td>21.223333</td>\n",
       "      <td>31.216250</td>\n",
       "      <td>37.864167</td>\n",
       "      <td>77.197500</td>\n",
       "      <td>2020-07-15T00:27:30.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.49</td>\n",
       "      <td>...</td>\n",
       "      <td>0.76</td>\n",
       "      <td>995050.0</td>\n",
       "      <td>6.63</td>\n",
       "      <td>3604.02</td>\n",
       "      <td>3534290.0</td>\n",
       "      <td>23.56</td>\n",
       "      <td>24532.03</td>\n",
       "      <td>36.60</td>\n",
       "      <td>98497.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8327</th>\n",
       "      <td>4.544583</td>\n",
       "      <td>9.235833</td>\n",
       "      <td>15.932083</td>\n",
       "      <td>17.556667</td>\n",
       "      <td>19.295000</td>\n",
       "      <td>71.974583</td>\n",
       "      <td>2020-07-15T00:29:00.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.49</td>\n",
       "      <td>...</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1126826.0</td>\n",
       "      <td>7.51</td>\n",
       "      <td>4158.83</td>\n",
       "      <td>3641409.0</td>\n",
       "      <td>24.28</td>\n",
       "      <td>26121.92</td>\n",
       "      <td>36.71</td>\n",
       "      <td>98501.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8328</th>\n",
       "      <td>4.834167</td>\n",
       "      <td>10.393750</td>\n",
       "      <td>16.646667</td>\n",
       "      <td>24.890000</td>\n",
       "      <td>29.965000</td>\n",
       "      <td>72.302500</td>\n",
       "      <td>2020-07-15T00:30:00.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.49</td>\n",
       "      <td>...</td>\n",
       "      <td>0.76</td>\n",
       "      <td>938517.0</td>\n",
       "      <td>6.26</td>\n",
       "      <td>3374.82</td>\n",
       "      <td>3211260.0</td>\n",
       "      <td>21.41</td>\n",
       "      <td>20184.45</td>\n",
       "      <td>36.71</td>\n",
       "      <td>98497.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8329</th>\n",
       "      <td>4.537917</td>\n",
       "      <td>9.765417</td>\n",
       "      <td>16.986667</td>\n",
       "      <td>18.576667</td>\n",
       "      <td>20.265833</td>\n",
       "      <td>70.609167</td>\n",
       "      <td>2020-07-15T00:30:30.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.49</td>\n",
       "      <td>...</td>\n",
       "      <td>0.76</td>\n",
       "      <td>861083.0</td>\n",
       "      <td>5.74</td>\n",
       "      <td>3068.57</td>\n",
       "      <td>4171597.0</td>\n",
       "      <td>27.81</td>\n",
       "      <td>35183.72</td>\n",
       "      <td>36.61</td>\n",
       "      <td>98495.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8330</th>\n",
       "      <td>4.430000</td>\n",
       "      <td>9.886667</td>\n",
       "      <td>17.401667</td>\n",
       "      <td>20.421250</td>\n",
       "      <td>22.930000</td>\n",
       "      <td>70.784167</td>\n",
       "      <td>2020-07-15T00:31:00.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.49</td>\n",
       "      <td>...</td>\n",
       "      <td>0.76</td>\n",
       "      <td>775231.0</td>\n",
       "      <td>5.17</td>\n",
       "      <td>2738.44</td>\n",
       "      <td>3814114.0</td>\n",
       "      <td>25.43</td>\n",
       "      <td>28850.20</td>\n",
       "      <td>36.54</td>\n",
       "      <td>98499.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8331 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pm1Palas  pm2_5Palas   pm4Palas  pm10Palas  pmTotalPalas   dCnPalas  \\\n",
       "0     2.718542    2.820000   3.085417   3.112500      3.148958  97.440417   \n",
       "1     2.067500    2.130000   2.232708   2.231875      2.232708  73.388750   \n",
       "2     1.904375    2.000000   2.135833   2.132083      2.135833  66.655000   \n",
       "3     1.681458    1.876042   2.078333   2.072917      2.086667  58.758958   \n",
       "4     1.540208    1.661458   1.834792   1.824792      1.834792  57.921875   \n",
       "...        ...         ...        ...        ...           ...        ...   \n",
       "8326  4.982917   12.055000  21.223333  31.216250     37.864167  77.197500   \n",
       "8327  4.544583    9.235833  15.932083  17.556667     19.295000  71.974583   \n",
       "8328  4.834167   10.393750  16.646667  24.890000     29.965000  72.302500   \n",
       "8329  4.537917    9.765417  16.986667  18.576667     20.265833  70.609167   \n",
       "8330  4.430000    9.886667  17.401667  20.421250     22.930000  70.784167   \n",
       "\n",
       "                   dateTime  NH3_loRa  CO2_loRa  NO2_loRa  ...  C2H5OH_loRa  \\\n",
       "0     2020-06-17T16:32:00.0      0.14     724.0      5.16  ...         0.94   \n",
       "1     2020-06-17T16:33:00.0      0.14     663.0      5.11  ...         0.92   \n",
       "2     2020-06-17T16:33:30.0      0.14     698.0      5.11  ...         0.92   \n",
       "3     2020-06-17T16:35:30.0      0.14     681.0      4.85  ...         0.85   \n",
       "4     2020-06-17T16:37:00.0      0.14     686.0      4.88  ...         0.86   \n",
       "...                     ...       ...       ...       ...  ...          ...   \n",
       "8326  2020-07-15T00:27:30.0      0.14       0.0      4.49  ...         0.76   \n",
       "8327  2020-07-15T00:29:00.0      0.14       0.0      4.49  ...         0.76   \n",
       "8328  2020-07-15T00:30:00.0      0.14       0.0      4.49  ...         0.76   \n",
       "8329  2020-07-15T00:30:30.0      0.14       0.0      4.49  ...         0.76   \n",
       "8330  2020-07-15T00:31:00.0      0.14       0.0      4.49  ...         0.76   \n",
       "\n",
       "      P1_lpo_loRa  P1_ratio_loRa  P1_conc_loRa  P2_lpo_loRa  P2_ratio_loRa  \\\n",
       "0       2051025.0          13.67       9212.48    3820665.0          25.47   \n",
       "1       2175028.0          14.50      10095.36    4235407.0          28.24   \n",
       "2       1672212.0          11.15       6849.39    3967298.0          26.45   \n",
       "3       2295499.0          15.30      11010.73    3623442.0          24.16   \n",
       "4       1544578.0          10.30       6153.25    3452410.0          23.02   \n",
       "...           ...            ...           ...          ...            ...   \n",
       "8326     995050.0           6.63       3604.02    3534290.0          23.56   \n",
       "8327    1126826.0           7.51       4158.83    3641409.0          24.28   \n",
       "8328     938517.0           6.26       3374.82    3211260.0          21.41   \n",
       "8329     861083.0           5.74       3068.57    4171597.0          27.81   \n",
       "8330     775231.0           5.17       2738.44    3814114.0          25.43   \n",
       "\n",
       "      P2_conc_loRa  Temperature_loRa  Pressure_loRa  Humidity_loRa  \n",
       "0         28957.81             32.78        99243.0           37.0  \n",
       "1         36416.78             32.90        99247.0           36.0  \n",
       "2         31447.53             32.89        99246.0           36.0  \n",
       "3         25849.88             33.07        99248.0           37.0  \n",
       "4         23367.73             33.12        99246.0           37.0  \n",
       "...            ...               ...            ...            ...  \n",
       "8326      24532.03             36.60        98497.0           39.0  \n",
       "8327      26121.92             36.71        98501.0           39.0  \n",
       "8328      20184.45             36.71        98497.0           39.0  \n",
       "8329      35183.72             36.61        98495.0           39.0  \n",
       "8330      28850.20             36.54        98499.0           39.0  \n",
       "\n",
       "[8331 rows x 24 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r'D:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\data\\final_df.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a16e9edb-c8ca-48cc-b16b-8782dd405119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pm1Palas', 'pm2_5Palas', 'pm4Palas', 'pm10Palas', 'pmTotalPalas',\n",
       "       'dCnPalas', 'dateTime', 'NH3_loRa', 'CO2_loRa', 'NO2_loRa', 'C3H8_loRa',\n",
       "       'C4H10_loRa', 'CH4_loRa', 'H2_loRa', 'C2H5OH_loRa', 'P1_lpo_loRa',\n",
       "       'P1_ratio_loRa', 'P1_conc_loRa', 'P2_lpo_loRa', 'P2_ratio_loRa',\n",
       "       'P2_conc_loRa', 'Temperature_loRa', 'Pressure_loRa', 'Humidity_loRa'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "14474c6e-8215-4d4a-9118-c28e469bb8f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c23653e2-a29d-46f5-9fbb-cc6a26b9e784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8331 entries, 0 to 8330\n",
      "Data columns (total 16 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   dateTime          8331 non-null   object \n",
      " 1   pm1Palas          8331 non-null   float64\n",
      " 2   pm2_5Palas        8331 non-null   float64\n",
      " 3   pm4Palas          8331 non-null   float64\n",
      " 4   pm10Palas         8331 non-null   float64\n",
      " 5   pmTotalPalas      8331 non-null   float64\n",
      " 6   dCnPalas          8331 non-null   float64\n",
      " 7   P1_lpo_loRa       8331 non-null   float64\n",
      " 8   P1_ratio_loRa     8331 non-null   float64\n",
      " 9   P1_conc_loRa      8331 non-null   float64\n",
      " 10  P2_lpo_loRa       8331 non-null   float64\n",
      " 11  P2_ratio_loRa     8331 non-null   float64\n",
      " 12  P2_conc_loRa      8331 non-null   float64\n",
      " 13  Temperature_loRa  8331 non-null   float64\n",
      " 14  Pressure_loRa     8331 non-null   float64\n",
      " 15  Humidity_loRa     8331 non-null   float64\n",
      "dtypes: float64(15), object(1)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df= pd.concat([data.iloc[:,6], data.iloc[:,0:6],data.iloc[:,15:data.shape[1]]],axis = 1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3d07673-2008-46ec-9f27-92b4066ba7ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dateTime</th>\n",
       "      <th>pm1Palas</th>\n",
       "      <th>pm2_5Palas</th>\n",
       "      <th>pm4Palas</th>\n",
       "      <th>pm10Palas</th>\n",
       "      <th>pmTotalPalas</th>\n",
       "      <th>dCnPalas</th>\n",
       "      <th>P1_lpo_loRa</th>\n",
       "      <th>P1_ratio_loRa</th>\n",
       "      <th>P1_conc_loRa</th>\n",
       "      <th>P2_lpo_loRa</th>\n",
       "      <th>P2_ratio_loRa</th>\n",
       "      <th>P2_conc_loRa</th>\n",
       "      <th>Temperature_loRa</th>\n",
       "      <th>Pressure_loRa</th>\n",
       "      <th>Humidity_loRa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-06-17T16:32:00.0</td>\n",
       "      <td>2.718542</td>\n",
       "      <td>2.820000</td>\n",
       "      <td>3.085417</td>\n",
       "      <td>3.112500</td>\n",
       "      <td>3.148958</td>\n",
       "      <td>97.440417</td>\n",
       "      <td>2051025.0</td>\n",
       "      <td>13.67</td>\n",
       "      <td>9212.48</td>\n",
       "      <td>3820665.0</td>\n",
       "      <td>25.47</td>\n",
       "      <td>28957.81</td>\n",
       "      <td>32.78</td>\n",
       "      <td>99243.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-06-17T16:33:00.0</td>\n",
       "      <td>2.067500</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>2.232708</td>\n",
       "      <td>2.231875</td>\n",
       "      <td>2.232708</td>\n",
       "      <td>73.388750</td>\n",
       "      <td>2175028.0</td>\n",
       "      <td>14.50</td>\n",
       "      <td>10095.36</td>\n",
       "      <td>4235407.0</td>\n",
       "      <td>28.24</td>\n",
       "      <td>36416.78</td>\n",
       "      <td>32.90</td>\n",
       "      <td>99247.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-06-17T16:33:30.0</td>\n",
       "      <td>1.904375</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.135833</td>\n",
       "      <td>2.132083</td>\n",
       "      <td>2.135833</td>\n",
       "      <td>66.655000</td>\n",
       "      <td>1672212.0</td>\n",
       "      <td>11.15</td>\n",
       "      <td>6849.39</td>\n",
       "      <td>3967298.0</td>\n",
       "      <td>26.45</td>\n",
       "      <td>31447.53</td>\n",
       "      <td>32.89</td>\n",
       "      <td>99246.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-06-17T16:35:30.0</td>\n",
       "      <td>1.681458</td>\n",
       "      <td>1.876042</td>\n",
       "      <td>2.078333</td>\n",
       "      <td>2.072917</td>\n",
       "      <td>2.086667</td>\n",
       "      <td>58.758958</td>\n",
       "      <td>2295499.0</td>\n",
       "      <td>15.30</td>\n",
       "      <td>11010.73</td>\n",
       "      <td>3623442.0</td>\n",
       "      <td>24.16</td>\n",
       "      <td>25849.88</td>\n",
       "      <td>33.07</td>\n",
       "      <td>99248.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-06-17T16:37:00.0</td>\n",
       "      <td>1.540208</td>\n",
       "      <td>1.661458</td>\n",
       "      <td>1.834792</td>\n",
       "      <td>1.824792</td>\n",
       "      <td>1.834792</td>\n",
       "      <td>57.921875</td>\n",
       "      <td>1544578.0</td>\n",
       "      <td>10.30</td>\n",
       "      <td>6153.25</td>\n",
       "      <td>3452410.0</td>\n",
       "      <td>23.02</td>\n",
       "      <td>23367.73</td>\n",
       "      <td>33.12</td>\n",
       "      <td>99246.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8326</th>\n",
       "      <td>2020-07-15T00:27:30.0</td>\n",
       "      <td>4.982917</td>\n",
       "      <td>12.055000</td>\n",
       "      <td>21.223333</td>\n",
       "      <td>31.216250</td>\n",
       "      <td>37.864167</td>\n",
       "      <td>77.197500</td>\n",
       "      <td>995050.0</td>\n",
       "      <td>6.63</td>\n",
       "      <td>3604.02</td>\n",
       "      <td>3534290.0</td>\n",
       "      <td>23.56</td>\n",
       "      <td>24532.03</td>\n",
       "      <td>36.60</td>\n",
       "      <td>98497.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8327</th>\n",
       "      <td>2020-07-15T00:29:00.0</td>\n",
       "      <td>4.544583</td>\n",
       "      <td>9.235833</td>\n",
       "      <td>15.932083</td>\n",
       "      <td>17.556667</td>\n",
       "      <td>19.295000</td>\n",
       "      <td>71.974583</td>\n",
       "      <td>1126826.0</td>\n",
       "      <td>7.51</td>\n",
       "      <td>4158.83</td>\n",
       "      <td>3641409.0</td>\n",
       "      <td>24.28</td>\n",
       "      <td>26121.92</td>\n",
       "      <td>36.71</td>\n",
       "      <td>98501.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8328</th>\n",
       "      <td>2020-07-15T00:30:00.0</td>\n",
       "      <td>4.834167</td>\n",
       "      <td>10.393750</td>\n",
       "      <td>16.646667</td>\n",
       "      <td>24.890000</td>\n",
       "      <td>29.965000</td>\n",
       "      <td>72.302500</td>\n",
       "      <td>938517.0</td>\n",
       "      <td>6.26</td>\n",
       "      <td>3374.82</td>\n",
       "      <td>3211260.0</td>\n",
       "      <td>21.41</td>\n",
       "      <td>20184.45</td>\n",
       "      <td>36.71</td>\n",
       "      <td>98497.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8329</th>\n",
       "      <td>2020-07-15T00:30:30.0</td>\n",
       "      <td>4.537917</td>\n",
       "      <td>9.765417</td>\n",
       "      <td>16.986667</td>\n",
       "      <td>18.576667</td>\n",
       "      <td>20.265833</td>\n",
       "      <td>70.609167</td>\n",
       "      <td>861083.0</td>\n",
       "      <td>5.74</td>\n",
       "      <td>3068.57</td>\n",
       "      <td>4171597.0</td>\n",
       "      <td>27.81</td>\n",
       "      <td>35183.72</td>\n",
       "      <td>36.61</td>\n",
       "      <td>98495.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8330</th>\n",
       "      <td>2020-07-15T00:31:00.0</td>\n",
       "      <td>4.430000</td>\n",
       "      <td>9.886667</td>\n",
       "      <td>17.401667</td>\n",
       "      <td>20.421250</td>\n",
       "      <td>22.930000</td>\n",
       "      <td>70.784167</td>\n",
       "      <td>775231.0</td>\n",
       "      <td>5.17</td>\n",
       "      <td>2738.44</td>\n",
       "      <td>3814114.0</td>\n",
       "      <td>25.43</td>\n",
       "      <td>28850.20</td>\n",
       "      <td>36.54</td>\n",
       "      <td>98499.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8331 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   dateTime  pm1Palas  pm2_5Palas   pm4Palas  pm10Palas  \\\n",
       "0     2020-06-17T16:32:00.0  2.718542    2.820000   3.085417   3.112500   \n",
       "1     2020-06-17T16:33:00.0  2.067500    2.130000   2.232708   2.231875   \n",
       "2     2020-06-17T16:33:30.0  1.904375    2.000000   2.135833   2.132083   \n",
       "3     2020-06-17T16:35:30.0  1.681458    1.876042   2.078333   2.072917   \n",
       "4     2020-06-17T16:37:00.0  1.540208    1.661458   1.834792   1.824792   \n",
       "...                     ...       ...         ...        ...        ...   \n",
       "8326  2020-07-15T00:27:30.0  4.982917   12.055000  21.223333  31.216250   \n",
       "8327  2020-07-15T00:29:00.0  4.544583    9.235833  15.932083  17.556667   \n",
       "8328  2020-07-15T00:30:00.0  4.834167   10.393750  16.646667  24.890000   \n",
       "8329  2020-07-15T00:30:30.0  4.537917    9.765417  16.986667  18.576667   \n",
       "8330  2020-07-15T00:31:00.0  4.430000    9.886667  17.401667  20.421250   \n",
       "\n",
       "      pmTotalPalas   dCnPalas  P1_lpo_loRa  P1_ratio_loRa  P1_conc_loRa  \\\n",
       "0         3.148958  97.440417    2051025.0          13.67       9212.48   \n",
       "1         2.232708  73.388750    2175028.0          14.50      10095.36   \n",
       "2         2.135833  66.655000    1672212.0          11.15       6849.39   \n",
       "3         2.086667  58.758958    2295499.0          15.30      11010.73   \n",
       "4         1.834792  57.921875    1544578.0          10.30       6153.25   \n",
       "...            ...        ...          ...            ...           ...   \n",
       "8326     37.864167  77.197500     995050.0           6.63       3604.02   \n",
       "8327     19.295000  71.974583    1126826.0           7.51       4158.83   \n",
       "8328     29.965000  72.302500     938517.0           6.26       3374.82   \n",
       "8329     20.265833  70.609167     861083.0           5.74       3068.57   \n",
       "8330     22.930000  70.784167     775231.0           5.17       2738.44   \n",
       "\n",
       "      P2_lpo_loRa  P2_ratio_loRa  P2_conc_loRa  Temperature_loRa  \\\n",
       "0       3820665.0          25.47      28957.81             32.78   \n",
       "1       4235407.0          28.24      36416.78             32.90   \n",
       "2       3967298.0          26.45      31447.53             32.89   \n",
       "3       3623442.0          24.16      25849.88             33.07   \n",
       "4       3452410.0          23.02      23367.73             33.12   \n",
       "...           ...            ...           ...               ...   \n",
       "8326    3534290.0          23.56      24532.03             36.60   \n",
       "8327    3641409.0          24.28      26121.92             36.71   \n",
       "8328    3211260.0          21.41      20184.45             36.71   \n",
       "8329    4171597.0          27.81      35183.72             36.61   \n",
       "8330    3814114.0          25.43      28850.20             36.54   \n",
       "\n",
       "      Pressure_loRa  Humidity_loRa  \n",
       "0           99243.0           37.0  \n",
       "1           99247.0           36.0  \n",
       "2           99246.0           36.0  \n",
       "3           99248.0           37.0  \n",
       "4           99246.0           37.0  \n",
       "...             ...            ...  \n",
       "8326        98497.0           39.0  \n",
       "8327        98501.0           39.0  \n",
       "8328        98497.0           39.0  \n",
       "8329        98495.0           39.0  \n",
       "8330        98499.0           39.0  \n",
       "\n",
       "[8331 rows x 16 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def data_checker(df):\n",
    "    data_frame = df.iloc[:,1:16]\n",
    "\n",
    "    # Palas limits are 0-100 mg/m3\n",
    "    #BME limits are :Temp -40C to 85C\n",
    "                   #:Pressure 300hPa to 1100 hPa or  300*100Pa to 1100*100 Pa\n",
    "                   #:Humidity 0% to 100%\n",
    "     #PPD42NS :Operating Temp is 0C to 45C\n",
    "    idx = data_frame[(data_frame['Temperature_loRa']>=0) & (data_frame['Temperature_loRa']<=45) &\n",
    "                    (data_frame['Pressure_loRa']>=300*100) & (data_frame['Pressure_loRa']<=1100*100) &\n",
    "                    (data_frame['Humidity_loRa']>=0) & (data_frame['Humidity_loRa']<=100) &\n",
    "                    (data_frame['pm1Palas']>=0) & (data_frame['pm2_5Palas']>=0) &\n",
    "                    (data_frame['pm4Palas']>=0) & (data_frame['pm10Palas']>=0) &\n",
    "                    (data_frame['pmTotalPalas']>=0) & (data_frame['dCnPalas']>=0)&\n",
    "                    (data_frame['pm1Palas']<=100000) & (data_frame['pm2_5Palas']<=100000) &\n",
    "                    (data_frame['pm4Palas']<=100000) & (data_frame['pm10Palas']<=100000) &\n",
    "                    (data_frame['pmTotalPalas']<=100000) & (data_frame['dCnPalas']<=100000)].index\n",
    "                    # &\n",
    "                    # (data_frame['P1_conc_loRa'] + data_frame['P2_conc_loRa']>=0) &\n",
    "                    # (data_frame['P1_conc_loRa'] + data_frame['P2_conc_loRa']<=28000)].index\n",
    "    return df.loc[idx]\n",
    "data_checker(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "35f36cd9-1ddb-423d-b407-0e584c1be1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_checker(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5da1e8ee-8f7b-45a5-994e-a65d84e7fc69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pm1Palas</th>\n",
       "      <th>pm2_5Palas</th>\n",
       "      <th>pm4Palas</th>\n",
       "      <th>pm10Palas</th>\n",
       "      <th>pmTotalPalas</th>\n",
       "      <th>dCnPalas</th>\n",
       "      <th>P1_lpo_loRa</th>\n",
       "      <th>P1_ratio_loRa</th>\n",
       "      <th>P1_conc_loRa</th>\n",
       "      <th>P2_lpo_loRa</th>\n",
       "      <th>P2_ratio_loRa</th>\n",
       "      <th>P2_conc_loRa</th>\n",
       "      <th>Temperature_loRa</th>\n",
       "      <th>Pressure_loRa</th>\n",
       "      <th>Humidity_loRa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8331.000000</td>\n",
       "      <td>8331.000000</td>\n",
       "      <td>8331.000000</td>\n",
       "      <td>8331.000000</td>\n",
       "      <td>8331.000000</td>\n",
       "      <td>8331.000000</td>\n",
       "      <td>8.331000e+03</td>\n",
       "      <td>8331.000000</td>\n",
       "      <td>8331.000000</td>\n",
       "      <td>8.331000e+03</td>\n",
       "      <td>8331.000000</td>\n",
       "      <td>8331.000000</td>\n",
       "      <td>8331.000000</td>\n",
       "      <td>8331.000000</td>\n",
       "      <td>8331.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.663561</td>\n",
       "      <td>10.574945</td>\n",
       "      <td>17.154541</td>\n",
       "      <td>21.210712</td>\n",
       "      <td>24.942589</td>\n",
       "      <td>118.392751</td>\n",
       "      <td>1.347020e+06</td>\n",
       "      <td>8.980150</td>\n",
       "      <td>5406.008278</td>\n",
       "      <td>3.946945e+06</td>\n",
       "      <td>26.312995</td>\n",
       "      <td>33441.307509</td>\n",
       "      <td>34.513146</td>\n",
       "      <td>98795.239827</td>\n",
       "      <td>46.928100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.081486</td>\n",
       "      <td>7.963725</td>\n",
       "      <td>15.343937</td>\n",
       "      <td>19.916630</td>\n",
       "      <td>24.352067</td>\n",
       "      <td>59.448553</td>\n",
       "      <td>4.568252e+05</td>\n",
       "      <td>3.045494</td>\n",
       "      <td>2356.873927</td>\n",
       "      <td>7.851387e+05</td>\n",
       "      <td>5.234295</td>\n",
       "      <td>15520.607924</td>\n",
       "      <td>2.975260</td>\n",
       "      <td>301.004359</td>\n",
       "      <td>10.636321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.511250</td>\n",
       "      <td>0.533750</td>\n",
       "      <td>0.557083</td>\n",
       "      <td>0.557083</td>\n",
       "      <td>0.557083</td>\n",
       "      <td>13.548750</td>\n",
       "      <td>5.930900e+04</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>205.700000</td>\n",
       "      <td>1.725361e+06</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>7153.120000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>91988.000000</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.130163</td>\n",
       "      <td>4.294896</td>\n",
       "      <td>5.628487</td>\n",
       "      <td>6.954312</td>\n",
       "      <td>7.750417</td>\n",
       "      <td>74.217983</td>\n",
       "      <td>1.038886e+06</td>\n",
       "      <td>6.925000</td>\n",
       "      <td>3785.255000</td>\n",
       "      <td>3.413145e+06</td>\n",
       "      <td>22.755000</td>\n",
       "      <td>22824.720000</td>\n",
       "      <td>33.050000</td>\n",
       "      <td>98643.000000</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.339167</td>\n",
       "      <td>9.227917</td>\n",
       "      <td>13.587083</td>\n",
       "      <td>15.651250</td>\n",
       "      <td>17.875833</td>\n",
       "      <td>105.651884</td>\n",
       "      <td>1.328860e+06</td>\n",
       "      <td>8.860000</td>\n",
       "      <td>5073.910000</td>\n",
       "      <td>3.743776e+06</td>\n",
       "      <td>24.960000</td>\n",
       "      <td>27713.980000</td>\n",
       "      <td>34.760000</td>\n",
       "      <td>98800.000000</td>\n",
       "      <td>46.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.435104</td>\n",
       "      <td>13.286250</td>\n",
       "      <td>21.502604</td>\n",
       "      <td>27.281667</td>\n",
       "      <td>32.010833</td>\n",
       "      <td>140.669167</td>\n",
       "      <td>1.637322e+06</td>\n",
       "      <td>10.915000</td>\n",
       "      <td>6654.515000</td>\n",
       "      <td>4.487964e+06</td>\n",
       "      <td>29.920000</td>\n",
       "      <td>41619.480000</td>\n",
       "      <td>36.425000</td>\n",
       "      <td>98951.000000</td>\n",
       "      <td>53.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.974375</td>\n",
       "      <td>57.148333</td>\n",
       "      <td>118.856667</td>\n",
       "      <td>168.116667</td>\n",
       "      <td>204.600000</td>\n",
       "      <td>383.967083</td>\n",
       "      <td>2.656432e+06</td>\n",
       "      <td>17.710000</td>\n",
       "      <td>14127.430000</td>\n",
       "      <td>6.209494e+06</td>\n",
       "      <td>41.400000</td>\n",
       "      <td>93049.530000</td>\n",
       "      <td>41.340000</td>\n",
       "      <td>99329.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          pm1Palas   pm2_5Palas     pm4Palas    pm10Palas  pmTotalPalas  \\\n",
       "count  8331.000000  8331.000000  8331.000000  8331.000000   8331.000000   \n",
       "mean      5.663561    10.574945    17.154541    21.210712     24.942589   \n",
       "std       3.081486     7.963725    15.343937    19.916630     24.352067   \n",
       "min       0.511250     0.533750     0.557083     0.557083      0.557083   \n",
       "25%       3.130163     4.294896     5.628487     6.954312      7.750417   \n",
       "50%       5.339167     9.227917    13.587083    15.651250     17.875833   \n",
       "75%       7.435104    13.286250    21.502604    27.281667     32.010833   \n",
       "max      17.974375    57.148333   118.856667   168.116667    204.600000   \n",
       "\n",
       "          dCnPalas   P1_lpo_loRa  P1_ratio_loRa  P1_conc_loRa   P2_lpo_loRa  \\\n",
       "count  8331.000000  8.331000e+03    8331.000000   8331.000000  8.331000e+03   \n",
       "mean    118.392751  1.347020e+06       8.980150   5406.008278  3.946945e+06   \n",
       "std      59.448553  4.568252e+05       3.045494   2356.873927  7.851387e+05   \n",
       "min      13.548750  5.930900e+04       0.400000    205.700000  1.725361e+06   \n",
       "25%      74.217983  1.038886e+06       6.925000   3785.255000  3.413145e+06   \n",
       "50%     105.651884  1.328860e+06       8.860000   5073.910000  3.743776e+06   \n",
       "75%     140.669167  1.637322e+06      10.915000   6654.515000  4.487964e+06   \n",
       "max     383.967083  2.656432e+06      17.710000  14127.430000  6.209494e+06   \n",
       "\n",
       "       P2_ratio_loRa  P2_conc_loRa  Temperature_loRa  Pressure_loRa  \\\n",
       "count    8331.000000   8331.000000       8331.000000    8331.000000   \n",
       "mean       26.312995  33441.307509         34.513146   98795.239827   \n",
       "std         5.234295  15520.607924          2.975260     301.004359   \n",
       "min        11.500000   7153.120000          0.000000   91988.000000   \n",
       "25%        22.755000  22824.720000         33.050000   98643.000000   \n",
       "50%        24.960000  27713.980000         34.760000   98800.000000   \n",
       "75%        29.920000  41619.480000         36.425000   98951.000000   \n",
       "max        41.400000  93049.530000         41.340000   99329.000000   \n",
       "\n",
       "       Humidity_loRa  \n",
       "count    8331.000000  \n",
       "mean       46.928100  \n",
       "std        10.636321  \n",
       "min        23.000000  \n",
       "25%        41.000000  \n",
       "50%        46.000000  \n",
       "75%        53.000000  \n",
       "max       100.000000  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bfb73d2a-843a-4083-93a3-8406ff736e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Plot the distribution of all the input parameters ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3c83f5fe-8a52-4b25-aea4-edc83cb82f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_col_names = {'pm1Palas':'PM$_{1.0}$ in $\\mu g/m^3 $ (Palas)', \n",
    "                  'pm2_5Palas': 'PM$_{2.5}$ in $\\mu g/m^3$ (Palas)',\n",
    "                  'pm4Palas': 'PM$_{4.0}$ in $\\mu g/m^3$ (Palas)',\n",
    "                  'pm10Palas': 'PM$_{10.0}$ in $\\mu g/m^3$ (Palas)',\n",
    "                  'pmTotalPalas': 'Total PM Concentration in $\\mu g/m^3$ (Palas)', \n",
    "                  'dCnPalas': 'Particle Count Density in #/cm$^{3}$ (Palas)', \n",
    "                  'P1_lpo_loRa': '> 1 μm LPO (LoRa)',\n",
    "                  'P1_ratio_loRa': '> 1 μm Ratio (LoRa)',\n",
    "                  'P1_conc_loRa':'> 1 μm Concentration in $\\mu g/m^3$ (LoRa)' , \n",
    "                  'P2_lpo_loRa': '> 2.5 μm LPO (LoRa)',\n",
    "                  'P2_ratio_loRa': '> 2.5 μm Ratio (LoRa)', \n",
    "                  'P2_conc_loRa': '> 2.5 μm Concentration in $\\mu g/m^3$ (LoRa)',\n",
    "                  'Temperature_loRa': 'Temperature in ℃ (LoRa)', \n",
    "                  'Pressure_loRa': 'Pressure in Pa (LoRa)',\n",
    "                  'Humidity_loRa': 'Humidity in % (LoRa)'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "43c45c84-645b-4cd9-bc1c-a4254813f1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_plots(df):\n",
    "    for col in df.columns[1:len(df.columns)]:\n",
    "        sns.histplot(df[col], kde = True, color = 'blue', bins = 30)\n",
    "        plt.title(f'{dict_col_names[col]} Histogram with KDE')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Count')\n",
    "        plt.show()\n",
    "#hist_plots(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a85f12a2-5670-4d87-b42b-9a498fb0a7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Plotting box plot for each parameter ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e19ce565-f907-498c-a796-b5d8d960a6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_plots(df):\n",
    "    for col in df.columns[1:len(df.columns)]:\n",
    "        plt.boxplot(df[col])\n",
    "        plt.title(f'{dict_col_names[col]} BoxPlot')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Count')\n",
    "        plt.show()\n",
    "# box_plots(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "79a6e93d-6c33-452e-9468-72968a9a5f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pm1Palas</th>\n",
       "      <th>pm2_5Palas</th>\n",
       "      <th>pm4Palas</th>\n",
       "      <th>pm10Palas</th>\n",
       "      <th>pmTotalPalas</th>\n",
       "      <th>dCnPalas</th>\n",
       "      <th>P1_lpo_loRa</th>\n",
       "      <th>P1_ratio_loRa</th>\n",
       "      <th>P1_conc_loRa</th>\n",
       "      <th>P2_lpo_loRa</th>\n",
       "      <th>P2_ratio_loRa</th>\n",
       "      <th>P2_conc_loRa</th>\n",
       "      <th>Temperature_loRa</th>\n",
       "      <th>Pressure_loRa</th>\n",
       "      <th>Humidity_loRa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.718542</td>\n",
       "      <td>2.820000</td>\n",
       "      <td>3.085417</td>\n",
       "      <td>3.112500</td>\n",
       "      <td>3.148958</td>\n",
       "      <td>97.440417</td>\n",
       "      <td>2051025.0</td>\n",
       "      <td>13.67</td>\n",
       "      <td>9212.48</td>\n",
       "      <td>3820665.0</td>\n",
       "      <td>25.47</td>\n",
       "      <td>28957.81</td>\n",
       "      <td>32.78</td>\n",
       "      <td>99243.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.067500</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>2.232708</td>\n",
       "      <td>2.231875</td>\n",
       "      <td>2.232708</td>\n",
       "      <td>73.388750</td>\n",
       "      <td>2175028.0</td>\n",
       "      <td>14.50</td>\n",
       "      <td>10095.36</td>\n",
       "      <td>4235407.0</td>\n",
       "      <td>28.24</td>\n",
       "      <td>36416.78</td>\n",
       "      <td>32.90</td>\n",
       "      <td>99247.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.904375</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.135833</td>\n",
       "      <td>2.132083</td>\n",
       "      <td>2.135833</td>\n",
       "      <td>66.655000</td>\n",
       "      <td>1672212.0</td>\n",
       "      <td>11.15</td>\n",
       "      <td>6849.39</td>\n",
       "      <td>3967298.0</td>\n",
       "      <td>26.45</td>\n",
       "      <td>31447.53</td>\n",
       "      <td>32.89</td>\n",
       "      <td>99246.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.681458</td>\n",
       "      <td>1.876042</td>\n",
       "      <td>2.078333</td>\n",
       "      <td>2.072917</td>\n",
       "      <td>2.086667</td>\n",
       "      <td>58.758958</td>\n",
       "      <td>2295499.0</td>\n",
       "      <td>15.30</td>\n",
       "      <td>11010.73</td>\n",
       "      <td>3623442.0</td>\n",
       "      <td>24.16</td>\n",
       "      <td>25849.88</td>\n",
       "      <td>33.07</td>\n",
       "      <td>99248.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.540208</td>\n",
       "      <td>1.661458</td>\n",
       "      <td>1.834792</td>\n",
       "      <td>1.824792</td>\n",
       "      <td>1.834792</td>\n",
       "      <td>57.921875</td>\n",
       "      <td>1544578.0</td>\n",
       "      <td>10.30</td>\n",
       "      <td>6153.25</td>\n",
       "      <td>3452410.0</td>\n",
       "      <td>23.02</td>\n",
       "      <td>23367.73</td>\n",
       "      <td>33.12</td>\n",
       "      <td>99246.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8326</th>\n",
       "      <td>4.982917</td>\n",
       "      <td>12.055000</td>\n",
       "      <td>21.223333</td>\n",
       "      <td>31.216250</td>\n",
       "      <td>37.864167</td>\n",
       "      <td>77.197500</td>\n",
       "      <td>995050.0</td>\n",
       "      <td>6.63</td>\n",
       "      <td>3604.02</td>\n",
       "      <td>3534290.0</td>\n",
       "      <td>23.56</td>\n",
       "      <td>24532.03</td>\n",
       "      <td>36.60</td>\n",
       "      <td>98497.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8327</th>\n",
       "      <td>4.544583</td>\n",
       "      <td>9.235833</td>\n",
       "      <td>15.932083</td>\n",
       "      <td>17.556667</td>\n",
       "      <td>19.295000</td>\n",
       "      <td>71.974583</td>\n",
       "      <td>1126826.0</td>\n",
       "      <td>7.51</td>\n",
       "      <td>4158.83</td>\n",
       "      <td>3641409.0</td>\n",
       "      <td>24.28</td>\n",
       "      <td>26121.92</td>\n",
       "      <td>36.71</td>\n",
       "      <td>98501.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8328</th>\n",
       "      <td>4.834167</td>\n",
       "      <td>10.393750</td>\n",
       "      <td>16.646667</td>\n",
       "      <td>24.890000</td>\n",
       "      <td>29.965000</td>\n",
       "      <td>72.302500</td>\n",
       "      <td>938517.0</td>\n",
       "      <td>6.26</td>\n",
       "      <td>3374.82</td>\n",
       "      <td>3211260.0</td>\n",
       "      <td>21.41</td>\n",
       "      <td>20184.45</td>\n",
       "      <td>36.71</td>\n",
       "      <td>98497.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8329</th>\n",
       "      <td>4.537917</td>\n",
       "      <td>9.765417</td>\n",
       "      <td>16.986667</td>\n",
       "      <td>18.576667</td>\n",
       "      <td>20.265833</td>\n",
       "      <td>70.609167</td>\n",
       "      <td>861083.0</td>\n",
       "      <td>5.74</td>\n",
       "      <td>3068.57</td>\n",
       "      <td>4171597.0</td>\n",
       "      <td>27.81</td>\n",
       "      <td>35183.72</td>\n",
       "      <td>36.61</td>\n",
       "      <td>98495.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8330</th>\n",
       "      <td>4.430000</td>\n",
       "      <td>9.886667</td>\n",
       "      <td>17.401667</td>\n",
       "      <td>20.421250</td>\n",
       "      <td>22.930000</td>\n",
       "      <td>70.784167</td>\n",
       "      <td>775231.0</td>\n",
       "      <td>5.17</td>\n",
       "      <td>2738.44</td>\n",
       "      <td>3814114.0</td>\n",
       "      <td>25.43</td>\n",
       "      <td>28850.20</td>\n",
       "      <td>36.54</td>\n",
       "      <td>98499.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8331 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pm1Palas  pm2_5Palas   pm4Palas  pm10Palas  pmTotalPalas   dCnPalas  \\\n",
       "0     2.718542    2.820000   3.085417   3.112500      3.148958  97.440417   \n",
       "1     2.067500    2.130000   2.232708   2.231875      2.232708  73.388750   \n",
       "2     1.904375    2.000000   2.135833   2.132083      2.135833  66.655000   \n",
       "3     1.681458    1.876042   2.078333   2.072917      2.086667  58.758958   \n",
       "4     1.540208    1.661458   1.834792   1.824792      1.834792  57.921875   \n",
       "...        ...         ...        ...        ...           ...        ...   \n",
       "8326  4.982917   12.055000  21.223333  31.216250     37.864167  77.197500   \n",
       "8327  4.544583    9.235833  15.932083  17.556667     19.295000  71.974583   \n",
       "8328  4.834167   10.393750  16.646667  24.890000     29.965000  72.302500   \n",
       "8329  4.537917    9.765417  16.986667  18.576667     20.265833  70.609167   \n",
       "8330  4.430000    9.886667  17.401667  20.421250     22.930000  70.784167   \n",
       "\n",
       "      P1_lpo_loRa  P1_ratio_loRa  P1_conc_loRa  P2_lpo_loRa  P2_ratio_loRa  \\\n",
       "0       2051025.0          13.67       9212.48    3820665.0          25.47   \n",
       "1       2175028.0          14.50      10095.36    4235407.0          28.24   \n",
       "2       1672212.0          11.15       6849.39    3967298.0          26.45   \n",
       "3       2295499.0          15.30      11010.73    3623442.0          24.16   \n",
       "4       1544578.0          10.30       6153.25    3452410.0          23.02   \n",
       "...           ...            ...           ...          ...            ...   \n",
       "8326     995050.0           6.63       3604.02    3534290.0          23.56   \n",
       "8327    1126826.0           7.51       4158.83    3641409.0          24.28   \n",
       "8328     938517.0           6.26       3374.82    3211260.0          21.41   \n",
       "8329     861083.0           5.74       3068.57    4171597.0          27.81   \n",
       "8330     775231.0           5.17       2738.44    3814114.0          25.43   \n",
       "\n",
       "      P2_conc_loRa  Temperature_loRa  Pressure_loRa  Humidity_loRa  \n",
       "0         28957.81             32.78        99243.0           37.0  \n",
       "1         36416.78             32.90        99247.0           36.0  \n",
       "2         31447.53             32.89        99246.0           36.0  \n",
       "3         25849.88             33.07        99248.0           37.0  \n",
       "4         23367.73             33.12        99246.0           37.0  \n",
       "...            ...               ...            ...            ...  \n",
       "8326      24532.03             36.60        98497.0           39.0  \n",
       "8327      26121.92             36.71        98501.0           39.0  \n",
       "8328      20184.45             36.71        98497.0           39.0  \n",
       "8329      35183.72             36.61        98495.0           39.0  \n",
       "8330      28850.20             36.54        98499.0           39.0  \n",
       "\n",
       "[8331 rows x 15 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_up = df.iloc[:,1:len(df.columns)]\n",
    "df_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7434b1c1-49b6-43aa-877d-e93993d17543",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Removing Outliers using IQR method ############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d9c5e378-37db-43dc-ac9b-44aff43e7e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist_plots(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "417f939f-8574-441a-ab5e-09b8ed767a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box_plots(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "581b55f0-bba5-4a47-af60-ec76d2131640",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### Start Running various Machine Learning Models ###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5cbce23a-3a52-4dd1-b365-68381526834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_residuals(predict_test,y_test,filtered_data):\n",
    "    idx = X_test.index\n",
    "    data_test = filtered_data.loc[idx]\n",
    "    data_test[\"dateTime\"] = pd.to_datetime(data_test[\"dateTime\"])\n",
    "    residuals = (np.array(y_test) - predict_test)**2\n",
    "    plt.figure()\n",
    "    plt.scatter(data_test[\"dateTime\"], residuals)\n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.xlabel('Date Time')\n",
    "    plt.ylabel(y_test.name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "99c21d12-850e-4f69-a3d8-14abaec77a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_col_regression = {'pm1Palas':'PM$_{1.0}$', \n",
    "                       'pm2_5Palas': 'PM$_{2.5}$',\n",
    "                       'pm4Palas': 'PM$_{4.0}$',\n",
    "                       'pm10Palas': 'PM$_{10.0}$',\n",
    "                       'pmTotalPalas': 'Total PM Concentration', \n",
    "                       'dCnPalas': 'Particle Count Density',\n",
    "                       'dateTime': 'Date Time',\n",
    "                       'P1_lpo_loRa':'> 1 μm LPO',\n",
    "                       'P1_ratio_loRa': '> 1 μm ratio', \n",
    "                       'P1_conc_loRa': '> 1 μm Concentration', \n",
    "                       'P2_lpo_loRa': '> 2.5 μm LPO', \n",
    "                       'P2_ratio_loRa': '> 2.5 μm ratio',\n",
    "                       'P2_conc_loRa': '> 2.5 μm Concentration' , \n",
    "                       'Temperature_loRa': 'Temperature', \n",
    "                       'Pressure_loRa': 'Pressure', \n",
    "                       'Humidity_loRa':'Humidity'}\n",
    "unit_regression = {'pm_conc':'($\\mu g/m^3$)',\n",
    "                   'dCn':'(#/cm$^{3}$)'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "82d80b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pm1Palas</th>\n",
       "      <th>pm2_5Palas</th>\n",
       "      <th>pm4Palas</th>\n",
       "      <th>pm10Palas</th>\n",
       "      <th>pmTotalPalas</th>\n",
       "      <th>dCnPalas</th>\n",
       "      <th>P1_lpo_loRa</th>\n",
       "      <th>P1_ratio_loRa</th>\n",
       "      <th>P1_conc_loRa</th>\n",
       "      <th>P2_lpo_loRa</th>\n",
       "      <th>P2_ratio_loRa</th>\n",
       "      <th>P2_conc_loRa</th>\n",
       "      <th>Temperature_loRa</th>\n",
       "      <th>Pressure_loRa</th>\n",
       "      <th>Humidity_loRa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8326</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8327</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8328</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8329</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8330</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8331 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pm1Palas  pm2_5Palas  pm4Palas  pm10Palas  pmTotalPalas  dCnPalas  \\\n",
       "0         True        True      True       True          True      True   \n",
       "1         True        True      True       True          True      True   \n",
       "2         True        True      True       True          True      True   \n",
       "3         True        True      True       True          True      True   \n",
       "4         True        True      True       True          True      True   \n",
       "...        ...         ...       ...        ...           ...       ...   \n",
       "8326      True        True      True       True          True      True   \n",
       "8327      True        True      True       True          True      True   \n",
       "8328      True        True      True       True          True      True   \n",
       "8329      True        True      True       True          True      True   \n",
       "8330      True        True      True       True          True      True   \n",
       "\n",
       "      P1_lpo_loRa  P1_ratio_loRa  P1_conc_loRa  P2_lpo_loRa  P2_ratio_loRa  \\\n",
       "0            True           True          True         True           True   \n",
       "1            True           True          True         True           True   \n",
       "2            True           True          True         True           True   \n",
       "3            True           True         False         True           True   \n",
       "4            True           True          True         True           True   \n",
       "...           ...            ...           ...          ...            ...   \n",
       "8326         True           True          True         True           True   \n",
       "8327         True           True          True         True           True   \n",
       "8328         True           True          True         True           True   \n",
       "8329         True           True          True         True           True   \n",
       "8330         True           True          True         True           True   \n",
       "\n",
       "      P2_conc_loRa  Temperature_loRa  Pressure_loRa  Humidity_loRa  \n",
       "0             True              True           True           True  \n",
       "1             True              True           True           True  \n",
       "2             True              True           True           True  \n",
       "3             True              True           True           True  \n",
       "4             True              True           True           True  \n",
       "...            ...               ...            ...            ...  \n",
       "8326          True              True           True           True  \n",
       "8327          True              True           True           True  \n",
       "8328          True              True           True           True  \n",
       "8329          True              True           True           True  \n",
       "8330          True              True           True           True  \n",
       "\n",
       "[8331 rows x 15 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iqr_multiplier = 1.5\n",
    "\n",
    "def filter_outliers(series):\n",
    "    q1 = series.quantile(0.25)\n",
    "    q3 = series.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - iqr_multiplier * iqr\n",
    "    upper_bound = q3 + iqr_multiplier * iqr\n",
    "    return (series >= lower_bound) & (series <= upper_bound)\n",
    "\n",
    "# Apply the filter_outliers function to each column\n",
    "filtered_df_outlier = df_up.apply(filter_outliers)\n",
    "\n",
    "filtered_df_outlier #If you check row 3 there is a mix of true and false values\n",
    "# In the next part of the code that row will be removed since there is one outlier\n",
    "# A row with atleast one outlier will be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5171fc7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dateTime</th>\n",
       "      <th>pm1Palas</th>\n",
       "      <th>pm2_5Palas</th>\n",
       "      <th>pm4Palas</th>\n",
       "      <th>pm10Palas</th>\n",
       "      <th>pmTotalPalas</th>\n",
       "      <th>dCnPalas</th>\n",
       "      <th>P1_lpo_loRa</th>\n",
       "      <th>P1_ratio_loRa</th>\n",
       "      <th>P1_conc_loRa</th>\n",
       "      <th>P2_lpo_loRa</th>\n",
       "      <th>P2_ratio_loRa</th>\n",
       "      <th>P2_conc_loRa</th>\n",
       "      <th>Temperature_loRa</th>\n",
       "      <th>Pressure_loRa</th>\n",
       "      <th>Humidity_loRa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-06-17T16:32:00.0</td>\n",
       "      <td>2.718542</td>\n",
       "      <td>2.820000</td>\n",
       "      <td>3.085417</td>\n",
       "      <td>3.112500</td>\n",
       "      <td>3.148958</td>\n",
       "      <td>97.440417</td>\n",
       "      <td>2051025.0</td>\n",
       "      <td>13.67</td>\n",
       "      <td>9212.48</td>\n",
       "      <td>3820665.0</td>\n",
       "      <td>25.47</td>\n",
       "      <td>28957.81</td>\n",
       "      <td>32.78</td>\n",
       "      <td>99243.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-06-17T16:33:00.0</td>\n",
       "      <td>2.067500</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>2.232708</td>\n",
       "      <td>2.231875</td>\n",
       "      <td>2.232708</td>\n",
       "      <td>73.388750</td>\n",
       "      <td>2175028.0</td>\n",
       "      <td>14.50</td>\n",
       "      <td>10095.36</td>\n",
       "      <td>4235407.0</td>\n",
       "      <td>28.24</td>\n",
       "      <td>36416.78</td>\n",
       "      <td>32.90</td>\n",
       "      <td>99247.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-06-17T16:33:30.0</td>\n",
       "      <td>1.904375</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.135833</td>\n",
       "      <td>2.132083</td>\n",
       "      <td>2.135833</td>\n",
       "      <td>66.655000</td>\n",
       "      <td>1672212.0</td>\n",
       "      <td>11.15</td>\n",
       "      <td>6849.39</td>\n",
       "      <td>3967298.0</td>\n",
       "      <td>26.45</td>\n",
       "      <td>31447.53</td>\n",
       "      <td>32.89</td>\n",
       "      <td>99246.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-06-17T16:37:00.0</td>\n",
       "      <td>1.540208</td>\n",
       "      <td>1.661458</td>\n",
       "      <td>1.834792</td>\n",
       "      <td>1.824792</td>\n",
       "      <td>1.834792</td>\n",
       "      <td>57.921875</td>\n",
       "      <td>1544578.0</td>\n",
       "      <td>10.30</td>\n",
       "      <td>6153.25</td>\n",
       "      <td>3452410.0</td>\n",
       "      <td>23.02</td>\n",
       "      <td>23367.73</td>\n",
       "      <td>33.12</td>\n",
       "      <td>99246.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-06-17T16:37:30.0</td>\n",
       "      <td>1.505000</td>\n",
       "      <td>1.545208</td>\n",
       "      <td>1.643333</td>\n",
       "      <td>1.640833</td>\n",
       "      <td>1.643333</td>\n",
       "      <td>58.833958</td>\n",
       "      <td>1418400.0</td>\n",
       "      <td>9.46</td>\n",
       "      <td>5508.03</td>\n",
       "      <td>3546983.0</td>\n",
       "      <td>23.65</td>\n",
       "      <td>24716.44</td>\n",
       "      <td>32.95</td>\n",
       "      <td>99244.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8326</th>\n",
       "      <td>2020-07-15T00:27:30.0</td>\n",
       "      <td>4.982917</td>\n",
       "      <td>12.055000</td>\n",
       "      <td>21.223333</td>\n",
       "      <td>31.216250</td>\n",
       "      <td>37.864167</td>\n",
       "      <td>77.197500</td>\n",
       "      <td>995050.0</td>\n",
       "      <td>6.63</td>\n",
       "      <td>3604.02</td>\n",
       "      <td>3534290.0</td>\n",
       "      <td>23.56</td>\n",
       "      <td>24532.03</td>\n",
       "      <td>36.60</td>\n",
       "      <td>98497.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8327</th>\n",
       "      <td>2020-07-15T00:29:00.0</td>\n",
       "      <td>4.544583</td>\n",
       "      <td>9.235833</td>\n",
       "      <td>15.932083</td>\n",
       "      <td>17.556667</td>\n",
       "      <td>19.295000</td>\n",
       "      <td>71.974583</td>\n",
       "      <td>1126826.0</td>\n",
       "      <td>7.51</td>\n",
       "      <td>4158.83</td>\n",
       "      <td>3641409.0</td>\n",
       "      <td>24.28</td>\n",
       "      <td>26121.92</td>\n",
       "      <td>36.71</td>\n",
       "      <td>98501.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8328</th>\n",
       "      <td>2020-07-15T00:30:00.0</td>\n",
       "      <td>4.834167</td>\n",
       "      <td>10.393750</td>\n",
       "      <td>16.646667</td>\n",
       "      <td>24.890000</td>\n",
       "      <td>29.965000</td>\n",
       "      <td>72.302500</td>\n",
       "      <td>938517.0</td>\n",
       "      <td>6.26</td>\n",
       "      <td>3374.82</td>\n",
       "      <td>3211260.0</td>\n",
       "      <td>21.41</td>\n",
       "      <td>20184.45</td>\n",
       "      <td>36.71</td>\n",
       "      <td>98497.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8329</th>\n",
       "      <td>2020-07-15T00:30:30.0</td>\n",
       "      <td>4.537917</td>\n",
       "      <td>9.765417</td>\n",
       "      <td>16.986667</td>\n",
       "      <td>18.576667</td>\n",
       "      <td>20.265833</td>\n",
       "      <td>70.609167</td>\n",
       "      <td>861083.0</td>\n",
       "      <td>5.74</td>\n",
       "      <td>3068.57</td>\n",
       "      <td>4171597.0</td>\n",
       "      <td>27.81</td>\n",
       "      <td>35183.72</td>\n",
       "      <td>36.61</td>\n",
       "      <td>98495.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8330</th>\n",
       "      <td>2020-07-15T00:31:00.0</td>\n",
       "      <td>4.430000</td>\n",
       "      <td>9.886667</td>\n",
       "      <td>17.401667</td>\n",
       "      <td>20.421250</td>\n",
       "      <td>22.930000</td>\n",
       "      <td>70.784167</td>\n",
       "      <td>775231.0</td>\n",
       "      <td>5.17</td>\n",
       "      <td>2738.44</td>\n",
       "      <td>3814114.0</td>\n",
       "      <td>25.43</td>\n",
       "      <td>28850.20</td>\n",
       "      <td>36.54</td>\n",
       "      <td>98499.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6528 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   dateTime  pm1Palas  pm2_5Palas   pm4Palas  pm10Palas  \\\n",
       "0     2020-06-17T16:32:00.0  2.718542    2.820000   3.085417   3.112500   \n",
       "1     2020-06-17T16:33:00.0  2.067500    2.130000   2.232708   2.231875   \n",
       "2     2020-06-17T16:33:30.0  1.904375    2.000000   2.135833   2.132083   \n",
       "4     2020-06-17T16:37:00.0  1.540208    1.661458   1.834792   1.824792   \n",
       "5     2020-06-17T16:37:30.0  1.505000    1.545208   1.643333   1.640833   \n",
       "...                     ...       ...         ...        ...        ...   \n",
       "8326  2020-07-15T00:27:30.0  4.982917   12.055000  21.223333  31.216250   \n",
       "8327  2020-07-15T00:29:00.0  4.544583    9.235833  15.932083  17.556667   \n",
       "8328  2020-07-15T00:30:00.0  4.834167   10.393750  16.646667  24.890000   \n",
       "8329  2020-07-15T00:30:30.0  4.537917    9.765417  16.986667  18.576667   \n",
       "8330  2020-07-15T00:31:00.0  4.430000    9.886667  17.401667  20.421250   \n",
       "\n",
       "      pmTotalPalas   dCnPalas  P1_lpo_loRa  P1_ratio_loRa  P1_conc_loRa  \\\n",
       "0         3.148958  97.440417    2051025.0          13.67       9212.48   \n",
       "1         2.232708  73.388750    2175028.0          14.50      10095.36   \n",
       "2         2.135833  66.655000    1672212.0          11.15       6849.39   \n",
       "4         1.834792  57.921875    1544578.0          10.30       6153.25   \n",
       "5         1.643333  58.833958    1418400.0           9.46       5508.03   \n",
       "...            ...        ...          ...            ...           ...   \n",
       "8326     37.864167  77.197500     995050.0           6.63       3604.02   \n",
       "8327     19.295000  71.974583    1126826.0           7.51       4158.83   \n",
       "8328     29.965000  72.302500     938517.0           6.26       3374.82   \n",
       "8329     20.265833  70.609167     861083.0           5.74       3068.57   \n",
       "8330     22.930000  70.784167     775231.0           5.17       2738.44   \n",
       "\n",
       "      P2_lpo_loRa  P2_ratio_loRa  P2_conc_loRa  Temperature_loRa  \\\n",
       "0       3820665.0          25.47      28957.81             32.78   \n",
       "1       4235407.0          28.24      36416.78             32.90   \n",
       "2       3967298.0          26.45      31447.53             32.89   \n",
       "4       3452410.0          23.02      23367.73             33.12   \n",
       "5       3546983.0          23.65      24716.44             32.95   \n",
       "...           ...            ...           ...               ...   \n",
       "8326    3534290.0          23.56      24532.03             36.60   \n",
       "8327    3641409.0          24.28      26121.92             36.71   \n",
       "8328    3211260.0          21.41      20184.45             36.71   \n",
       "8329    4171597.0          27.81      35183.72             36.61   \n",
       "8330    3814114.0          25.43      28850.20             36.54   \n",
       "\n",
       "      Pressure_loRa  Humidity_loRa  \n",
       "0           99243.0           37.0  \n",
       "1           99247.0           36.0  \n",
       "2           99246.0           36.0  \n",
       "4           99246.0           37.0  \n",
       "5           99244.0           36.0  \n",
       "...             ...            ...  \n",
       "8326        98497.0           39.0  \n",
       "8327        98501.0           39.0  \n",
       "8328        98497.0           39.0  \n",
       "8329        98495.0           39.0  \n",
       "8330        98499.0           39.0  \n",
       "\n",
       "[6528 rows x 16 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the filters for all parameters using logical AND\n",
    "final_filter = filtered_df_outlier.all(axis=1)\n",
    "\n",
    "# Apply the final filter to the original DataFrame\n",
    "filtered_data = df[final_filter]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "395fc419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pm1Palas', 'pm2_5Palas', 'pm4Palas', 'pm10Palas', 'pmTotalPalas', 'dCnPalas']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_name = list(df)\n",
    "x =[]\n",
    "y_Palas  = []\n",
    "for i in col_name:\n",
    "    if \"_loRa\" in i:\n",
    "        x.append(i)\n",
    "    if \"Palas\" in i:\n",
    "        y_Palas.append(i)\n",
    "Palas = {}\n",
    "for i in y_Palas:\n",
    "    Palas_cols = x + [i]\n",
    "    Palas[i[:-len(\"Palas\")]] = filtered_data[Palas_cols]\n",
    "y_Palas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fc400ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Actual  Predicted  Category\n",
      "0     2.718542   2.943583   Testing\n",
      "1     2.067500   2.792482   Testing\n",
      "2     1.904375   2.958355  Training\n",
      "4     1.540208   3.304568  Training\n",
      "5     1.505000   2.983411  Training\n",
      "...        ...        ...       ...\n",
      "8326  4.982917   4.578527  Training\n",
      "8327  4.544583   4.734284  Training\n",
      "8328  4.834167   4.540106  Training\n",
      "8329  4.537917   4.536078  Training\n",
      "8330  4.430000   4.441826   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "R2 value of train data 0.4\n",
      "R2 value of test data 0.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Actual  Predicted  Category\n",
      "0     2.718542   1.479338   Testing\n",
      "1     2.067500   1.765275   Testing\n",
      "2     1.904375   1.764712  Training\n",
      "4     1.540208   1.547454  Training\n",
      "5     1.505000   1.492893  Training\n",
      "...        ...        ...       ...\n",
      "8326  4.982917   4.887670  Training\n",
      "8327  4.544583   4.546332  Training\n",
      "8328  4.834167   4.760429  Training\n",
      "8329  4.537917   4.698949  Training\n",
      "8330  4.430000   4.736949   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "R2 value of train data 0.99\n",
      "R2 value of test data 0.89\n",
      "        Actual  Predicted  Category\n",
      "0     2.718542   1.472708   Testing\n",
      "1     2.067500   1.904375   Testing\n",
      "2     1.904375   1.904375  Training\n",
      "4     1.540208   1.540208  Training\n",
      "5     1.505000   1.505000  Training\n",
      "...        ...        ...       ...\n",
      "8326  4.982917   4.982917  Training\n",
      "8327  4.544583   4.544583  Training\n",
      "8328  4.834167   4.834167  Training\n",
      "8329  4.537917   4.537917  Training\n",
      "8330  4.430000   4.958750   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "R2 value of train data 1.0\n",
      "R2 value of test data 0.82\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000255 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4.948597\n",
      "        Actual  Predicted  Category\n",
      "0     2.718542   2.236055   Testing\n",
      "1     2.067500   1.974394   Testing\n",
      "2     1.904375   1.896253  Training\n",
      "4     1.540208   1.821910  Training\n",
      "5     1.505000   1.436833  Training\n",
      "...        ...        ...       ...\n",
      "8326  4.982917   4.624790  Training\n",
      "8327  4.544583   4.608983  Training\n",
      "8328  4.834167   4.605357  Training\n",
      "8329  4.537917   4.501166  Training\n",
      "8330  4.430000   4.501445   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "        Actual  Predicted  Category\n",
      "0     2.718542   1.596005   Testing\n",
      "1     2.067500   2.364021   Testing\n",
      "2     1.904375   1.781625  Training\n",
      "4     1.540208   1.519792  Training\n",
      "5     1.505000   1.482271  Training\n",
      "...        ...        ...       ...\n",
      "8326  4.982917   4.974500  Training\n",
      "8327  4.544583   4.488667  Training\n",
      "8328  4.834167   4.615290  Training\n",
      "8329  4.537917   4.664167  Training\n",
      "8330  4.430000   4.755438   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.98\n",
      "r2 test 0.88\n",
      "        Actual  Predicted  Category\n",
      "0     2.718542   2.688535   Testing\n",
      "1     2.067500   3.952737   Testing\n",
      "2     1.904375   2.004281  Training\n",
      "4     1.540208   1.698217  Training\n",
      "5     1.505000   1.203947  Training\n",
      "...        ...        ...       ...\n",
      "8326  4.982917   4.958751  Training\n",
      "8327  4.544583   5.013866  Training\n",
      "8328  4.834167   4.979101  Training\n",
      "8329  4.537917   4.615838  Training\n",
      "8330  4.430000   5.002079   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.9\n",
      "r2 test 0.84\n",
      "        Actual  Predicted  Category\n",
      "0     2.718542   2.906973   Testing\n",
      "1     2.067500   2.770715   Testing\n",
      "2     1.904375   2.977144  Training\n",
      "4     1.540208   3.320455  Training\n",
      "5     1.505000   3.017011  Training\n",
      "...        ...        ...       ...\n",
      "8326  4.982917   4.542568  Training\n",
      "8327  4.544583   4.685457  Training\n",
      "8328  4.834167   4.572033  Training\n",
      "8329  4.537917   4.532916  Training\n",
      "8330  4.430000   4.450839   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.4\n",
      "r2 test 0.39\n",
      "        Actual  Predicted  Category\n",
      "0     2.718542   5.350455   Testing\n",
      "1     2.067500   7.583627   Testing\n",
      "2     1.904375   1.533372  Training\n",
      "4     1.540208   1.392000  Training\n",
      "5     1.505000   1.342646  Training\n",
      "...        ...        ...       ...\n",
      "8326  4.982917   4.724006  Training\n",
      "8327  4.544583   5.177220  Training\n",
      "8328  4.834167   5.080000  Training\n",
      "8329  4.537917   4.856741  Training\n",
      "8330  4.430000   5.084197   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.79\n",
      "r2 test 0.65\n",
      "        Actual  Predicted  Category\n",
      "0     2.718542   1.391096   Testing\n",
      "1     2.067500   1.724064   Testing\n",
      "2     1.904375   1.677742  Training\n",
      "4     1.540208   1.435050  Training\n",
      "5     1.505000   1.392406  Training\n",
      "...        ...        ...       ...\n",
      "8326  4.982917   4.895519  Training\n",
      "8327  4.544583   4.532710  Training\n",
      "8328  4.834167   4.749444  Training\n",
      "8329  4.537917   4.686686  Training\n",
      "8330  4.430000   4.744712   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.99\n",
      "r2 test 0.89\n",
      "         Actual  Predicted  Category\n",
      "0      2.820000   4.675772   Testing\n",
      "1      2.130000   4.756707   Testing\n",
      "2      2.000000   4.655065  Training\n",
      "4      1.661458   5.279334  Training\n",
      "5      1.545208   4.563973  Training\n",
      "...         ...        ...       ...\n",
      "8326  12.055000   8.154660  Training\n",
      "8327   9.235833   8.729561  Training\n",
      "8328  10.393750   8.350060  Training\n",
      "8329   9.765417   8.125992  Training\n",
      "8330   9.886667   7.932295   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "R2 value of train data 0.41\n",
      "R2 value of test data 0.42\n",
      "         Actual  Predicted  Category\n",
      "0      2.820000   1.700957   Testing\n",
      "1      2.130000   1.907625   Testing\n",
      "2      2.000000   1.934229  Training\n",
      "4      1.661458   1.640708  Training\n",
      "5      1.545208   1.583000  Training\n",
      "...         ...        ...       ...\n",
      "8326  12.055000  11.667729  Training\n",
      "8327   9.235833  10.143008  Training\n",
      "8328  10.393750  10.576488  Training\n",
      "8329   9.765417  10.291772  Training\n",
      "8330   9.886667  11.070773   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "R2 value of train data 0.99\n",
      "R2 value of test data 0.9\n",
      "         Actual  Predicted  Category\n",
      "0      2.820000   1.612083   Testing\n",
      "1      2.130000   2.000000   Testing\n",
      "2      2.000000   2.000000  Training\n",
      "4      1.661458   1.661458  Training\n",
      "5      1.545208   1.545208  Training\n",
      "...         ...        ...       ...\n",
      "8326  12.055000  12.055000  Training\n",
      "8327   9.235833   9.235833  Training\n",
      "8328  10.393750  10.393750  Training\n",
      "8329   9.765417   9.765417  Training\n",
      "8330   9.886667  12.062083   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "R2 value of train data 1.0\n",
      "R2 value of test data 0.82\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000150 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 8.596943\n",
      "         Actual  Predicted  Category\n",
      "0      2.820000   2.251044   Testing\n",
      "1      2.130000   1.367849   Testing\n",
      "2      2.000000   1.550900  Training\n",
      "4      1.661458   1.643385  Training\n",
      "5      1.545208   1.699518  Training\n",
      "...         ...        ...       ...\n",
      "8326  12.055000  10.997597  Training\n",
      "8327   9.235833  10.796870  Training\n",
      "8328  10.393750  10.930406  Training\n",
      "8329   9.765417  10.109804  Training\n",
      "8330   9.886667  10.666022   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "         Actual  Predicted  Category\n",
      "0      2.820000   3.864521   Testing\n",
      "1      2.130000   1.943896   Testing\n",
      "2      2.000000   1.888562  Training\n",
      "4      1.661458   1.633604  Training\n",
      "5      1.545208   1.569667  Training\n",
      "...         ...        ...       ...\n",
      "8326  12.055000  11.432377  Training\n",
      "8327   9.235833   9.659250  Training\n",
      "8328  10.393750  10.618333  Training\n",
      "8329   9.765417  10.293250  Training\n",
      "8330   9.886667  10.539038   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.99\n",
      "r2 test 0.88\n",
      "         Actual  Predicted  Category\n",
      "0      2.820000   5.080629   Testing\n",
      "1      2.130000   6.397045   Testing\n",
      "2      2.000000   1.398482  Training\n",
      "4      1.661458   0.665855  Training\n",
      "5      1.545208   0.334344  Training\n",
      "...         ...        ...       ...\n",
      "8326  12.055000  10.422414  Training\n",
      "8327   9.235833  10.500060  Training\n",
      "8328  10.393750  10.825976  Training\n",
      "8329   9.765417   9.574824  Training\n",
      "8330   9.886667  10.260773   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.9\n",
      "r2 test 0.84\n",
      "         Actual  Predicted  Category\n",
      "0      2.820000   4.743659   Testing\n",
      "1      2.130000   4.680285   Testing\n",
      "2      2.000000   4.604506  Training\n",
      "4      1.661458   5.162827  Training\n",
      "5      1.545208   4.443509  Training\n",
      "...         ...        ...       ...\n",
      "8326  12.055000   8.236098  Training\n",
      "8327   9.235833   8.668571  Training\n",
      "8328  10.393750   8.276156  Training\n",
      "8329   9.765417   8.144763  Training\n",
      "8330   9.886667   7.855924   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.41\n",
      "r2 test 0.42\n",
      "         Actual  Predicted  Category\n",
      "0      2.820000  13.885324   Testing\n",
      "1      2.130000  17.850196   Testing\n",
      "2      2.000000   1.613602  Training\n",
      "4      1.661458   1.468541  Training\n",
      "5      1.545208   1.372175  Training\n",
      "...         ...        ...       ...\n",
      "8326  12.055000  10.966098  Training\n",
      "8327   9.235833  10.823066  Training\n",
      "8328  10.393750  10.688637  Training\n",
      "8329   9.765417   9.064470  Training\n",
      "8330   9.886667  10.474330   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.82\n",
      "r2 test 0.7\n",
      "         Actual  Predicted  Category\n",
      "0      2.820000   1.814505   Testing\n",
      "1      2.130000   1.809605   Testing\n",
      "2      2.000000   1.801487  Training\n",
      "4      1.661458   1.472726  Training\n",
      "5      1.545208   1.433830  Training\n",
      "...         ...        ...       ...\n",
      "8326  12.055000  11.798403  Training\n",
      "8327   9.235833  10.141916  Training\n",
      "8328  10.393750  10.675450  Training\n",
      "8329   9.765417  10.375149  Training\n",
      "8330   9.886667  11.189219   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.99\n",
      "r2 test 0.9\n",
      "         Actual  Predicted  Category\n",
      "0      3.085417   7.000631   Testing\n",
      "1      2.232708   7.429650   Testing\n",
      "2      2.135833   6.941073  Training\n",
      "4      1.834792   7.935561  Training\n",
      "5      1.643333   6.694337  Training\n",
      "...         ...        ...       ...\n",
      "8326  21.223333  12.931808  Training\n",
      "8327  15.932083  14.103576  Training\n",
      "8328  16.646667  13.430414  Training\n",
      "8329  16.986667  12.938663  Training\n",
      "8330  17.401667  12.615783   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "R2 value of train data 0.38\n",
      "R2 value of test data 0.39\n",
      "         Actual  Predicted  Category\n",
      "0      3.085417   2.267279   Testing\n",
      "1      2.232708   2.356612   Testing\n",
      "2      2.135833   2.020550  Training\n",
      "4      1.834792   1.861192  Training\n",
      "5      1.643333   1.737018  Training\n",
      "...         ...        ...       ...\n",
      "8326  21.223333  20.297058  Training\n",
      "8327  15.932083  17.763533  Training\n",
      "8328  16.646667  17.439299  Training\n",
      "8329  16.986667  16.852276  Training\n",
      "8330  17.401667  18.523804   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "R2 value of train data 0.99\n",
      "R2 value of test data 0.89\n",
      "         Actual  Predicted  Category\n",
      "0      3.085417   2.135833   Testing\n",
      "1      2.232708   2.135833   Testing\n",
      "2      2.135833   2.135833  Training\n",
      "4      1.834792   1.834792  Training\n",
      "5      1.643333   1.643333  Training\n",
      "...         ...        ...       ...\n",
      "8326  21.223333  21.223333  Training\n",
      "8327  15.932083  15.932083  Training\n",
      "8328  16.646667  16.646667  Training\n",
      "8329  16.986667  16.986667  Training\n",
      "8330  17.401667  17.089583   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "R2 value of train data 1.0\n",
      "R2 value of test data 0.75\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000279 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 13.462858\n",
      "         Actual  Predicted  Category\n",
      "0      3.085417   3.864411   Testing\n",
      "1      2.232708   2.888238   Testing\n",
      "2      2.135833   2.376417  Training\n",
      "4      1.834792   2.055732  Training\n",
      "5      1.643333   2.103662  Training\n",
      "...         ...        ...       ...\n",
      "8326  21.223333  18.269788  Training\n",
      "8327  15.932083  18.269788  Training\n",
      "8328  16.646667  18.367352  Training\n",
      "8329  16.986667  16.536550  Training\n",
      "8330  17.401667  17.475045   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "         Actual  Predicted  Category\n",
      "0      3.085417   3.498937   Testing\n",
      "1      2.232708   3.621312   Testing\n",
      "2      2.135833   2.035021  Training\n",
      "4      1.834792   1.831542  Training\n",
      "5      1.643333   1.779917  Training\n",
      "...         ...        ...       ...\n",
      "8326  21.223333  21.363375  Training\n",
      "8327  15.932083  17.445708  Training\n",
      "8328  16.646667  16.611958  Training\n",
      "8329  16.986667  16.811667  Training\n",
      "8330  17.401667  18.711497   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.98\n",
      "r2 test 0.88\n",
      "         Actual  Predicted  Category\n",
      "0      3.085417   8.201556   Testing\n",
      "1      2.232708  14.543535   Testing\n",
      "2      2.135833   4.150152  Training\n",
      "4      1.834792   0.663310  Training\n",
      "5      1.643333   0.467500  Training\n",
      "...         ...        ...       ...\n",
      "8326  21.223333  16.797232  Training\n",
      "8327  15.932083  16.020054  Training\n",
      "8328  16.646667  15.714561  Training\n",
      "8329  16.986667  15.487875  Training\n",
      "8330  17.401667  18.618888   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.92\n",
      "r2 test 0.85\n",
      "         Actual  Predicted  Category\n",
      "0      3.085417   7.202583   Testing\n",
      "1      2.232708   7.252210   Testing\n",
      "2      2.135833   6.797181  Training\n",
      "4      1.834792   7.626067  Training\n",
      "5      1.643333   6.360617  Training\n",
      "...         ...        ...       ...\n",
      "8326  21.223333  13.167869  Training\n",
      "8327  15.932083  13.987151  Training\n",
      "8328  16.646667  13.216732  Training\n",
      "8329  16.986667  12.988950  Training\n",
      "8330  17.401667  12.414060   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.38\n",
      "r2 test 0.39\n",
      "         Actual  Predicted  Category\n",
      "0      3.085417  25.885383   Testing\n",
      "1      2.232708  31.454314   Testing\n",
      "2      2.135833   1.755720  Training\n",
      "4      1.834792   1.639234  Training\n",
      "5      1.643333   1.434207  Training\n",
      "...         ...        ...       ...\n",
      "8326  21.223333  19.546312  Training\n",
      "8327  15.932083  18.725593  Training\n",
      "8328  16.646667  17.821986  Training\n",
      "8329  16.986667  14.383281  Training\n",
      "8330  17.401667  17.414180   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.81\n",
      "r2 test 0.7\n",
      "         Actual  Predicted  Category\n",
      "0      3.085417   2.503278   Testing\n",
      "1      2.232708   2.585814   Testing\n",
      "2      2.135833   1.801632  Training\n",
      "4      1.834792   1.666902  Training\n",
      "5      1.643333   1.517507  Training\n",
      "...         ...        ...       ...\n",
      "8326  21.223333  20.900806  Training\n",
      "8327  15.932083  17.803080  Training\n",
      "8328  16.646667  17.256636  Training\n",
      "8329  16.986667  16.959423  Training\n",
      "8330  17.401667  18.758818   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.99\n",
      "r2 test 0.89\n",
      "         Actual  Predicted  Category\n",
      "0      3.112500   9.060295   Testing\n",
      "1      2.231875   9.677403   Testing\n",
      "2      2.132083   9.200025  Training\n",
      "4      1.824792  10.221596  Training\n",
      "5      1.640833   8.820739  Training\n",
      "...         ...        ...       ...\n",
      "8326  31.216250  16.064403  Training\n",
      "8327  17.556667  17.443394  Training\n",
      "8328  24.890000  16.745194  Training\n",
      "8329  18.576667  16.209770  Training\n",
      "8330  20.421250  15.724858   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "R2 value of train data 0.36\n",
      "R2 value of test data 0.38\n",
      "         Actual  Predicted  Category\n",
      "0      3.112500   3.308858   Testing\n",
      "1      2.231875   2.298362   Testing\n",
      "2      2.132083   2.038196  Training\n",
      "4      1.824792   1.901667  Training\n",
      "5      1.640833   1.766279  Training\n",
      "...         ...        ...       ...\n",
      "8326  31.216250  28.549817  Training\n",
      "8327  17.556667  23.483042  Training\n",
      "8328  24.890000  25.210898  Training\n",
      "8329  18.576667  19.976575  Training\n",
      "8330  20.421250  22.237918   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "R2 value of train data 0.98\n",
      "R2 value of test data 0.86\n",
      "         Actual  Predicted  Category\n",
      "0      3.112500   1.666458   Testing\n",
      "1      2.231875   2.464583   Testing\n",
      "2      2.132083   2.132083  Training\n",
      "4      1.824792   1.824792  Training\n",
      "5      1.640833   1.640833  Training\n",
      "...         ...        ...       ...\n",
      "8326  31.216250  31.216250  Training\n",
      "8327  17.556667  17.556667  Training\n",
      "8328  24.890000  24.890000  Training\n",
      "8329  18.576667  18.576667  Training\n",
      "8330  20.421250  24.890000   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "R2 value of train data 1.0\n",
      "R2 value of test data 0.73\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000140 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 16.449515\n",
      "         Actual  Predicted  Category\n",
      "0      3.112500   3.760200   Testing\n",
      "1      2.231875   3.132916   Testing\n",
      "2      2.132083   2.456459  Training\n",
      "4      1.824792   2.102323  Training\n",
      "5      1.640833   2.447472  Training\n",
      "...         ...        ...       ...\n",
      "8326  31.216250  24.208330  Training\n",
      "8327  17.556667  23.813533  Training\n",
      "8328  24.890000  23.818768  Training\n",
      "8329  18.576667  21.165295  Training\n",
      "8330  20.421250  21.585862   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "         Actual  Predicted  Category\n",
      "0      3.112500   4.237498   Testing\n",
      "1      2.231875   2.485313   Testing\n",
      "2      2.132083   2.050437  Training\n",
      "4      1.824792   1.900250  Training\n",
      "5      1.640833   1.770187  Training\n",
      "...         ...        ...       ...\n",
      "8326  31.216250  31.306822  Training\n",
      "8327  17.556667  18.931417  Training\n",
      "8328  24.890000  24.310750  Training\n",
      "8329  18.576667  19.635208  Training\n",
      "8330  20.421250  24.792761   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.97\n",
      "r2 test 0.84\n",
      "         Actual  Predicted  Category\n",
      "0      3.112500   8.342459   Testing\n",
      "1      2.231875  10.174120   Testing\n",
      "2      2.132083   0.873860  Training\n",
      "4      1.824792   1.230321  Training\n",
      "5      1.640833  -1.925418  Training\n",
      "...         ...        ...       ...\n",
      "8326  31.216250  24.097903  Training\n",
      "8327  17.556667  24.634629  Training\n",
      "8328  24.890000  23.071020  Training\n",
      "8329  18.576667  18.358666  Training\n",
      "8330  20.421250  23.795136   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.85\n",
      "r2 test 0.8\n",
      "         Actual  Predicted  Category\n",
      "0      3.112500   9.383686   Testing\n",
      "1      2.231875   9.554251   Testing\n",
      "2      2.132083   8.992092  Training\n",
      "4      1.824792   9.846402  Training\n",
      "5      1.640833   8.367367  Training\n",
      "...         ...        ...       ...\n",
      "8326  31.216250  16.421659  Training\n",
      "8327  17.556667  17.464804  Training\n",
      "8328  24.890000  16.423731  Training\n",
      "8329  18.576667  16.272655  Training\n",
      "8330  20.421250  15.484172   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.36\n",
      "r2 test 0.38\n",
      "         Actual  Predicted  Category\n",
      "0      3.112500  32.317308   Testing\n",
      "1      2.231875  37.824981   Testing\n",
      "2      2.132083   1.839429  Training\n",
      "4      1.824792   1.734650  Training\n",
      "5      1.640833   1.518079  Training\n",
      "...         ...        ...       ...\n",
      "8326  31.216250  26.995352  Training\n",
      "8327  17.556667  25.265118  Training\n",
      "8328  24.890000  23.013094  Training\n",
      "8329  18.576667  17.469415  Training\n",
      "8330  20.421250  21.268276   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.79\n",
      "r2 test 0.68\n",
      "         Actual  Predicted  Category\n",
      "0      3.112500   3.432623   Testing\n",
      "1      2.231875   2.163867   Testing\n",
      "2      2.132083   1.739413  Training\n",
      "4      1.824792   1.478313  Training\n",
      "5      1.640833   1.469822  Training\n",
      "...         ...        ...       ...\n",
      "8326  31.216250  29.378508  Training\n",
      "8327  17.556667  22.655602  Training\n",
      "8328  24.890000  25.114240  Training\n",
      "8329  18.576667  20.017071  Training\n",
      "8330  20.421250  22.938094   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.98\n",
      "r2 test 0.86\n",
      "         Actual  Predicted  Category\n",
      "0      3.148958  10.789873   Testing\n",
      "1      2.232708  11.479979   Testing\n",
      "2      2.135833  11.283333  Training\n",
      "4      1.834792  12.301677  Training\n",
      "5      1.643333  10.813493  Training\n",
      "...         ...        ...       ...\n",
      "8326  37.864167  18.997262  Training\n",
      "8327  19.295000  20.512738  Training\n",
      "8328  29.965000  19.799191  Training\n",
      "8329  20.265833  19.296239  Training\n",
      "8330  22.930000  18.628802   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "R2 value of train data 0.33\n",
      "R2 value of test data 0.35\n",
      "         Actual  Predicted  Category\n",
      "0      3.148958   4.555928   Testing\n",
      "1      2.232708   2.518804   Testing\n",
      "2      2.135833   2.023996  Training\n",
      "4      1.834792   1.969333  Training\n",
      "5      1.643333   1.782907  Training\n",
      "...         ...        ...       ...\n",
      "8326  37.864167  33.892725  Training\n",
      "8327  19.295000  29.520333  Training\n",
      "8328  29.965000  30.183320  Training\n",
      "8329  20.265833  23.265920  Training\n",
      "8330  22.930000  27.693917   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "R2 value of train data 0.97\n",
      "R2 value of test data 0.79\n",
      "         Actual  Predicted  Category\n",
      "0      3.148958   7.245833   Testing\n",
      "1      2.232708   3.017292   Testing\n",
      "2      2.135833   2.135833  Training\n",
      "4      1.834792   1.834792  Training\n",
      "5      1.643333   1.643333  Training\n",
      "...         ...        ...       ...\n",
      "8326  37.864167  37.864167  Training\n",
      "8327  19.295000  19.295000  Training\n",
      "8328  29.965000  29.965000  Training\n",
      "8329  20.265833  20.265833  Training\n",
      "8330  22.930000  20.708750   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "R2 value of train data 1.0\n",
      "R2 value of test data 0.61\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000143 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 19.166156\n",
      "         Actual  Predicted  Category\n",
      "0      3.148958   3.663429   Testing\n",
      "1      2.232708   3.143709   Testing\n",
      "2      2.135833   2.392465  Training\n",
      "4      1.834792   2.007192  Training\n",
      "5      1.643333   2.709317  Training\n",
      "...         ...        ...       ...\n",
      "8326  37.864167  31.764484  Training\n",
      "8327  19.295000  29.771758  Training\n",
      "8328  29.965000  29.267233  Training\n",
      "8329  20.265833  25.988934  Training\n",
      "8330  22.930000  24.004147   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "         Actual  Predicted  Category\n",
      "0      3.148958   4.943437   Testing\n",
      "1      2.232708   2.277125   Testing\n",
      "2      2.135833   2.072812  Training\n",
      "4      1.834792   1.866750  Training\n",
      "5      1.643333   1.741500  Training\n",
      "...         ...        ...       ...\n",
      "8326  37.864167  34.159694  Training\n",
      "8327  19.295000  25.957042  Training\n",
      "8328  29.965000  30.755940  Training\n",
      "8329  20.265833  25.414277  Training\n",
      "8330  22.930000  31.548792   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.96\n",
      "r2 test 0.77\n",
      "         Actual  Predicted  Category\n",
      "0      3.148958   9.132846   Testing\n",
      "1      2.232708  14.430174   Testing\n",
      "2      2.135833   2.271198  Training\n",
      "4      1.834792   2.758353  Training\n",
      "5      1.643333  -1.854922  Training\n",
      "...         ...        ...       ...\n",
      "8326  37.864167  28.767522  Training\n",
      "8327  19.295000  30.657022  Training\n",
      "8328  29.965000  27.094975  Training\n",
      "8329  20.265833  19.135470  Training\n",
      "8330  22.930000  24.358184   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.8\n",
      "r2 test 0.74\n",
      "         Actual  Predicted  Category\n",
      "0      3.148958  11.219700   Testing\n",
      "1      2.232708  11.435758   Testing\n",
      "2      2.135833  11.023699  Training\n",
      "4      1.834792  11.892372  Training\n",
      "5      1.643333  10.271082  Training\n",
      "...         ...        ...       ...\n",
      "8326  37.864167  19.456673  Training\n",
      "8327  19.295000  20.695364  Training\n",
      "8328  29.965000  19.387322  Training\n",
      "8329  20.265833  19.366741  Training\n",
      "8330  22.930000  18.370007   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.33\n",
      "r2 test 0.35\n",
      "         Actual  Predicted  Category\n",
      "0      3.148958  37.342998   Testing\n",
      "1      2.232708  43.220893   Testing\n",
      "2      2.135833   1.829777  Training\n",
      "4      1.834792   1.734513  Training\n",
      "5      1.643333   1.495286  Training\n",
      "...         ...        ...       ...\n",
      "8326  37.864167  36.274299  Training\n",
      "8327  19.295000  34.090161  Training\n",
      "8328  29.965000  27.542607  Training\n",
      "8329  20.265833  20.926920  Training\n",
      "8330  22.930000  25.285516   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.76\n",
      "r2 test 0.62\n",
      "         Actual  Predicted  Category\n",
      "0      3.148958   4.201957   Testing\n",
      "1      2.232708   2.113039   Testing\n",
      "2      2.135833   1.721445  Training\n",
      "4      1.834792   1.701794  Training\n",
      "5      1.643333   1.468882  Training\n",
      "...         ...        ...       ...\n",
      "8326  37.864167  33.717193  Training\n",
      "8327  19.295000  29.266225  Training\n",
      "8328  29.965000  30.201447  Training\n",
      "8329  20.265833  23.607059  Training\n",
      "8330  22.930000  28.497162   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.97\n",
      "r2 test 0.8\n",
      "         Actual  Predicted  Category\n",
      "0     97.440417  85.565107   Testing\n",
      "1     73.388750  80.828618   Testing\n",
      "2     66.655000  87.226485  Training\n",
      "4     57.921875  92.867056  Training\n",
      "5     58.833958  88.722764  Training\n",
      "...         ...        ...       ...\n",
      "8326  77.197500  98.556964  Training\n",
      "8327  71.974583  99.663367  Training\n",
      "8328  72.302500  95.603308  Training\n",
      "8329  70.609167  97.752273  Training\n",
      "8330  70.784167  96.426185   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "R2 value of train data 0.2\n",
      "R2 value of test data 0.19\n",
      "         Actual  Predicted  Category\n",
      "0     97.440417  63.432797   Testing\n",
      "1     73.388750  75.015079   Testing\n",
      "2     66.655000  75.856671  Training\n",
      "4     57.921875  61.735583  Training\n",
      "5     58.833958  58.975918  Training\n",
      "...         ...        ...       ...\n",
      "8326  77.197500  76.318929  Training\n",
      "8327  71.974583  74.891935  Training\n",
      "8328  72.302500  73.874370  Training\n",
      "8329  70.609167  72.874756  Training\n",
      "8330  70.784167  76.119486   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "R2 value of train data 0.98\n",
      "R2 value of test data 0.86\n",
      "         Actual  Predicted  Category\n",
      "0     97.440417  59.072708   Testing\n",
      "1     73.388750  59.072708   Testing\n",
      "2     66.655000  66.655000  Training\n",
      "4     57.921875  57.921875  Training\n",
      "5     58.833958  58.833958  Training\n",
      "...         ...        ...       ...\n",
      "8326  77.197500  77.197500  Training\n",
      "8327  71.974583  71.974583  Training\n",
      "8328  72.302500  72.302500  Training\n",
      "8329  70.609167  70.609167  Training\n",
      "8330  70.784167  69.029167   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "R2 value of train data 1.0\n",
      "R2 value of test data 0.71\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000142 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 108.936655\n",
      "         Actual  Predicted  Category\n",
      "0     97.440417  61.383082   Testing\n",
      "1     73.388750  65.844884   Testing\n",
      "2     66.655000  65.325946  Training\n",
      "4     57.921875  68.715226  Training\n",
      "5     58.833958  59.892456  Training\n",
      "...         ...        ...       ...\n",
      "8326  77.197500  79.025002  Training\n",
      "8327  71.974583  80.614105  Training\n",
      "8328  72.302500  80.936302  Training\n",
      "8329  70.609167  77.447165  Training\n",
      "8330  70.784167  78.052940   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "         Actual  Predicted  Category\n",
      "0     97.440417  59.742417   Testing\n",
      "1     73.388750  84.253938   Testing\n",
      "2     66.655000  86.492188  Training\n",
      "4     57.921875  60.037646  Training\n",
      "5     58.833958  63.438108  Training\n",
      "...         ...        ...       ...\n",
      "8326  77.197500  76.276417  Training\n",
      "8327  71.974583  75.141245  Training\n",
      "8328  72.302500  73.882060  Training\n",
      "8329  70.609167  70.594887  Training\n",
      "8330  70.784167  77.544905   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.97\n",
      "r2 test 0.82\n",
      "         Actual  Predicted  Category\n",
      "0     97.440417  71.122359   Testing\n",
      "1     73.388750  93.453297   Testing\n",
      "2     66.655000  76.820736  Training\n",
      "4     57.921875  58.430420  Training\n",
      "5     58.833958  55.266813  Training\n",
      "...         ...        ...       ...\n",
      "8326  77.197500  78.634374  Training\n",
      "8327  71.974583  77.528913  Training\n",
      "8328  72.302500  82.593083  Training\n",
      "8329  70.609167  66.079590  Training\n",
      "8330  70.784167  72.780405   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.83\n",
      "r2 test 0.75\n",
      "         Actual  Predicted  Category\n",
      "0     97.440417  83.909986   Testing\n",
      "1     73.388750  80.390222   Testing\n",
      "2     66.655000  88.146940  Training\n",
      "4     57.921875  93.990795  Training\n",
      "5     58.833958  90.511018  Training\n",
      "...         ...        ...       ...\n",
      "8326  77.197500  96.863524  Training\n",
      "8327  71.974583  98.167033  Training\n",
      "8328  72.302500  97.114070  Training\n",
      "8329  70.609167  97.548549  Training\n",
      "8330  70.784167  97.112084   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.2\n",
      "r2 test 0.19\n",
      "         Actual   Predicted  Category\n",
      "0     97.440417   88.712662   Testing\n",
      "1     73.388750  130.848920   Testing\n",
      "2     66.655000   55.137653  Training\n",
      "4     57.921875   57.835730  Training\n",
      "5     58.833958   49.729375  Training\n",
      "...         ...         ...       ...\n",
      "8326  77.197500   77.110342  Training\n",
      "8327  71.974583   93.101905  Training\n",
      "8328  72.302500   89.251507  Training\n",
      "8329  70.609167   91.932406  Training\n",
      "8330  70.784167   90.061381   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.74\n",
      "r2 test 0.58\n",
      "         Actual  Predicted  Category\n",
      "0     97.440417  60.657037   Testing\n",
      "1     73.388750  73.677968   Testing\n",
      "2     66.655000  75.062759  Training\n",
      "4     57.921875  59.134528  Training\n",
      "5     58.833958  57.193538  Training\n",
      "...         ...        ...       ...\n",
      "8326  77.197500  74.800292  Training\n",
      "8327  71.974583  73.149045  Training\n",
      "8328  72.302500  72.127869  Training\n",
      "8329  70.609167  70.781033  Training\n",
      "8330  70.784167  74.357627   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.98\n",
      "r2 test 0.86\n",
      "after filtering\n",
      "lr {'pm1': 0.39, 'pm2_5': 0.42, 'pm4': 0.39, 'pm10': 0.38, 'pmTotal': 0.35, 'dCn': 0.19}\n",
      "rf {'pm1': 0.89, 'pm2_5': 0.9, 'pm4': 0.89, 'pm10': 0.86, 'pmTotal': 0.79, 'dCn': 0.86}\n",
      "dt {'pm1': 0.82, 'pm2_5': 0.82, 'pm4': 0.75, 'pm10': 0.73, 'pmTotal': 0.61, 'dCn': 0.71}\n",
      "lgbm {'pm1': 0.86, 'pm2_5': 0.87, 'pm4': 0.86, 'pm10': 0.84, 'pmTotal': 0.77, 'dCn': 0.82}\n",
      "br {'pm1': 0.88, 'pm2_5': 0.88, 'pm4': 0.88, 'pm10': 0.84, 'pmTotal': 0.77, 'dCn': 0.82}\n",
      "nn {'pm1': 0.84, 'pm2_5': 0.84, 'pm4': 0.85, 'pm10': 0.8, 'pmTotal': 0.74, 'dCn': 0.75}\n",
      "rr {'pm1': 0.39, 'pm2_5': 0.42, 'pm4': 0.39, 'pm10': 0.38, 'pmTotal': 0.35, 'dCn': 0.19}\n",
      "knn {'pm1': 0.65, 'pm2_5': 0.7, 'pm4': 0.7, 'pm10': 0.68, 'pmTotal': 0.62, 'dCn': 0.58}\n",
      "sl {'pm1': 0.89, 'pm2_5': 0.9, 'pm4': 0.89, 'pm10': 0.86, 'pmTotal': 0.8, 'dCn': 0.86}\n"
     ]
    }
   ],
   "source": [
    "r2_score_test_lr ={}\n",
    "r2_score_test_nn ={}\n",
    "r2_score_test_rf ={}\n",
    "r2_score_test_br ={}\n",
    "r2_score_test_dt ={}\n",
    "r2_score_test_lgbm ={}\n",
    "r2_score_test_gr = {}\n",
    "r2_score_test_rr = {}\n",
    "r2_score_test_knn = {}\n",
    "r2_score_test_sl = {}\n",
    "\n",
    "\n",
    "trained_model_nn={}\n",
    "trained_model_rf={}\n",
    "trained_model_br={}\n",
    "trained_model_dt={}\n",
    "trained_model_lgbm={}\n",
    "\n",
    "hyperparameters_nn={}\n",
    "hyperparameters_rf={}\n",
    "hyperparameters_br={}\n",
    "hyperparameters_dt={}\n",
    "hyperparameters_lgbm={} \n",
    "\n",
    "\n",
    "\n",
    "%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Linear_Regression.ipynb\n",
    "%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Random_Forest_Regression.ipynb\n",
    "%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Decision_Tree_Regression.ipynb\n",
    "%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Ensemble_Bagging_Regression.ipynb\n",
    "%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/LGBM_Regression.ipynb\n",
    "%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Gaussian_Regression.ipynb\n",
    "%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Ridge_Regression.ipynb\n",
    "%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/KNN_Regression.ipynb\n",
    "%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Neural_Network_Regression.ipynb\n",
    "%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Stacking_Regression.ipynb\n",
    "# %run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Random_Forest_Regression_Optimized.ipynb\n",
    "# %run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Neural_Network_Regression_Optimized.ipynb\n",
    "# %run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Decision_Tree_Regression_Optimized.ipynb\n",
    "# %run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Ensemble_Bagging_Regression_Optimized.ipynb\n",
    "# %run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/LGBM_Regression_Optimized.ipynb\n",
    "\n",
    "for k,v in enumerate(Palas):\n",
    "    X = Palas[v].drop([v+\"Palas\"],axis = 1)\n",
    "    y = Palas[v][v+\"Palas\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state= 40)\n",
    "    # print(\"x train\",X_train.shape)\n",
    "    # print(\"x test\",X_test.shape)\n",
    "    # print(\"y train\",y_train.shape)\n",
    "    # print(\"y test\",y_test.shape)\n",
    "    \n",
    "    r2_score_test_lr[v] =  Linear_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "    r2_score_test_rf[v] = Random_Forest_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "    r2_score_test_dt[v] = Decision_Tree_Regression(X_train,X_test,y_train,y_test,filtered_data)   \n",
    "    r2_score_test_lgbm[v]  =  LGBM_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "    r2_score_test_br[v] = Ensemble_Bagging_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "    r2_score_test_nn[v] = Neural_Network_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "    ##r2_score_test_gr[v] =  Gaussian_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "    r2_score_test_rr[v] = Ridge_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "    r2_score_test_knn[v] = KNN_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "    r2_score_test_sl[v] = Stacking_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "    \n",
    "\n",
    "print('after filtering')\n",
    "\n",
    "print('lr',r2_score_test_lr)\n",
    "print('rf',r2_score_test_rf)\n",
    "print('dt',r2_score_test_dt)   \n",
    "print('lgbm',r2_score_test_lgbm)\n",
    "print('br',r2_score_test_br)\n",
    "print('nn',r2_score_test_nn)\n",
    "##print('gr',r2_score_test_gr[v])\n",
    "print('rr',r2_score_test_rr)\n",
    "print('knn',r2_score_test_knn)\n",
    "print('sl',r2_score_test_sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "129ff19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dict ={key: [r2_score_test_lr.get(key),r2_score_test_rf.get(key), r2_score_test_br.get(key),r2_score_test_lgbm.get(key), r2_score_test_nn.get(key), r2_score_test_dt.get(key),r2_score_test_rr.get(key),r2_score_test_knn.get(key),r2_score_test_sl.get(key)] for key in r2_score_test_nn}\n",
    "r2_values = combined_dict\n",
    "# Creating a DataFrame from the dictionary\n",
    "models = [\"Linear Regressor\",\"Random Forest\", \"Ensemble Bagging\",\"LGBM\", \"Neural Network\", \"Decision Tree\",\"Ridge Regressor\",\"KNN\",\"Super Learner\"]\n",
    "r2_values\n",
    "df_updated_r2 = pd.DataFrame(r2_values, index=models)\n",
    "df_updated_r2.index.name = 'Model'\n",
    "df_updated_r2.columns.name = 'Target Variable'\n",
    "\n",
    "df_updated_r2\n",
    "df_updated_r2.to_csv(\"After_filtering_R2.csv\",index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "45b99f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score_test_rf_optim ={}\n",
    "r2_score_test_nn_optim ={}\n",
    "r2_score_test_br_optim ={}\n",
    "r2_score_test_lgbm_optim ={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "03f37136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= Random Forest Regressor ================================\n",
      "|   iter    |  target   | max_depth | min_sa... | min_sa... | n_esti... |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.8763   \u001b[0m | \u001b[0m18.72    \u001b[0m | \u001b[0m3.861    \u001b[0m | \u001b[0m6.822    \u001b[0m | \u001b[0m59.04    \u001b[0m |\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m0.8783   \u001b[0m | \u001b[95m15.59    \u001b[0m | \u001b[95m3.584    \u001b[0m | \u001b[95m5.501    \u001b[0m | \u001b[95m90.26    \u001b[0m |\n",
      "| \u001b[95m3        \u001b[0m | \u001b[95m0.8795   \u001b[0m | \u001b[95m29.09    \u001b[0m | \u001b[95m2.534    \u001b[0m | \u001b[95m8.334    \u001b[0m | \u001b[95m57.6     \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.8607   \u001b[0m | \u001b[0m19.2     \u001b[0m | \u001b[0m4.702    \u001b[0m | \u001b[0m2.568    \u001b[0m | \u001b[0m17.84    \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.6529   \u001b[0m | \u001b[0m5.505    \u001b[0m | \u001b[0m4.33     \u001b[0m | \u001b[0m8.225    \u001b[0m | \u001b[0m88.3     \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.8688   \u001b[0m | \u001b[0m19.91    \u001b[0m | \u001b[0m4.214    \u001b[0m | \u001b[0m7.34     \u001b[0m | \u001b[0m56.12    \u001b[0m |\n",
      "| \u001b[95m7        \u001b[0m | \u001b[95m0.8869   \u001b[0m | \u001b[95m23.73    \u001b[0m | \u001b[95m2.976    \u001b[0m | \u001b[95m3.325    \u001b[0m | \u001b[95m91.8     \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.8781   \u001b[0m | \u001b[0m23.27    \u001b[0m | \u001b[0m3.072    \u001b[0m | \u001b[0m4.572    \u001b[0m | \u001b[0m81.08    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.8626   \u001b[0m | \u001b[0m27.64    \u001b[0m | \u001b[0m5.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m68.05    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.8638   \u001b[0m | \u001b[0m21.62    \u001b[0m | \u001b[0m5.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m100.0    \u001b[0m |\n",
      "| \u001b[95m11       \u001b[0m | \u001b[95m0.8909   \u001b[0m | \u001b[95m30.0     \u001b[0m | \u001b[95m1.0      \u001b[0m | \u001b[95m2.0      \u001b[0m | \u001b[95m46.16    \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.8614   \u001b[0m | \u001b[0m30.0     \u001b[0m | \u001b[0m5.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m40.02    \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.8892   \u001b[0m | \u001b[0m19.56    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m41.72    \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.829    \u001b[0m | \u001b[0m8.74     \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m46.95    \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.8884   \u001b[0m | \u001b[0m25.37    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m33.39    \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m0.8771   \u001b[0m | \u001b[0m16.94    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m9.334    \u001b[0m | \u001b[0m32.44    \u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m0.8753   \u001b[0m | \u001b[0m30.0     \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m24.74    \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m0.8635   \u001b[0m | \u001b[0m30.0     \u001b[0m | \u001b[0m5.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m87.41    \u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m0.6504   \u001b[0m | \u001b[0m5.16     \u001b[0m | \u001b[0m3.636    \u001b[0m | \u001b[0m9.266    \u001b[0m | \u001b[0m23.25    \u001b[0m |\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.8484   \u001b[0m | \u001b[0m30.0     \u001b[0m | \u001b[0m5.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m10.0     \u001b[0m |\n",
      "=========================================================================\n",
      "Optimal Parameters: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 46}\n",
      "R2 Score Train: 0.99\n",
      "R2 Score Test: 0.89\n",
      "|   iter    |  target   | max_depth | min_sa... | min_sa... | n_esti... |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.8835   \u001b[0m | \u001b[0m18.72    \u001b[0m | \u001b[0m3.861    \u001b[0m | \u001b[0m6.822    \u001b[0m | \u001b[0m59.04    \u001b[0m |\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m0.8864   \u001b[0m | \u001b[95m15.59    \u001b[0m | \u001b[95m3.584    \u001b[0m | \u001b[95m5.501    \u001b[0m | \u001b[95m90.26    \u001b[0m |\n",
      "| \u001b[95m3        \u001b[0m | \u001b[95m0.8879   \u001b[0m | \u001b[95m29.09    \u001b[0m | \u001b[95m2.534    \u001b[0m | \u001b[95m8.334    \u001b[0m | \u001b[95m57.6     \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.8727   \u001b[0m | \u001b[0m19.2     \u001b[0m | \u001b[0m4.702    \u001b[0m | \u001b[0m2.568    \u001b[0m | \u001b[0m17.84    \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.6949   \u001b[0m | \u001b[0m5.505    \u001b[0m | \u001b[0m4.33     \u001b[0m | \u001b[0m8.225    \u001b[0m | \u001b[0m88.3     \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.8778   \u001b[0m | \u001b[0m19.91    \u001b[0m | \u001b[0m4.214    \u001b[0m | \u001b[0m7.34     \u001b[0m | \u001b[0m56.12    \u001b[0m |\n",
      "| \u001b[95m7        \u001b[0m | \u001b[95m0.894    \u001b[0m | \u001b[95m23.73    \u001b[0m | \u001b[95m2.971    \u001b[0m | \u001b[95m3.305    \u001b[0m | \u001b[95m91.78    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.8937   \u001b[0m | \u001b[0m23.3     \u001b[0m | \u001b[0m2.964    \u001b[0m | \u001b[0m4.513    \u001b[0m | \u001b[0m81.05    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.8734   \u001b[0m | \u001b[0m27.95    \u001b[0m | \u001b[0m5.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m68.9     \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.8746   \u001b[0m | \u001b[0m21.71    \u001b[0m | \u001b[0m5.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m100.0    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.8788   \u001b[0m | \u001b[0m29.92    \u001b[0m | \u001b[0m4.054    \u001b[0m | \u001b[0m9.294    \u001b[0m | \u001b[0m88.34    \u001b[0m |\n",
      "| \u001b[95m12       \u001b[0m | \u001b[95m0.896    \u001b[0m | \u001b[95m30.0     \u001b[0m | \u001b[95m1.0      \u001b[0m | \u001b[95m2.0      \u001b[0m | \u001b[95m45.02    \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.8731   \u001b[0m | \u001b[0m30.0     \u001b[0m | \u001b[0m5.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m37.52    \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.8941   \u001b[0m | \u001b[0m18.92    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m40.71    \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.761    \u001b[0m | \u001b[0m6.995    \u001b[0m | \u001b[0m3.514    \u001b[0m | \u001b[0m2.678    \u001b[0m | \u001b[0m42.56    \u001b[0m |\n",
      "| \u001b[95m16       \u001b[0m | \u001b[95m0.8963   \u001b[0m | \u001b[95m24.92    \u001b[0m | \u001b[95m1.0      \u001b[0m | \u001b[95m2.0      \u001b[0m | \u001b[95m32.62    \u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m0.881    \u001b[0m | \u001b[0m30.0     \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m22.33    \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m0.8824   \u001b[0m | \u001b[0m18.57    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m29.16    \u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m0.6715   \u001b[0m | \u001b[0m5.0      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m |\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.8649   \u001b[0m | \u001b[0m30.0     \u001b[0m | \u001b[0m5.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m10.0     \u001b[0m |\n",
      "=========================================================================\n",
      "Optimal Parameters: {'max_depth': 24, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 32}\n",
      "R2 Score Train: 0.99\n",
      "R2 Score Test: 0.9\n",
      "|   iter    |  target   | max_depth | min_sa... | min_sa... | n_esti... |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.8785   \u001b[0m | \u001b[0m18.72    \u001b[0m | \u001b[0m3.861    \u001b[0m | \u001b[0m6.822    \u001b[0m | \u001b[0m59.04    \u001b[0m |\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m0.8802   \u001b[0m | \u001b[95m15.59    \u001b[0m | \u001b[95m3.584    \u001b[0m | \u001b[95m5.501    \u001b[0m | \u001b[95m90.26    \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.8789   \u001b[0m | \u001b[0m29.09    \u001b[0m | \u001b[0m2.534    \u001b[0m | \u001b[0m8.334    \u001b[0m | \u001b[0m57.6     \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.8757   \u001b[0m | \u001b[0m19.2     \u001b[0m | \u001b[0m4.702    \u001b[0m | \u001b[0m2.568    \u001b[0m | \u001b[0m17.84    \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.684    \u001b[0m | \u001b[0m5.505    \u001b[0m | \u001b[0m4.33     \u001b[0m | \u001b[0m8.225    \u001b[0m | \u001b[0m88.3     \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.8724   \u001b[0m | \u001b[0m19.91    \u001b[0m | \u001b[0m4.214    \u001b[0m | \u001b[0m7.34     \u001b[0m | \u001b[0m56.12    \u001b[0m |\n",
      "| \u001b[95m7        \u001b[0m | \u001b[95m0.8857   \u001b[0m | \u001b[95m23.88    \u001b[0m | \u001b[95m2.962    \u001b[0m | \u001b[95m3.229    \u001b[0m | \u001b[95m91.79    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.8801   \u001b[0m | \u001b[0m23.28    \u001b[0m | \u001b[0m3.177    \u001b[0m | \u001b[0m4.818    \u001b[0m | \u001b[0m80.94    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.8696   \u001b[0m | \u001b[0m27.49    \u001b[0m | \u001b[0m5.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m68.26    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.8714   \u001b[0m | \u001b[0m21.55    \u001b[0m | \u001b[0m5.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m100.0    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.8772   \u001b[0m | \u001b[0m25.11    \u001b[0m | \u001b[0m3.381    \u001b[0m | \u001b[0m4.163    \u001b[0m | \u001b[0m30.37    \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.8679   \u001b[0m | \u001b[0m13.62    \u001b[0m | \u001b[0m5.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m27.82    \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.8726   \u001b[0m | \u001b[0m29.85    \u001b[0m | \u001b[0m4.881    \u001b[0m | \u001b[0m9.675    \u001b[0m | \u001b[0m20.3     \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.8844   \u001b[0m | \u001b[0m30.0     \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m7.021    \u001b[0m | \u001b[0m43.18    \u001b[0m |\n",
      "| \u001b[95m15       \u001b[0m | \u001b[95m0.8866   \u001b[0m | \u001b[95m19.71    \u001b[0m | \u001b[95m1.0      \u001b[0m | \u001b[95m2.0      \u001b[0m | \u001b[95m41.47    \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m0.8368   \u001b[0m | \u001b[0m8.609    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m45.97    \u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m0.8693   \u001b[0m | \u001b[0m22.12    \u001b[0m | \u001b[0m5.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m39.39    \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m0.8708   \u001b[0m | \u001b[0m30.0     \u001b[0m | \u001b[0m5.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m87.54    \u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m0.8769   \u001b[0m | \u001b[0m19.61    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m69.88    \u001b[0m |\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.8634   \u001b[0m | \u001b[0m30.0     \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m10.0     \u001b[0m |\n",
      "=========================================================================\n",
      "Optimal Parameters: {'max_depth': 19, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 41}\n",
      "R2 Score Train: 0.99\n",
      "R2 Score Test: 0.89\n",
      "|   iter    |  target   | max_depth | min_sa... | min_sa... | n_esti... |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.8575   \u001b[0m | \u001b[0m18.72    \u001b[0m | \u001b[0m3.861    \u001b[0m | \u001b[0m6.822    \u001b[0m | \u001b[0m59.04    \u001b[0m |\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m0.8605   \u001b[0m | \u001b[95m15.59    \u001b[0m | \u001b[95m3.584    \u001b[0m | \u001b[95m5.501    \u001b[0m | \u001b[95m90.26    \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.8576   \u001b[0m | \u001b[0m29.09    \u001b[0m | \u001b[0m2.534    \u001b[0m | \u001b[0m8.334    \u001b[0m | \u001b[0m57.6     \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.8516   \u001b[0m | \u001b[0m19.2     \u001b[0m | \u001b[0m4.702    \u001b[0m | \u001b[0m2.568    \u001b[0m | \u001b[0m17.84    \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.67     \u001b[0m | \u001b[0m5.505    \u001b[0m | \u001b[0m4.33     \u001b[0m | \u001b[0m8.225    \u001b[0m | \u001b[0m88.3     \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.8532   \u001b[0m | \u001b[0m19.91    \u001b[0m | \u001b[0m4.214    \u001b[0m | \u001b[0m7.34     \u001b[0m | \u001b[0m56.12    \u001b[0m |\n",
      "| \u001b[95m7        \u001b[0m | \u001b[95m0.8627   \u001b[0m | \u001b[95m23.71    \u001b[0m | \u001b[95m2.991    \u001b[0m | \u001b[95m3.295    \u001b[0m | \u001b[95m91.98    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.8607   \u001b[0m | \u001b[0m22.94    \u001b[0m | \u001b[0m3.037    \u001b[0m | \u001b[0m4.656    \u001b[0m | \u001b[0m81.29    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.8477   \u001b[0m | \u001b[0m27.27    \u001b[0m | \u001b[0m5.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m68.61    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.8515   \u001b[0m | \u001b[0m21.12    \u001b[0m | \u001b[0m5.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m100.0    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.8617   \u001b[0m | \u001b[0m29.97    \u001b[0m | \u001b[0m1.463    \u001b[0m | \u001b[0m3.93     \u001b[0m | \u001b[0m44.1     \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.862    \u001b[0m | \u001b[0m24.84    \u001b[0m | \u001b[0m2.846    \u001b[0m | \u001b[0m3.81     \u001b[0m | \u001b[0m32.64    \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.8623   \u001b[0m | \u001b[0m17.69    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m41.8     \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.8584   \u001b[0m | \u001b[0m14.17    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m32.65    \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.7823   \u001b[0m | \u001b[0m7.734    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m9.885    \u001b[0m | \u001b[0m44.96    \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m0.853    \u001b[0m | \u001b[0m23.79    \u001b[0m | \u001b[0m4.935    \u001b[0m | \u001b[0m9.759    \u001b[0m | \u001b[0m40.08    \u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m0.85     \u001b[0m | \u001b[0m14.01    \u001b[0m | \u001b[0m5.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m28.88    \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m0.8563   \u001b[0m | \u001b[0m29.49    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m23.14    \u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m0.8597   \u001b[0m | \u001b[0m30.0     \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m86.86    \u001b[0m |\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.839    \u001b[0m | \u001b[0m30.0     \u001b[0m | \u001b[0m5.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m |\n",
      "=========================================================================\n",
      "Optimal Parameters: {'max_depth': 23, 'min_samples_leaf': 2, 'min_samples_split': 3, 'n_estimators': 91}\n",
      "R2 Score Train: 0.97\n",
      "R2 Score Test: 0.86\n",
      "|   iter    |  target   | max_depth | min_sa... | min_sa... | n_esti... |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.7929   \u001b[0m | \u001b[0m18.72    \u001b[0m | \u001b[0m3.861    \u001b[0m | \u001b[0m6.822    \u001b[0m | \u001b[0m59.04    \u001b[0m |\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m0.796    \u001b[0m | \u001b[95m15.59    \u001b[0m | \u001b[95m3.584    \u001b[0m | \u001b[95m5.501    \u001b[0m | \u001b[95m90.26    \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.7926   \u001b[0m | \u001b[0m29.09    \u001b[0m | \u001b[0m2.534    \u001b[0m | \u001b[0m8.334    \u001b[0m | \u001b[0m57.6     \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.7901   \u001b[0m | \u001b[0m19.2     \u001b[0m | \u001b[0m4.702    \u001b[0m | \u001b[0m2.568    \u001b[0m | \u001b[0m17.84    \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.6139   \u001b[0m | \u001b[0m5.505    \u001b[0m | \u001b[0m4.33     \u001b[0m | \u001b[0m8.225    \u001b[0m | \u001b[0m88.3     \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.7926   \u001b[0m | \u001b[0m19.91    \u001b[0m | \u001b[0m4.214    \u001b[0m | \u001b[0m7.34     \u001b[0m | \u001b[0m56.12    \u001b[0m |\n",
      "| \u001b[95m7        \u001b[0m | \u001b[95m0.7978   \u001b[0m | \u001b[95m23.92    \u001b[0m | \u001b[95m2.928    \u001b[0m | \u001b[95m3.244    \u001b[0m | \u001b[95m91.85    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.7948   \u001b[0m | \u001b[0m22.95    \u001b[0m | \u001b[0m3.237    \u001b[0m | \u001b[0m4.351    \u001b[0m | \u001b[0m80.98    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.7895   \u001b[0m | \u001b[0m21.21    \u001b[0m | \u001b[0m5.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m100.0    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.7875   \u001b[0m | \u001b[0m28.77    \u001b[0m | \u001b[0m5.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m68.88    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.7914   \u001b[0m | \u001b[0m29.53    \u001b[0m | \u001b[0m3.386    \u001b[0m | \u001b[0m2.573    \u001b[0m | \u001b[0m45.16    \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.7909   \u001b[0m | \u001b[0m22.8     \u001b[0m | \u001b[0m4.247    \u001b[0m | \u001b[0m3.097    \u001b[0m | \u001b[0m32.25    \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.7936   \u001b[0m | \u001b[0m16.13    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m43.78    \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.7625   \u001b[0m | \u001b[0m9.999    \u001b[0m | \u001b[0m1.261    \u001b[0m | \u001b[0m2.15     \u001b[0m | \u001b[0m30.88    \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.7958   \u001b[0m | \u001b[0m22.48    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m88.14    \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m0.7102   \u001b[0m | \u001b[0m7.19     \u001b[0m | \u001b[0m5.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m53.43    \u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m0.7928   \u001b[0m | \u001b[0m23.64    \u001b[0m | \u001b[0m1.064    \u001b[0m | \u001b[0m9.921    \u001b[0m | \u001b[0m41.79    \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m0.7902   \u001b[0m | \u001b[0m29.99    \u001b[0m | \u001b[0m2.519    \u001b[0m | \u001b[0m9.73     \u001b[0m | \u001b[0m23.05    \u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m0.7745   \u001b[0m | \u001b[0m29.66    \u001b[0m | \u001b[0m2.765    \u001b[0m | \u001b[0m3.372    \u001b[0m | \u001b[0m10.82    \u001b[0m |\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.7941   \u001b[0m | \u001b[0m21.93    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m69.78    \u001b[0m |\n",
      "=========================================================================\n",
      "Optimal Parameters: {'max_depth': 23, 'min_samples_leaf': 2, 'min_samples_split': 3, 'n_estimators': 91}\n",
      "R2 Score Train: 0.95\n",
      "R2 Score Test: 0.8\n",
      "|   iter    |  target   | max_depth | min_sa... | min_sa... | n_esti... |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.8481   \u001b[0m | \u001b[0m18.72    \u001b[0m | \u001b[0m3.861    \u001b[0m | \u001b[0m6.822    \u001b[0m | \u001b[0m59.04    \u001b[0m |\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m0.847    \u001b[0m | \u001b[0m15.59    \u001b[0m | \u001b[0m3.584    \u001b[0m | \u001b[0m5.501    \u001b[0m | \u001b[0m90.26    \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.8474   \u001b[0m | \u001b[0m29.09    \u001b[0m | \u001b[0m2.534    \u001b[0m | \u001b[0m8.334    \u001b[0m | \u001b[0m57.6     \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.8263   \u001b[0m | \u001b[0m19.2     \u001b[0m | \u001b[0m4.702    \u001b[0m | \u001b[0m2.568    \u001b[0m | \u001b[0m17.84    \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.5483   \u001b[0m | \u001b[0m5.505    \u001b[0m | \u001b[0m4.33     \u001b[0m | \u001b[0m8.225    \u001b[0m | \u001b[0m88.3     \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.8373   \u001b[0m | \u001b[0m19.91    \u001b[0m | \u001b[0m4.214    \u001b[0m | \u001b[0m7.34     \u001b[0m | \u001b[0m56.12    \u001b[0m |\n",
      "| \u001b[95m7        \u001b[0m | \u001b[95m0.856    \u001b[0m | \u001b[95m23.83    \u001b[0m | \u001b[95m2.972    \u001b[0m | \u001b[95m3.281    \u001b[0m | \u001b[95m91.8     \u001b[0m |\n",
      "| \u001b[95m8        \u001b[0m | \u001b[95m0.8611   \u001b[0m | \u001b[95m23.13    \u001b[0m | \u001b[95m1.602    \u001b[0m | \u001b[95m2.777    \u001b[0m | \u001b[95m81.1     \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.8324   \u001b[0m | \u001b[0m26.71    \u001b[0m | \u001b[0m5.0      \u001b[0m | \u001b[0m8.05     \u001b[0m | \u001b[0m70.48    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.8466   \u001b[0m | \u001b[0m30.0     \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m84.95    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.843    \u001b[0m | \u001b[0m21.46    \u001b[0m | \u001b[0m3.814    \u001b[0m | \u001b[0m9.603    \u001b[0m | \u001b[0m99.82    \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.5458   \u001b[0m | \u001b[0m5.635    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m57.41    \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.8318   \u001b[0m | \u001b[0m30.0     \u001b[0m | \u001b[0m4.872    \u001b[0m | \u001b[0m9.066    \u001b[0m | \u001b[0m31.73    \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.8234   \u001b[0m | \u001b[0m30.0     \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.8566   \u001b[0m | \u001b[0m30.0     \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m44.15    \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m0.853    \u001b[0m | \u001b[0m18.99    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m34.07    \u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m0.7848   \u001b[0m | \u001b[0m9.283    \u001b[0m | \u001b[0m5.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m28.11    \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m0.8297   \u001b[0m | \u001b[0m23.02    \u001b[0m | \u001b[0m5.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m41.91    \u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m0.5391   \u001b[0m | \u001b[0m5.0      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m |\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.855    \u001b[0m | \u001b[0m30.0     \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m22.01    \u001b[0m |\n",
      "=========================================================================\n",
      "Optimal Parameters: {'max_depth': 23, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 81}\n",
      "R2 Score Train: 0.98\n",
      "R2 Score Test: 0.86\n"
     ]
    }
   ],
   "source": [
    "print(\"========================= Random Forest Regressor ================================\")\n",
    "%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Random_Forest_Regression_Optimized.ipynb\n",
    "\n",
    "for k,v in enumerate(Palas):\n",
    "    X = Palas[v].drop([v+\"Palas\"],axis = 1)\n",
    "    y = Palas[v][v+\"Palas\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state= 40)\n",
    "    # print(\"x train\",X_train.shape)\n",
    "    # print(\"x test\",X_test.shape)\n",
    "    # print(\"y train\",y_train.shape)\n",
    "    # print(\"y test\",y_test.shape)\n",
    "\n",
    "    model,hyperparameters,r2_score_test = Random_Forest_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "    trained_model_rf[v] = model\n",
    "    hyperparameters_rf[v] = hyperparameters\n",
    "    r2_score_test_rf_optim[v] = r2_score_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c5d16b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= Neural Network Regressor ================================\n",
      "|   iter    |  target   | activa... | hidden... | max_iter  |\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balag\\AppData\\Local\\Temp\\ipykernel_15068\\3835145909.py:32: DeprecationWarning: \n",
      "Passing acquisition function parameters or gaussian process parameters to maximize\n",
      "is no longer supported, and will cause an error in future releases. Instead,\n",
      "please use the \"set_gp_params\" method to set the gp params, and pass an instance\n",
      " of bayes_opt.util.UtilityFunction using the acquisition_function argument\n",
      "\n",
      "  optimizer.maximize(init_points=10, n_iter=25, acq='ei')\n",
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m1        \u001b[0m | \u001b[0m-0.9536  \u001b[0m | \u001b[0m0.417    \u001b[0m | \u001b[0m74.83    \u001b[0m | \u001b[0m200.0    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (227) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m2        \u001b[0m | \u001b[95m-0.8927  \u001b[0m | \u001b[95m0.3023   \u001b[0m | \u001b[95m23.21    \u001b[0m | \u001b[95m227.7    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (319) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m3        \u001b[0m | \u001b[0m-0.955   \u001b[0m | \u001b[0m0.1863   \u001b[0m | \u001b[0m41.1     \u001b[0m | \u001b[0m319.0    \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m-0.9483  \u001b[0m | \u001b[0m0.5388   \u001b[0m | \u001b[0m47.73    \u001b[0m | \u001b[0m405.6    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (208) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m5        \u001b[0m | \u001b[0m-0.9636  \u001b[0m | \u001b[0m0.2045   \u001b[0m | \u001b[0m89.03    \u001b[0m | \u001b[0m208.2    \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m-0.9483  \u001b[0m | \u001b[0m0.6705   \u001b[0m | \u001b[0m47.56    \u001b[0m | \u001b[0m367.6    \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m-0.9338  \u001b[0m | \u001b[0m0.1404   \u001b[0m | \u001b[0m27.83    \u001b[0m | \u001b[0m440.2    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m-0.9207  \u001b[0m | \u001b[0m0.9683   \u001b[0m | \u001b[0m38.21    \u001b[0m | \u001b[0m407.7    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m-0.9378  \u001b[0m | \u001b[0m0.8764   \u001b[0m | \u001b[0m90.51    \u001b[0m | \u001b[0m225.5    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m-0.928   \u001b[0m | \u001b[0m0.03905  \u001b[0m | \u001b[0m25.28    \u001b[0m | \u001b[0m463.4    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m-0.9182  \u001b[0m | \u001b[0m0.2927   \u001b[0m | \u001b[0m28.21    \u001b[0m | \u001b[0m440.1    \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m-0.9182  \u001b[0m | \u001b[0m0.04973  \u001b[0m | \u001b[0m28.89    \u001b[0m | \u001b[0m440.5    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (227) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m13       \u001b[0m | \u001b[95m-0.8816  \u001b[0m | \u001b[95m0.8808   \u001b[0m | \u001b[95m23.77    \u001b[0m | \u001b[95m227.1    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (226) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m14       \u001b[0m | \u001b[0m-0.9013  \u001b[0m | \u001b[0m0.4977   \u001b[0m | \u001b[0m24.71    \u001b[0m | \u001b[0m226.3    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (226) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m15       \u001b[0m | \u001b[0m-0.8909  \u001b[0m | \u001b[0m0.556    \u001b[0m | \u001b[0m21.87    \u001b[0m | \u001b[0m226.8    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (225) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m16       \u001b[0m | \u001b[0m-0.8907  \u001b[0m | \u001b[0m0.9308   \u001b[0m | \u001b[0m22.37    \u001b[0m | \u001b[0m225.3    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (228) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m17       \u001b[0m | \u001b[0m-0.8923  \u001b[0m | \u001b[0m0.9027   \u001b[0m | \u001b[0m24.77    \u001b[0m | \u001b[0m228.6    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (225) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m18       \u001b[0m | \u001b[95m-0.8793  \u001b[0m | \u001b[95m0.1878   \u001b[0m | \u001b[95m18.7     \u001b[0m | \u001b[95m225.2    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (224) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m19       \u001b[0m | \u001b[0m-0.8903  \u001b[0m | \u001b[0m0.1266   \u001b[0m | \u001b[0m20.25    \u001b[0m | \u001b[0m224.2    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (223) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m20       \u001b[0m | \u001b[95m-0.8732  \u001b[0m | \u001b[95m0.2486   \u001b[0m | \u001b[95m17.74    \u001b[0m | \u001b[95m223.5    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (224) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m21       \u001b[0m | \u001b[95m-0.8676  \u001b[0m | \u001b[95m0.373    \u001b[0m | \u001b[95m15.98    \u001b[0m | \u001b[95m224.6    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (222) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m22       \u001b[0m | \u001b[95m-0.8428  \u001b[0m | \u001b[95m0.2787   \u001b[0m | \u001b[95m14.43    \u001b[0m | \u001b[95m222.4    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (219) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m23       \u001b[0m | \u001b[95m-0.8281  \u001b[0m | \u001b[95m0.793    \u001b[0m | \u001b[95m14.83    \u001b[0m | \u001b[95m220.0    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (220) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m24       \u001b[0m | \u001b[95m-0.8259  \u001b[0m | \u001b[95m0.5961   \u001b[0m | \u001b[95m11.57    \u001b[0m | \u001b[95m220.7    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (217) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m25       \u001b[0m | \u001b[95m-0.8221  \u001b[0m | \u001b[95m0.9132   \u001b[0m | \u001b[95m11.97    \u001b[0m | \u001b[95m217.5    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (214) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m26       \u001b[0m | \u001b[0m-0.8237  \u001b[0m | \u001b[0m0.5361   \u001b[0m | \u001b[0m14.11    \u001b[0m | \u001b[0m214.6    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (212) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m27       \u001b[0m | \u001b[0m-0.8238  \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m212.7    \u001b[0m |\n",
      "| \u001b[0m28       \u001b[0m | \u001b[0m-0.9209  \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m31.9     \u001b[0m | \u001b[0m435.0    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (209) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m29       \u001b[0m | \u001b[95m-0.8206  \u001b[0m | \u001b[95m0.5759   \u001b[0m | \u001b[95m14.02    \u001b[0m | \u001b[95m209.5    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (207) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m30       \u001b[0m | \u001b[95m-0.7904  \u001b[0m | \u001b[95m1.0      \u001b[0m | \u001b[95m10.0     \u001b[0m | \u001b[95m207.4    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (204) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m31       \u001b[0m | \u001b[0m-0.8302  \u001b[0m | \u001b[0m0.2044   \u001b[0m | \u001b[0m11.96    \u001b[0m | \u001b[0m204.2    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (211) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m32       \u001b[0m | \u001b[0m-0.8794  \u001b[0m | \u001b[0m0.9265   \u001b[0m | \u001b[0m20.66    \u001b[0m | \u001b[0m211.2    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (209) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m33       \u001b[0m | \u001b[0m-0.8216  \u001b[0m | \u001b[0m0.5273   \u001b[0m | \u001b[0m11.11    \u001b[0m | \u001b[0m209.8    \u001b[0m |\n",
      "| \u001b[0m34       \u001b[0m | \u001b[0m-0.8795  \u001b[0m | \u001b[0m0.8596   \u001b[0m | \u001b[0m19.89    \u001b[0m | \u001b[0m201.8    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (233) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m35       \u001b[0m | \u001b[0m-0.7957  \u001b[0m | \u001b[0m0.829    \u001b[0m | \u001b[0m10.16    \u001b[0m | \u001b[0m233.4    \u001b[0m |\n",
      "=============================================================\n",
      "Optimal Parameters: {'activation': 1.0, 'hidden_layer_sizes': 10, 'max_iter': 207}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (207) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\balag\\AppData\\Local\\Temp\\ipykernel_15068\\3835145909.py:32: DeprecationWarning: \n",
      "Passing acquisition function parameters or gaussian process parameters to maximize\n",
      "is no longer supported, and will cause an error in future releases. Instead,\n",
      "please use the \"set_gp_params\" method to set the gp params, and pass an instance\n",
      " of bayes_opt.util.UtilityFunction using the acquisition_function argument\n",
      "\n",
      "  optimizer.maximize(init_points=10, n_iter=25, acq='ei')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Actual  Predicted  Category\n",
      "0     2.718542   2.689786   Testing\n",
      "1     2.067500   2.749375   Testing\n",
      "2     1.904375   2.635015  Training\n",
      "4     1.540208   3.084172  Training\n",
      "5     1.505000   2.585364  Training\n",
      "...        ...        ...       ...\n",
      "8326  4.982917   4.954793  Training\n",
      "8327  4.544583   5.076337  Training\n",
      "8328  4.834167   4.723167  Training\n",
      "8329  4.537917   4.673979  Training\n",
      "8330  4.430000   4.773445   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.79\n",
      "r2 test 0.74\n",
      "|   iter    |  target   | activa... | hidden... | max_iter  |\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m1        \u001b[0m | \u001b[0m-0.9697  \u001b[0m | \u001b[0m0.417    \u001b[0m | \u001b[0m74.83    \u001b[0m | \u001b[0m200.0    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (227) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m2        \u001b[0m | \u001b[95m-0.9198  \u001b[0m | \u001b[95m0.3023   \u001b[0m | \u001b[95m23.21    \u001b[0m | \u001b[95m227.7    \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m-0.969   \u001b[0m | \u001b[0m0.1863   \u001b[0m | \u001b[0m41.1     \u001b[0m | \u001b[0m319.0    \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m-0.9674  \u001b[0m | \u001b[0m0.5388   \u001b[0m | \u001b[0m47.73    \u001b[0m | \u001b[0m405.6    \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m-0.9652  \u001b[0m | \u001b[0m0.2045   \u001b[0m | \u001b[0m89.03    \u001b[0m | \u001b[0m208.2    \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m-0.9674  \u001b[0m | \u001b[0m0.6705   \u001b[0m | \u001b[0m47.56    \u001b[0m | \u001b[0m367.6    \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m-0.9439  \u001b[0m | \u001b[0m0.1404   \u001b[0m | \u001b[0m27.83    \u001b[0m | \u001b[0m440.2    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m-0.9386  \u001b[0m | \u001b[0m0.9683   \u001b[0m | \u001b[0m38.21    \u001b[0m | \u001b[0m407.7    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m-0.9646  \u001b[0m | \u001b[0m0.8764   \u001b[0m | \u001b[0m90.51    \u001b[0m | \u001b[0m225.5    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m-0.9384  \u001b[0m | \u001b[0m0.03905  \u001b[0m | \u001b[0m25.28    \u001b[0m | \u001b[0m463.4    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m-0.9393  \u001b[0m | \u001b[0m0.2927   \u001b[0m | \u001b[0m28.21    \u001b[0m | \u001b[0m440.1    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (229) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m12       \u001b[0m | \u001b[95m-0.919   \u001b[0m | \u001b[95m0.02564  \u001b[0m | \u001b[95m24.11    \u001b[0m | \u001b[95m229.2    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (230) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m13       \u001b[0m | \u001b[0m-0.9243  \u001b[0m | \u001b[0m0.6514   \u001b[0m | \u001b[0m21.89    \u001b[0m | \u001b[0m230.4    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (226) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m14       \u001b[0m | \u001b[0m-0.9294  \u001b[0m | \u001b[0m0.2913   \u001b[0m | \u001b[0m27.26    \u001b[0m | \u001b[0m226.8    \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m-0.9502  \u001b[0m | \u001b[0m0.9601   \u001b[0m | \u001b[0m33.93    \u001b[0m | \u001b[0m439.4    \u001b[0m |\n",
      "| \u001b[95m16       \u001b[0m | \u001b[95m-0.9145  \u001b[0m | \u001b[95m1.0      \u001b[0m | \u001b[95m26.8     \u001b[0m | \u001b[95m232.6    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (237) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m17       \u001b[0m | \u001b[0m-0.9345  \u001b[0m | \u001b[0m0.4325   \u001b[0m | \u001b[0m25.05    \u001b[0m | \u001b[0m237.5    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (232) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m18       \u001b[0m | \u001b[0m-0.9441  \u001b[0m | \u001b[0m0.2629   \u001b[0m | \u001b[0m31.24    \u001b[0m | \u001b[0m232.8    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (223) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m19       \u001b[0m | \u001b[95m-0.908   \u001b[0m | \u001b[95m0.01413  \u001b[0m | \u001b[95m21.51    \u001b[0m | \u001b[95m223.3    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (223) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m20       \u001b[0m | \u001b[95m-0.893   \u001b[0m | \u001b[95m0.2486   \u001b[0m | \u001b[95m17.74    \u001b[0m | \u001b[95m223.5    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (220) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m21       \u001b[0m | \u001b[95m-0.8797  \u001b[0m | \u001b[95m0.1509   \u001b[0m | \u001b[95m15.23    \u001b[0m | \u001b[95m220.0    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (222) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m22       \u001b[0m | \u001b[95m-0.851   \u001b[0m | \u001b[95m0.4204   \u001b[0m | \u001b[95m12.73    \u001b[0m | \u001b[95m222.6    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (225) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m23       \u001b[0m | \u001b[0m-0.8562  \u001b[0m | \u001b[0m0.2487   \u001b[0m | \u001b[0m10.32    \u001b[0m | \u001b[0m225.4    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (219) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m24       \u001b[0m | \u001b[0m-0.8549  \u001b[0m | \u001b[0m0.3725   \u001b[0m | \u001b[0m10.01    \u001b[0m | \u001b[0m219.2    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (210) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m25       \u001b[0m | \u001b[95m-0.8417  \u001b[0m | \u001b[95m1.0      \u001b[0m | \u001b[95m10.0     \u001b[0m | \u001b[95m210.5    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (203) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m26       \u001b[0m | \u001b[95m-0.8403  \u001b[0m | \u001b[95m1.0      \u001b[0m | \u001b[95m10.0     \u001b[0m | \u001b[95m203.8    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (205) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m27       \u001b[0m | \u001b[0m-0.8989  \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m16.66    \u001b[0m | \u001b[0m205.4    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m28       \u001b[0m | \u001b[0m-0.845   \u001b[0m | \u001b[0m0.1312   \u001b[0m | \u001b[0m12.16    \u001b[0m | \u001b[0m200.1    \u001b[0m |\n",
      "| \u001b[0m29       \u001b[0m | \u001b[0m-0.974   \u001b[0m | \u001b[0m0.8647   \u001b[0m | \u001b[0m99.82    \u001b[0m | \u001b[0m497.9    \u001b[0m |\n",
      "| \u001b[0m30       \u001b[0m | \u001b[0m-0.9799  \u001b[0m | \u001b[0m0.3246   \u001b[0m | \u001b[0m99.36    \u001b[0m | \u001b[0m293.1    \u001b[0m |\n",
      "| \u001b[0m31       \u001b[0m | \u001b[0m-0.974   \u001b[0m | \u001b[0m0.8935   \u001b[0m | \u001b[0m99.76    \u001b[0m | \u001b[0m432.8    \u001b[0m |\n",
      "| \u001b[0m32       \u001b[0m | \u001b[0m-0.974   \u001b[0m | \u001b[0m0.8786   \u001b[0m | \u001b[0m99.84    \u001b[0m | \u001b[0m357.5    \u001b[0m |\n",
      "| \u001b[0m33       \u001b[0m | \u001b[0m-0.963   \u001b[0m | \u001b[0m0.8408   \u001b[0m | \u001b[0m45.97    \u001b[0m | \u001b[0m499.8    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (282) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m34       \u001b[0m | \u001b[0m-0.8527  \u001b[0m | \u001b[0m0.8852   \u001b[0m | \u001b[0m10.92    \u001b[0m | \u001b[0m282.8    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (291) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m35       \u001b[0m | \u001b[0m-0.8557  \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m291.6    \u001b[0m |\n",
      "=============================================================\n",
      "Optimal Parameters: {'activation': 1.0, 'hidden_layer_sizes': 10, 'max_iter': 203}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (203) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\balag\\AppData\\Local\\Temp\\ipykernel_15068\\3835145909.py:32: DeprecationWarning: \n",
      "Passing acquisition function parameters or gaussian process parameters to maximize\n",
      "is no longer supported, and will cause an error in future releases. Instead,\n",
      "please use the \"set_gp_params\" method to set the gp params, and pass an instance\n",
      " of bayes_opt.util.UtilityFunction using the acquisition_function argument\n",
      "\n",
      "  optimizer.maximize(init_points=10, n_iter=25, acq='ei')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Actual  Predicted  Category\n",
      "0      2.820000   4.016155   Testing\n",
      "1      2.130000   4.230433   Testing\n",
      "2      2.000000   3.430496  Training\n",
      "4      1.661458   3.598425  Training\n",
      "5      1.545208   3.229406  Training\n",
      "...         ...        ...       ...\n",
      "8326  12.055000  10.494442  Training\n",
      "8327   9.235833  10.707718  Training\n",
      "8328  10.393750  10.453184  Training\n",
      "8329   9.765417  10.220038  Training\n",
      "8330   9.886667  10.055032   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.84\n",
      "r2 test 0.82\n",
      "|   iter    |  target   | activa... | hidden... | max_iter  |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m-0.9421  \u001b[0m | \u001b[0m0.417    \u001b[0m | \u001b[0m74.83    \u001b[0m | \u001b[0m200.0    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (227) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m2        \u001b[0m | \u001b[95m-0.9103  \u001b[0m | \u001b[95m0.3023   \u001b[0m | \u001b[95m23.21    \u001b[0m | \u001b[95m227.7    \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m-0.9549  \u001b[0m | \u001b[0m0.1863   \u001b[0m | \u001b[0m41.1     \u001b[0m | \u001b[0m319.0    \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m-0.951   \u001b[0m | \u001b[0m0.5388   \u001b[0m | \u001b[0m47.73    \u001b[0m | \u001b[0m405.6    \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m-0.9551  \u001b[0m | \u001b[0m0.2045   \u001b[0m | \u001b[0m89.03    \u001b[0m | \u001b[0m208.2    \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m-0.951   \u001b[0m | \u001b[0m0.6705   \u001b[0m | \u001b[0m47.56    \u001b[0m | \u001b[0m367.6    \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m-0.9158  \u001b[0m | \u001b[0m0.1404   \u001b[0m | \u001b[0m27.83    \u001b[0m | \u001b[0m440.2    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m-0.9314  \u001b[0m | \u001b[0m0.9683   \u001b[0m | \u001b[0m38.21    \u001b[0m | \u001b[0m407.7    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m-0.9716  \u001b[0m | \u001b[0m0.8764   \u001b[0m | \u001b[0m90.51    \u001b[0m | \u001b[0m225.5    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m-0.9385  \u001b[0m | \u001b[0m0.03905  \u001b[0m | \u001b[0m25.28    \u001b[0m | \u001b[0m463.4    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m-0.9141  \u001b[0m | \u001b[0m0.2927   \u001b[0m | \u001b[0m28.21    \u001b[0m | \u001b[0m440.1    \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m-0.9323  \u001b[0m | \u001b[0m0.9271   \u001b[0m | \u001b[0m35.15    \u001b[0m | \u001b[0m435.8    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (224) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m13       \u001b[0m | \u001b[95m-0.8956  \u001b[0m | \u001b[95m0.5011   \u001b[0m | \u001b[95m18.23    \u001b[0m | \u001b[95m224.9    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (229) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m14       \u001b[0m | \u001b[95m-0.8658  \u001b[0m | \u001b[95m0.2318   \u001b[0m | \u001b[95m14.75    \u001b[0m | \u001b[95m230.0    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (233) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m15       \u001b[0m | \u001b[95m-0.8358  \u001b[0m | \u001b[95m1.0      \u001b[0m | \u001b[95m10.0     \u001b[0m | \u001b[95m233.6    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (241) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m16       \u001b[0m | \u001b[0m-0.8541  \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m241.9    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (259) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m17       \u001b[0m | \u001b[0m-0.8394  \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m259.0    \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m-0.9149  \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m21.32    \u001b[0m | \u001b[0m263.5    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (269) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m19       \u001b[0m | \u001b[0m-0.8422  \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m269.7    \u001b[0m |\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m-0.8424  \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m284.8    \u001b[0m |\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m-0.8424  \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m300.1    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (293) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m22       \u001b[0m | \u001b[0m-0.9184  \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m22.43    \u001b[0m | \u001b[0m293.4    \u001b[0m |\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m-0.8424  \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m315.8    \u001b[0m |\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m-0.8652  \u001b[0m | \u001b[0m0.3412   \u001b[0m | \u001b[0m10.26    \u001b[0m | \u001b[0m336.7    \u001b[0m |\n",
      "| \u001b[0m25       \u001b[0m | \u001b[0m-0.8652  \u001b[0m | \u001b[0m0.1308   \u001b[0m | \u001b[0m10.24    \u001b[0m | \u001b[0m359.1    \u001b[0m |\n",
      "| \u001b[0m26       \u001b[0m | \u001b[0m-0.8424  \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m381.9    \u001b[0m |\n",
      "| \u001b[0m27       \u001b[0m | \u001b[0m-0.8424  \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m398.7    \u001b[0m |\n",
      "| \u001b[0m28       \u001b[0m | \u001b[0m-0.9802  \u001b[0m | \u001b[0m0.6288   \u001b[0m | \u001b[0m99.73    \u001b[0m | \u001b[0m499.8    \u001b[0m |\n",
      "| \u001b[0m29       \u001b[0m | \u001b[0m-0.9697  \u001b[0m | \u001b[0m0.1457   \u001b[0m | \u001b[0m99.74    \u001b[0m | \u001b[0m300.9    \u001b[0m |\n",
      "| \u001b[0m30       \u001b[0m | \u001b[0m-0.9546  \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m100.0    \u001b[0m | \u001b[0m429.1    \u001b[0m |\n",
      "| \u001b[0m31       \u001b[0m | \u001b[0m-0.9149  \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m21.91    \u001b[0m | \u001b[0m389.0    \u001b[0m |\n",
      "| \u001b[0m32       \u001b[0m | \u001b[0m-0.8424  \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m414.3    \u001b[0m |\n",
      "| \u001b[0m33       \u001b[0m | \u001b[0m-0.9721  \u001b[0m | \u001b[0m0.2718   \u001b[0m | \u001b[0m100.0    \u001b[0m | \u001b[0m364.0    \u001b[0m |\n",
      "| \u001b[0m34       \u001b[0m | \u001b[0m-0.956   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m43.34    \u001b[0m | \u001b[0m500.0    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m35       \u001b[0m | \u001b[95m-0.8213  \u001b[0m | \u001b[95m1.0      \u001b[0m | \u001b[95m10.0     \u001b[0m | \u001b[95m200.0    \u001b[0m |\n",
      "=============================================================\n",
      "Optimal Parameters: {'activation': 1.0, 'hidden_layer_sizes': 10, 'max_iter': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\balag\\AppData\\Local\\Temp\\ipykernel_15068\\3835145909.py:32: DeprecationWarning: \n",
      "Passing acquisition function parameters or gaussian process parameters to maximize\n",
      "is no longer supported, and will cause an error in future releases. Instead,\n",
      "please use the \"set_gp_params\" method to set the gp params, and pass an instance\n",
      " of bayes_opt.util.UtilityFunction using the acquisition_function argument\n",
      "\n",
      "  optimizer.maximize(init_points=10, n_iter=25, acq='ei')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Actual  Predicted  Category\n",
      "0      3.085417   4.470964   Testing\n",
      "1      2.232708   4.719283   Testing\n",
      "2      2.135833   4.238361  Training\n",
      "4      1.834792   6.527448  Training\n",
      "5      1.643333   4.448492  Training\n",
      "...         ...        ...       ...\n",
      "8326  21.223333  18.027304  Training\n",
      "8327  15.932083  18.134011  Training\n",
      "8328  16.646667  18.008682  Training\n",
      "8329  16.986667  17.973907  Training\n",
      "8330  17.401667  17.962721   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.82\n",
      "r2 test 0.81\n",
      "|   iter    |  target   | activa... | hidden... | max_iter  |\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m1        \u001b[0m | \u001b[0m-0.9396  \u001b[0m | \u001b[0m0.417    \u001b[0m | \u001b[0m74.83    \u001b[0m | \u001b[0m200.0    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (227) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m2        \u001b[0m | \u001b[95m-0.8769  \u001b[0m | \u001b[95m0.3023   \u001b[0m | \u001b[95m23.21    \u001b[0m | \u001b[95m227.7    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (319) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m3        \u001b[0m | \u001b[0m-0.9351  \u001b[0m | \u001b[0m0.1863   \u001b[0m | \u001b[0m41.1     \u001b[0m | \u001b[0m319.0    \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m-0.9394  \u001b[0m | \u001b[0m0.5388   \u001b[0m | \u001b[0m47.73    \u001b[0m | \u001b[0m405.6    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (208) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m5        \u001b[0m | \u001b[0m-0.9451  \u001b[0m | \u001b[0m0.2045   \u001b[0m | \u001b[0m89.03    \u001b[0m | \u001b[0m208.2    \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m-0.9394  \u001b[0m | \u001b[0m0.6705   \u001b[0m | \u001b[0m47.56    \u001b[0m | \u001b[0m367.6    \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m-0.8802  \u001b[0m | \u001b[0m0.1404   \u001b[0m | \u001b[0m27.83    \u001b[0m | \u001b[0m440.2    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m-0.9135  \u001b[0m | \u001b[0m0.9683   \u001b[0m | \u001b[0m38.21    \u001b[0m | \u001b[0m407.7    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m-0.9497  \u001b[0m | \u001b[0m0.8764   \u001b[0m | \u001b[0m90.51    \u001b[0m | \u001b[0m225.5    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m-0.8921  \u001b[0m | \u001b[0m0.03905  \u001b[0m | \u001b[0m25.28    \u001b[0m | \u001b[0m463.4    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m-0.8958  \u001b[0m | \u001b[0m0.2927   \u001b[0m | \u001b[0m28.21    \u001b[0m | \u001b[0m440.1    \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m-0.8802  \u001b[0m | \u001b[0m0.3151   \u001b[0m | \u001b[0m27.06    \u001b[0m | \u001b[0m441.2    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (227) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m13       \u001b[0m | \u001b[95m-0.8741  \u001b[0m | \u001b[95m0.728    \u001b[0m | \u001b[95m24.54    \u001b[0m | \u001b[95m227.6    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (225) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m14       \u001b[0m | \u001b[95m-0.8739  \u001b[0m | \u001b[95m0.2562   \u001b[0m | \u001b[95m24.26    \u001b[0m | \u001b[95m225.4    \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m-0.8788  \u001b[0m | \u001b[0m0.7478   \u001b[0m | \u001b[0m26.72    \u001b[0m | \u001b[0m226.4    \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m-0.8921  \u001b[0m | \u001b[0m0.09211  \u001b[0m | \u001b[0m25.31    \u001b[0m | \u001b[0m439.5    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (224) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m17       \u001b[0m | \u001b[0m-0.8952  \u001b[0m | \u001b[0m0.7306   \u001b[0m | \u001b[0m25.77    \u001b[0m | \u001b[0m224.4    \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m-0.8965  \u001b[0m | \u001b[0m0.601    \u001b[0m | \u001b[0m27.57    \u001b[0m | \u001b[0m229.0    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (224) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m19       \u001b[0m | \u001b[95m-0.8683  \u001b[0m | \u001b[95m0.2493   \u001b[0m | \u001b[95m21.27    \u001b[0m | \u001b[95m224.5    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (225) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m20       \u001b[0m | \u001b[95m-0.8615  \u001b[0m | \u001b[95m0.4066   \u001b[0m | \u001b[95m19.48    \u001b[0m | \u001b[95m225.3    \u001b[0m |\n",
      "| \u001b[95m21       \u001b[0m | \u001b[95m-0.8611  \u001b[0m | \u001b[95m0.7807   \u001b[0m | \u001b[95m19.99    \u001b[0m | \u001b[95m227.0    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (227) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m22       \u001b[0m | \u001b[95m-0.8564  \u001b[0m | \u001b[95m0.9083   \u001b[0m | \u001b[95m16.47    \u001b[0m | \u001b[95m227.5    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (225) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m23       \u001b[0m | \u001b[95m-0.8555  \u001b[0m | \u001b[95m0.1567   \u001b[0m | \u001b[95m16.15    \u001b[0m | \u001b[95m225.2    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (226) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m24       \u001b[0m | \u001b[95m-0.8316  \u001b[0m | \u001b[95m0.5683   \u001b[0m | \u001b[95m13.44    \u001b[0m | \u001b[95m226.2    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (228) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m25       \u001b[0m | \u001b[0m-0.834   \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m13.15    \u001b[0m | \u001b[0m228.3    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (227) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m26       \u001b[0m | \u001b[95m-0.8198  \u001b[0m | \u001b[95m0.04669  \u001b[0m | \u001b[95m10.64    \u001b[0m | \u001b[95m227.9    \u001b[0m |\n",
      "| \u001b[95m27       \u001b[0m | \u001b[95m-0.8037  \u001b[0m | \u001b[95m0.8432   \u001b[0m | \u001b[95m10.38    \u001b[0m | \u001b[95m230.2    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (233) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m28       \u001b[0m | \u001b[0m-0.8221  \u001b[0m | \u001b[0m0.2099   \u001b[0m | \u001b[0m11.98    \u001b[0m | \u001b[0m233.5    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (232) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m29       \u001b[0m | \u001b[0m-0.8209  \u001b[0m | \u001b[0m0.1176   \u001b[0m | \u001b[0m10.1     \u001b[0m | \u001b[0m232.6    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (222) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m30       \u001b[0m | \u001b[0m-0.8194  \u001b[0m | \u001b[0m0.1378   \u001b[0m | \u001b[0m11.09    \u001b[0m | \u001b[0m222.3    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (218) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m31       \u001b[0m | \u001b[0m-0.8209  \u001b[0m | \u001b[0m0.8797   \u001b[0m | \u001b[0m12.48    \u001b[0m | \u001b[0m218.1    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (219) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m32       \u001b[0m | \u001b[0m-0.834   \u001b[0m | \u001b[0m0.9249   \u001b[0m | \u001b[0m16.06    \u001b[0m | \u001b[0m219.6    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (213) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m33       \u001b[0m | \u001b[0m-0.8169  \u001b[0m | \u001b[0m0.9713   \u001b[0m | \u001b[0m12.84    \u001b[0m | \u001b[0m213.4    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (208) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m34       \u001b[0m | \u001b[0m-0.8161  \u001b[0m | \u001b[0m0.7072   \u001b[0m | \u001b[0m12.67    \u001b[0m | \u001b[0m208.8    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (211) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m35       \u001b[0m | \u001b[0m-0.8545  \u001b[0m | \u001b[0m0.581    \u001b[0m | \u001b[0m16.53    \u001b[0m | \u001b[0m211.8    \u001b[0m |\n",
      "=============================================================\n",
      "Optimal Parameters: {'activation': 0.8431799102852747, 'hidden_layer_sizes': 10, 'max_iter': 230}\n",
      "         Actual  Predicted  Category\n",
      "0      3.112500   2.044661   Testing\n",
      "1      2.231875   2.346593   Testing\n",
      "2      2.132083   3.606973  Training\n",
      "4      1.824792   4.850807  Training\n",
      "5      1.640833   4.520821  Training\n",
      "...         ...        ...       ...\n",
      "8326  31.216250  22.261541  Training\n",
      "8327  17.556667  23.355886  Training\n",
      "8328  24.890000  23.637569  Training\n",
      "8329  18.576667  21.522817  Training\n",
      "8330  20.421250  20.709927   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.8\n",
      "r2 test 0.76\n",
      "|   iter    |  target   | activa... | hidden... | max_iter  |\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balag\\AppData\\Local\\Temp\\ipykernel_15068\\3835145909.py:32: DeprecationWarning: \n",
      "Passing acquisition function parameters or gaussian process parameters to maximize\n",
      "is no longer supported, and will cause an error in future releases. Instead,\n",
      "please use the \"set_gp_params\" method to set the gp params, and pass an instance\n",
      " of bayes_opt.util.UtilityFunction using the acquisition_function argument\n",
      "\n",
      "  optimizer.maximize(init_points=10, n_iter=25, acq='ei')\n",
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m1        \u001b[0m | \u001b[0m-0.8943  \u001b[0m | \u001b[0m0.417    \u001b[0m | \u001b[0m74.83    \u001b[0m | \u001b[0m200.0    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (227) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m2        \u001b[0m | \u001b[95m-0.8186  \u001b[0m | \u001b[95m0.3023   \u001b[0m | \u001b[95m23.21    \u001b[0m | \u001b[95m227.7    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (319) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m3        \u001b[0m | \u001b[0m-0.886   \u001b[0m | \u001b[0m0.1863   \u001b[0m | \u001b[0m41.1     \u001b[0m | \u001b[0m319.0    \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m-0.9081  \u001b[0m | \u001b[0m0.5388   \u001b[0m | \u001b[0m47.73    \u001b[0m | \u001b[0m405.6    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (208) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m5        \u001b[0m | \u001b[0m-0.9082  \u001b[0m | \u001b[0m0.2045   \u001b[0m | \u001b[0m89.03    \u001b[0m | \u001b[0m208.2    \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m-0.9081  \u001b[0m | \u001b[0m0.6705   \u001b[0m | \u001b[0m47.56    \u001b[0m | \u001b[0m367.6    \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m-0.8413  \u001b[0m | \u001b[0m0.1404   \u001b[0m | \u001b[0m27.83    \u001b[0m | \u001b[0m440.2    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m-0.8602  \u001b[0m | \u001b[0m0.9683   \u001b[0m | \u001b[0m38.21    \u001b[0m | \u001b[0m407.7    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m-0.919   \u001b[0m | \u001b[0m0.8764   \u001b[0m | \u001b[0m90.51    \u001b[0m | \u001b[0m225.5    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m-0.852   \u001b[0m | \u001b[0m0.03905  \u001b[0m | \u001b[0m25.28    \u001b[0m | \u001b[0m463.4    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m-0.8345  \u001b[0m | \u001b[0m0.2927   \u001b[0m | \u001b[0m28.21    \u001b[0m | \u001b[0m440.1    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (227) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m12       \u001b[0m | \u001b[95m-0.8032  \u001b[0m | \u001b[95m0.1525   \u001b[0m | \u001b[95m20.51    \u001b[0m | \u001b[95m227.3    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (223) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m13       \u001b[0m | \u001b[95m-0.8006  \u001b[0m | \u001b[95m0.4112   \u001b[0m | \u001b[95m20.25    \u001b[0m | \u001b[95m223.1    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (224) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m14       \u001b[0m | \u001b[95m-0.7712  \u001b[0m | \u001b[95m0.452    \u001b[0m | \u001b[95m15.04    \u001b[0m | \u001b[95m224.5    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (229) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m15       \u001b[0m | \u001b[95m-0.7519  \u001b[0m | \u001b[95m1.0      \u001b[0m | \u001b[95m10.0     \u001b[0m | \u001b[95m229.5    \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m-0.8741  \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m40.44    \u001b[0m | \u001b[0m436.5    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (236) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m17       \u001b[0m | \u001b[0m-0.772   \u001b[0m | \u001b[0m0.7337   \u001b[0m | \u001b[0m12.35    \u001b[0m | \u001b[0m236.5    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (216) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m18       \u001b[0m | \u001b[95m-0.7461  \u001b[0m | \u001b[95m1.0      \u001b[0m | \u001b[95m10.0     \u001b[0m | \u001b[95m216.5    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (206) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m19       \u001b[0m | \u001b[0m-0.7489  \u001b[0m | \u001b[0m0.993    \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m206.2    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (205) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m20       \u001b[0m | \u001b[0m-0.7985  \u001b[0m | \u001b[0m0.4427   \u001b[0m | \u001b[0m20.25    \u001b[0m | \u001b[0m205.3    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (253) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m21       \u001b[0m | \u001b[0m-0.7567  \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m253.5    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (257) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m22       \u001b[0m | \u001b[0m-0.8191  \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m22.22    \u001b[0m | \u001b[0m257.1    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (266) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m23       \u001b[0m | \u001b[0m-0.7614  \u001b[0m | \u001b[0m0.6139   \u001b[0m | \u001b[0m10.58    \u001b[0m | \u001b[0m266.3    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (282) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m24       \u001b[0m | \u001b[0m-0.7648  \u001b[0m | \u001b[0m0.9484   \u001b[0m | \u001b[0m10.21    \u001b[0m | \u001b[0m282.2    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (280) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m25       \u001b[0m | \u001b[0m-0.8292  \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m24.05    \u001b[0m | \u001b[0m280.1    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (298) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m26       \u001b[0m | \u001b[0m-0.7684  \u001b[0m | \u001b[0m0.2283   \u001b[0m | \u001b[0m11.49    \u001b[0m | \u001b[0m298.9    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (314) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m27       \u001b[0m | \u001b[0m-0.7704  \u001b[0m | \u001b[0m0.6273   \u001b[0m | \u001b[0m10.56    \u001b[0m | \u001b[0m314.9    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (331) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m28       \u001b[0m | \u001b[0m-0.773   \u001b[0m | \u001b[0m0.5575   \u001b[0m | \u001b[0m10.79    \u001b[0m | \u001b[0m331.0    \u001b[0m |\n",
      "| \u001b[0m29       \u001b[0m | \u001b[0m-0.7561  \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m348.6    \u001b[0m |\n",
      "| \u001b[0m30       \u001b[0m | \u001b[0m-0.7561  \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m363.7    \u001b[0m |\n",
      "| \u001b[0m31       \u001b[0m | \u001b[0m-0.8276  \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m21.46    \u001b[0m | \u001b[0m354.8    \u001b[0m |\n",
      "| \u001b[0m32       \u001b[0m | \u001b[0m-0.7561  \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m379.4    \u001b[0m |\n",
      "| \u001b[0m33       \u001b[0m | \u001b[0m-0.7801  \u001b[0m | \u001b[0m0.6231   \u001b[0m | \u001b[0m10.23    \u001b[0m | \u001b[0m396.8    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (385) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m34       \u001b[0m | \u001b[0m-0.8421  \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m22.07    \u001b[0m | \u001b[0m385.3    \u001b[0m |\n",
      "| \u001b[0m35       \u001b[0m | \u001b[0m-0.9247  \u001b[0m | \u001b[0m0.6004   \u001b[0m | \u001b[0m99.9     \u001b[0m | \u001b[0m499.2    \u001b[0m |\n",
      "=============================================================\n",
      "Optimal Parameters: {'activation': 1.0, 'hidden_layer_sizes': 10, 'max_iter': 216}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (216) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\balag\\AppData\\Local\\Temp\\ipykernel_15068\\3835145909.py:32: DeprecationWarning: \n",
      "Passing acquisition function parameters or gaussian process parameters to maximize\n",
      "is no longer supported, and will cause an error in future releases. Instead,\n",
      "please use the \"set_gp_params\" method to set the gp params, and pass an instance\n",
      " of bayes_opt.util.UtilityFunction using the acquisition_function argument\n",
      "\n",
      "  optimizer.maximize(init_points=10, n_iter=25, acq='ei')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Actual  Predicted  Category\n",
      "0      3.148958   4.022782   Testing\n",
      "1      2.232708   4.133459   Testing\n",
      "2      2.135833   4.506401  Training\n",
      "4      1.834792   5.354273  Training\n",
      "5      1.643333   4.485303  Training\n",
      "...         ...        ...       ...\n",
      "8326  37.864167  26.155128  Training\n",
      "8327  19.295000  26.751037  Training\n",
      "8328  29.965000  25.656431  Training\n",
      "8329  20.265833  23.980487  Training\n",
      "8330  22.930000  24.418632   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.75\n",
      "r2 test 0.71\n",
      "|   iter    |  target   | activa... | hidden... | max_iter  |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m-0.913   \u001b[0m | \u001b[0m0.417    \u001b[0m | \u001b[0m74.83    \u001b[0m | \u001b[0m200.0    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (227) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m2        \u001b[0m | \u001b[95m-0.823   \u001b[0m | \u001b[95m0.3023   \u001b[0m | \u001b[95m23.21    \u001b[0m | \u001b[95m227.7    \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m-0.8842  \u001b[0m | \u001b[0m0.1863   \u001b[0m | \u001b[0m41.1     \u001b[0m | \u001b[0m319.0    \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m-0.901   \u001b[0m | \u001b[0m0.5388   \u001b[0m | \u001b[0m47.73    \u001b[0m | \u001b[0m405.6    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (208) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m5        \u001b[0m | \u001b[0m-0.9271  \u001b[0m | \u001b[0m0.2045   \u001b[0m | \u001b[0m89.03    \u001b[0m | \u001b[0m208.2    \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m-0.901   \u001b[0m | \u001b[0m0.6705   \u001b[0m | \u001b[0m47.56    \u001b[0m | \u001b[0m367.6    \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m-0.8602  \u001b[0m | \u001b[0m0.1404   \u001b[0m | \u001b[0m27.83    \u001b[0m | \u001b[0m440.2    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m-0.8903  \u001b[0m | \u001b[0m0.9683   \u001b[0m | \u001b[0m38.21    \u001b[0m | \u001b[0m407.7    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m-0.9492  \u001b[0m | \u001b[0m0.8764   \u001b[0m | \u001b[0m90.51    \u001b[0m | \u001b[0m225.5    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m-0.8642  \u001b[0m | \u001b[0m0.03905  \u001b[0m | \u001b[0m25.28    \u001b[0m | \u001b[0m463.4    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m-0.871   \u001b[0m | \u001b[0m0.2927   \u001b[0m | \u001b[0m28.21    \u001b[0m | \u001b[0m440.1    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (226) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m12       \u001b[0m | \u001b[0m-0.8287  \u001b[0m | \u001b[0m0.678    \u001b[0m | \u001b[0m22.7     \u001b[0m | \u001b[0m226.4    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (228) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m13       \u001b[0m | \u001b[0m-0.8253  \u001b[0m | \u001b[0m0.4283   \u001b[0m | \u001b[0m20.75    \u001b[0m | \u001b[0m228.2    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (231) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m14       \u001b[0m | \u001b[0m-0.8295  \u001b[0m | \u001b[0m0.5612   \u001b[0m | \u001b[0m22.31    \u001b[0m | \u001b[0m231.6    \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m-0.8545  \u001b[0m | \u001b[0m0.1524   \u001b[0m | \u001b[0m23.3     \u001b[0m | \u001b[0m443.4    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (230) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m16       \u001b[0m | \u001b[0m-0.8556  \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m27.53    \u001b[0m | \u001b[0m230.7    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (233) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m17       \u001b[0m | \u001b[95m-0.8082  \u001b[0m | \u001b[95m0.6223   \u001b[0m | \u001b[95m16.2     \u001b[0m | \u001b[95m233.0    \u001b[0m |\n",
      "| \u001b[95m18       \u001b[0m | \u001b[95m-0.787   \u001b[0m | \u001b[95m0.8269   \u001b[0m | \u001b[95m13.17    \u001b[0m | \u001b[95m229.7    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (233) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m19       \u001b[0m | \u001b[95m-0.7351  \u001b[0m | \u001b[95m0.8165   \u001b[0m | \u001b[95m11.7     \u001b[0m | \u001b[95m233.5    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (238) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m20       \u001b[0m | \u001b[95m-0.7     \u001b[0m | \u001b[95m0.9899   \u001b[0m | \u001b[95m10.18    \u001b[0m | \u001b[95m238.3    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (243) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m21       \u001b[0m | \u001b[0m-0.748   \u001b[0m | \u001b[0m0.4635   \u001b[0m | \u001b[0m11.63    \u001b[0m | \u001b[0m243.1    \u001b[0m |\n",
      "| \u001b[0m22       \u001b[0m | \u001b[0m-0.787   \u001b[0m | \u001b[0m0.8079   \u001b[0m | \u001b[0m13.74    \u001b[0m | \u001b[0m238.7    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (249) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m23       \u001b[0m | \u001b[0m-0.7046  \u001b[0m | \u001b[0m0.9981   \u001b[0m | \u001b[0m10.39    \u001b[0m | \u001b[0m249.5    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (256) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m24       \u001b[0m | \u001b[0m-0.7082  \u001b[0m | \u001b[0m0.974    \u001b[0m | \u001b[0m10.12    \u001b[0m | \u001b[0m256.4    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (255) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m25       \u001b[0m | \u001b[0m-0.828   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m17.2     \u001b[0m | \u001b[0m255.1    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (264) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m26       \u001b[0m | \u001b[0m-0.7441  \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m264.8    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (276) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m27       \u001b[0m | \u001b[0m-0.7184  \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m276.2    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (275) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m28       \u001b[0m | \u001b[0m-0.8225  \u001b[0m | \u001b[0m0.3421   \u001b[0m | \u001b[0m19.07    \u001b[0m | \u001b[0m275.9    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (285) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m29       \u001b[0m | \u001b[0m-0.7123  \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m285.1    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (293) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m30       \u001b[0m | \u001b[0m-0.7521  \u001b[0m | \u001b[0m0.2518   \u001b[0m | \u001b[0m10.68    \u001b[0m | \u001b[0m294.0    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (290) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m31       \u001b[0m | \u001b[0m-0.827   \u001b[0m | \u001b[0m0.1241   \u001b[0m | \u001b[0m19.99    \u001b[0m | \u001b[0m290.2    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (309) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m32       \u001b[0m | \u001b[0m-0.757   \u001b[0m | \u001b[0m0.04017  \u001b[0m | \u001b[0m10.05    \u001b[0m | \u001b[0m309.7    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (322) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m33       \u001b[0m | \u001b[0m-0.7305  \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m322.4    \u001b[0m |\n",
      "| \u001b[0m34       \u001b[0m | \u001b[0m-0.834   \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m18.85    \u001b[0m | \u001b[0m318.7    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (332) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m35       \u001b[0m | \u001b[0m-0.7606  \u001b[0m | \u001b[0m0.06657  \u001b[0m | \u001b[0m10.59    \u001b[0m | \u001b[0m332.5    \u001b[0m |\n",
      "=============================================================\n",
      "Optimal Parameters: {'activation': 0.9898705759506914, 'hidden_layer_sizes': 10, 'max_iter': 238}\n",
      "         Actual  Predicted  Category\n",
      "0     97.440417  53.210874   Testing\n",
      "1     73.388750  55.140255   Testing\n",
      "2     66.655000  65.331886  Training\n",
      "4     57.921875  78.520996  Training\n",
      "5     58.833958  55.083030  Training\n",
      "...         ...        ...       ...\n",
      "8326  77.197500  79.410414  Training\n",
      "8327  71.974583  81.056082  Training\n",
      "8328  72.302500  80.357639  Training\n",
      "8329  70.609167  76.222076  Training\n",
      "8330  70.784167  70.607799   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.7\n",
      "r2 test 0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UTD\\UTDFall2023\\Calibration-of-LoRaNodes-using-Super-Learners\\.calibration\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (238) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"========================= Neural Network Regressor ================================\")\n",
    "%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Neural_Network_Regression_Optimized.ipynb\n",
    "\n",
    "for k,v in enumerate(Palas):\n",
    "    X = Palas[v].drop([v+\"Palas\"],axis = 1)\n",
    "    y = Palas[v][v+\"Palas\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state= 40)\n",
    "    # print(\"x train\",X_train.shape)\n",
    "    # print(\"x test\",X_test.shape)\n",
    "    # print(\"y train\",y_train.shape)\n",
    "    # print(\"y test\",y_test.shape)\n",
    "    \n",
    "    model,hyperparameters,r2_score_test = Neural_Network_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "    trained_model_nn[v] = model\n",
    "    hyperparameters_nn[v] = hyperparameters\n",
    "    r2_score_test_nn_optim[v] = r2_score_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "69c0ef71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= Ensemble Bagging Regressor ================================\n",
      "|   iter    |  target   | max_fe... | max_sa... | n_esti... |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.7712   \u001b[0m | \u001b[0m0.6873   \u001b[0m | \u001b[0m0.9754   \u001b[0m | \u001b[0m75.88    \u001b[0m |\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m0.8033   \u001b[0m | \u001b[95m0.7993   \u001b[0m | \u001b[95m0.578    \u001b[0m | \u001b[95m24.04    \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.5467   \u001b[0m | \u001b[0m0.529    \u001b[0m | \u001b[0m0.9331   \u001b[0m | \u001b[0m64.1     \u001b[0m |\n",
      "| \u001b[95m4        \u001b[0m | \u001b[95m0.8095   \u001b[0m | \u001b[95m0.854    \u001b[0m | \u001b[95m0.5103   \u001b[0m | \u001b[95m97.29    \u001b[0m |\n",
      "| \u001b[95m5        \u001b[0m | \u001b[95m0.8484   \u001b[0m | \u001b[95m0.9162   \u001b[0m | \u001b[95m0.6062   \u001b[0m | \u001b[95m26.36    \u001b[0m |\n",
      "| \u001b[95m6        \u001b[0m | \u001b[95m0.884    \u001b[0m | \u001b[95m1.0      \u001b[0m | \u001b[95m0.7439   \u001b[0m | \u001b[95m34.18    \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.5855   \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m43.91    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.5544   \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m86.88    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.6226   \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m30.71    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.8794   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.6404   \u001b[0m | \u001b[0m35.72    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.5577   \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m94.01    \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.8693   \u001b[0m | \u001b[0m0.9038   \u001b[0m | \u001b[0m0.8555   \u001b[0m | \u001b[0m99.98    \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.8503   \u001b[0m | \u001b[0m0.9122   \u001b[0m | \u001b[0m0.9835   \u001b[0m | \u001b[0m18.58    \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.7482   \u001b[0m | \u001b[0m0.6775   \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m15.29    \u001b[0m |\n",
      "| \u001b[95m15       \u001b[0m | \u001b[95m0.8958   \u001b[0m | \u001b[95m1.0      \u001b[0m | \u001b[95m1.0      \u001b[0m | \u001b[95m53.86    \u001b[0m |\n",
      "=============================================================\n",
      "Optimal Parameters: {'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 53}\n",
      "R2 Score Train: 0.99\n",
      "R2 Score Test: 0.9\n",
      "|   iter    |  target   | max_fe... | max_sa... | n_esti... |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.7949   \u001b[0m | \u001b[0m0.6873   \u001b[0m | \u001b[0m0.9754   \u001b[0m | \u001b[0m75.88    \u001b[0m |\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m0.8273   \u001b[0m | \u001b[95m0.7993   \u001b[0m | \u001b[95m0.578    \u001b[0m | \u001b[95m24.04    \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.5914   \u001b[0m | \u001b[0m0.529    \u001b[0m | \u001b[0m0.9331   \u001b[0m | \u001b[0m64.1     \u001b[0m |\n",
      "| \u001b[95m4        \u001b[0m | \u001b[95m0.8299   \u001b[0m | \u001b[95m0.854    \u001b[0m | \u001b[95m0.5103   \u001b[0m | \u001b[95m97.29    \u001b[0m |\n",
      "| \u001b[95m5        \u001b[0m | \u001b[95m0.8657   \u001b[0m | \u001b[95m0.9162   \u001b[0m | \u001b[95m0.6062   \u001b[0m | \u001b[95m26.36    \u001b[0m |\n",
      "| \u001b[95m6        \u001b[0m | \u001b[95m0.8925   \u001b[0m | \u001b[95m1.0      \u001b[0m | \u001b[95m0.6968   \u001b[0m | \u001b[95m34.63    \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.5989   \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m86.63    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.6499   \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m0.8223   \u001b[0m | \u001b[0m42.27    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.6654   \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m30.93    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.8814   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m36.33    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.6005   \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m93.97    \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.8749   \u001b[0m | \u001b[0m0.9038   \u001b[0m | \u001b[0m0.8555   \u001b[0m | \u001b[0m99.98    \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.8576   \u001b[0m | \u001b[0m0.9915   \u001b[0m | \u001b[0m0.9269   \u001b[0m | \u001b[0m18.76    \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.7302   \u001b[0m | \u001b[0m0.6264   \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m15.35    \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.6123   \u001b[0m | \u001b[0m0.5224   \u001b[0m | \u001b[0m0.5275   \u001b[0m | \u001b[0m21.12    \u001b[0m |\n",
      "=============================================================\n",
      "Optimal Parameters: {'max_features': 1.0, 'max_samples': 0.6968, 'n_estimators': 34}\n",
      "R2 Score Train: 0.98\n",
      "R2 Score Test: 0.89\n",
      "|   iter    |  target   | max_fe... | max_sa... | n_esti... |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.7936   \u001b[0m | \u001b[0m0.6873   \u001b[0m | \u001b[0m0.9754   \u001b[0m | \u001b[0m75.88    \u001b[0m |\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m0.8205   \u001b[0m | \u001b[95m0.7993   \u001b[0m | \u001b[95m0.578    \u001b[0m | \u001b[95m24.04    \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.5938   \u001b[0m | \u001b[0m0.529    \u001b[0m | \u001b[0m0.9331   \u001b[0m | \u001b[0m64.1     \u001b[0m |\n",
      "| \u001b[95m4        \u001b[0m | \u001b[95m0.8306   \u001b[0m | \u001b[95m0.854    \u001b[0m | \u001b[95m0.5103   \u001b[0m | \u001b[95m97.29    \u001b[0m |\n",
      "| \u001b[95m5        \u001b[0m | \u001b[95m0.8563   \u001b[0m | \u001b[95m0.9162   \u001b[0m | \u001b[95m0.6062   \u001b[0m | \u001b[95m26.36    \u001b[0m |\n",
      "| \u001b[95m6        \u001b[0m | \u001b[95m0.8855   \u001b[0m | \u001b[95m1.0      \u001b[0m | \u001b[95m0.6932   \u001b[0m | \u001b[95m34.86    \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.6083   \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m86.68    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.6574   \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m0.79     \u001b[0m | \u001b[0m42.31    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.6614   \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m31.08    \u001b[0m |\n",
      "| \u001b[95m10       \u001b[0m | \u001b[95m0.8884   \u001b[0m | \u001b[95m1.0      \u001b[0m | \u001b[95m0.5      \u001b[0m | \u001b[95m36.54    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.6105   \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m93.97    \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.8708   \u001b[0m | \u001b[0m0.9038   \u001b[0m | \u001b[0m0.8555   \u001b[0m | \u001b[0m99.98    \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.856    \u001b[0m | \u001b[0m0.9204   \u001b[0m | \u001b[0m0.9398   \u001b[0m | \u001b[0m18.39    \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.7197   \u001b[0m | \u001b[0m0.6252   \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m14.93    \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.6352   \u001b[0m | \u001b[0m0.5131   \u001b[0m | \u001b[0m0.9621   \u001b[0m | \u001b[0m20.85    \u001b[0m |\n",
      "=============================================================\n",
      "Optimal Parameters: {'max_features': 1.0, 'max_samples': 0.5, 'n_estimators': 36}\n",
      "R2 Score Train: 0.96\n",
      "R2 Score Test: 0.89\n",
      "|   iter    |  target   | max_fe... | max_sa... | n_esti... |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.7726   \u001b[0m | \u001b[0m0.6873   \u001b[0m | \u001b[0m0.9754   \u001b[0m | \u001b[0m75.88    \u001b[0m |\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m0.7963   \u001b[0m | \u001b[95m0.7993   \u001b[0m | \u001b[95m0.578    \u001b[0m | \u001b[95m24.04    \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.5919   \u001b[0m | \u001b[0m0.529    \u001b[0m | \u001b[0m0.9331   \u001b[0m | \u001b[0m64.1     \u001b[0m |\n",
      "| \u001b[95m4        \u001b[0m | \u001b[95m0.8074   \u001b[0m | \u001b[95m0.854    \u001b[0m | \u001b[95m0.5103   \u001b[0m | \u001b[95m97.29    \u001b[0m |\n",
      "| \u001b[95m5        \u001b[0m | \u001b[95m0.8249   \u001b[0m | \u001b[95m0.9162   \u001b[0m | \u001b[95m0.6062   \u001b[0m | \u001b[95m26.36    \u001b[0m |\n",
      "| \u001b[95m6        \u001b[0m | \u001b[95m0.8401   \u001b[0m | \u001b[95m0.9881   \u001b[0m | \u001b[95m0.8468   \u001b[0m | \u001b[95m35.59    \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.5957   \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m86.73    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.6328   \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m42.89    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.6421   \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m31.42    \u001b[0m |\n",
      "| \u001b[95m10       \u001b[0m | \u001b[95m0.8568   \u001b[0m | \u001b[95m1.0      \u001b[0m | \u001b[95m1.0      \u001b[0m | \u001b[95m37.55    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.5974   \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m93.97    \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.7556   \u001b[0m | \u001b[0m0.6698   \u001b[0m | \u001b[0m0.5162   \u001b[0m | \u001b[0m99.91    \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.8098   \u001b[0m | \u001b[0m0.8846   \u001b[0m | \u001b[0m0.9874   \u001b[0m | \u001b[0m20.23    \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.5705   \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m17.17    \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.6141   \u001b[0m | \u001b[0m0.5353   \u001b[0m | \u001b[0m0.985    \u001b[0m | \u001b[0m21.9     \u001b[0m |\n",
      "=============================================================\n",
      "Optimal Parameters: {'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 37}\n",
      "R2 Score Train: 0.98\n",
      "R2 Score Test: 0.86\n",
      "|   iter    |  target   | max_fe... | max_sa... | n_esti... |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.7186   \u001b[0m | \u001b[0m0.6873   \u001b[0m | \u001b[0m0.9754   \u001b[0m | \u001b[0m75.88    \u001b[0m |\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m0.73     \u001b[0m | \u001b[95m0.7993   \u001b[0m | \u001b[95m0.578    \u001b[0m | \u001b[95m24.04    \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.543    \u001b[0m | \u001b[0m0.529    \u001b[0m | \u001b[0m0.9331   \u001b[0m | \u001b[0m64.1     \u001b[0m |\n",
      "| \u001b[95m4        \u001b[0m | \u001b[95m0.7465   \u001b[0m | \u001b[95m0.854    \u001b[0m | \u001b[95m0.5103   \u001b[0m | \u001b[95m97.29    \u001b[0m |\n",
      "| \u001b[95m5        \u001b[0m | \u001b[95m0.7659   \u001b[0m | \u001b[95m0.9162   \u001b[0m | \u001b[95m0.6062   \u001b[0m | \u001b[95m26.36    \u001b[0m |\n",
      "| \u001b[95m6        \u001b[0m | \u001b[95m0.7917   \u001b[0m | \u001b[95m1.0      \u001b[0m | \u001b[95m0.6966   \u001b[0m | \u001b[95m33.75    \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.552    \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m86.66    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.6026   \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m41.16    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.6044   \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m30.45    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.7749   \u001b[0m | \u001b[0m0.925    \u001b[0m | \u001b[0m0.8759   \u001b[0m | \u001b[0m35.1     \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.5582   \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m100.0    \u001b[0m |\n",
      "| \u001b[95m12       \u001b[0m | \u001b[95m0.7933   \u001b[0m | \u001b[95m1.0      \u001b[0m | \u001b[95m0.5      \u001b[0m | \u001b[95m95.21    \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.6447   \u001b[0m | \u001b[0m0.5871   \u001b[0m | \u001b[0m0.9782   \u001b[0m | \u001b[0m92.86    \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.7688   \u001b[0m | \u001b[0m0.9348   \u001b[0m | \u001b[0m0.5094   \u001b[0m | \u001b[0m72.66    \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.5447   \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m70.4     \u001b[0m |\n",
      "=============================================================\n",
      "Optimal Parameters: {'max_features': 1.0, 'max_samples': 0.5, 'n_estimators': 95}\n",
      "R2 Score Train: 0.92\n",
      "R2 Score Test: 0.79\n",
      "|   iter    |  target   | max_fe... | max_sa... | n_esti... |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.7339   \u001b[0m | \u001b[0m0.6873   \u001b[0m | \u001b[0m0.9754   \u001b[0m | \u001b[0m75.88    \u001b[0m |\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m0.756    \u001b[0m | \u001b[95m0.7993   \u001b[0m | \u001b[95m0.578    \u001b[0m | \u001b[95m24.04    \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.5213   \u001b[0m | \u001b[0m0.529    \u001b[0m | \u001b[0m0.9331   \u001b[0m | \u001b[0m64.1     \u001b[0m |\n",
      "| \u001b[95m4        \u001b[0m | \u001b[95m0.773    \u001b[0m | \u001b[95m0.854    \u001b[0m | \u001b[95m0.5103   \u001b[0m | \u001b[95m97.29    \u001b[0m |\n",
      "| \u001b[95m5        \u001b[0m | \u001b[95m0.8107   \u001b[0m | \u001b[95m0.9162   \u001b[0m | \u001b[95m0.6062   \u001b[0m | \u001b[95m26.36    \u001b[0m |\n",
      "| \u001b[95m6        \u001b[0m | \u001b[95m0.8483   \u001b[0m | \u001b[95m1.0      \u001b[0m | \u001b[95m0.6775   \u001b[0m | \u001b[95m32.61    \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.5485   \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m42.54    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.5259   \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m87.15    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.8066   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m10.0     \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.5768   \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m15.53    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.6071   \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m29.81    \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.8394   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.5574   \u001b[0m | \u001b[0m33.66    \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.5795   \u001b[0m | \u001b[0m0.5149   \u001b[0m | \u001b[0m0.9332   \u001b[0m | \u001b[0m36.12    \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.6486   \u001b[0m | \u001b[0m0.6241   \u001b[0m | \u001b[0m0.9829   \u001b[0m | \u001b[0m11.7     \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.7908   \u001b[0m | \u001b[0m0.8009   \u001b[0m | \u001b[0m0.7092   \u001b[0m | \u001b[0m99.3     \u001b[0m |\n",
      "=============================================================\n",
      "Optimal Parameters: {'max_features': 1.0, 'max_samples': 0.6775, 'n_estimators': 32}\n",
      "R2 Score Train: 0.96\n",
      "R2 Score Test: 0.85\n"
     ]
    }
   ],
   "source": [
    "print(\"========================= Ensemble Bagging Regressor ================================\")\n",
    "%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Ensemble_Bagging_Regression_Optimized.ipynb\n",
    "\n",
    "for k,v in enumerate(Palas):\n",
    "    X = Palas[v].drop([v+\"Palas\"],axis = 1)\n",
    "    y = Palas[v][v+\"Palas\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state= 40)\n",
    "    # print(\"x train\",X_train.shape)\n",
    "    # print(\"x test\",X_test.shape)\n",
    "    # print(\"y train\",y_train.shape)\n",
    "    # print(\"y test\",y_test.shape)\n",
    "    \n",
    "    model,hyperparameters,r2_score_test = Ensemble_Bagging_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "    trained_model_br[v] = model\n",
    "    hyperparameters_br[v] = hyperparameters\n",
    "    r2_score_test_br_optim[v] = r2_score_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "511b1470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= LGBM Regressor ================================\n",
      "|   iter    |  target   | learni... | max_depth | min_ch... | n_esti... | num_le... |\n",
      "-------------------------------------------------------------------------------------\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000320 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4.948597\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.8741   \u001b[0m | \u001b[0m0.1309   \u001b[0m | \u001b[0m23.01    \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m186.0    \u001b[0m | \u001b[0m24.4     \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000146 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4.948597\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m0.8656   \u001b[0m | \u001b[0m0.03678  \u001b[0m | \u001b[0m9.657    \u001b[0m | \u001b[0m16.91    \u001b[0m | \u001b[0m228.5    \u001b[0m | \u001b[0m36.16    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000236 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4.948597\n",
      "| \u001b[95m3        \u001b[0m | \u001b[95m0.8785   \u001b[0m | \u001b[95m0.1316   \u001b[0m | \u001b[95m22.13    \u001b[0m | \u001b[95m14.09    \u001b[0m | \u001b[95m445.2    \u001b[0m | \u001b[95m20.82    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000146 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4.948597\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.8731   \u001b[0m | \u001b[0m0.2044   \u001b[0m | \u001b[0m15.43    \u001b[0m | \u001b[0m21.17    \u001b[0m | \u001b[0m113.2    \u001b[0m | \u001b[0m25.94    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000389 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4.948597\n",
      "| \u001b[95m5        \u001b[0m | \u001b[95m0.8939   \u001b[0m | \u001b[95m0.2422   \u001b[0m | \u001b[95m29.21    \u001b[0m | \u001b[95m16.27    \u001b[0m | \u001b[95m361.5    \u001b[0m | \u001b[95m46.29    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000387 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4.948597\n",
      "| \u001b[95m6        \u001b[0m | \u001b[95m0.8989   \u001b[0m | \u001b[95m0.1428   \u001b[0m | \u001b[95m28.69    \u001b[0m | \u001b[95m13.62    \u001b[0m | \u001b[95m360.7    \u001b[0m | \u001b[95m44.39    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000160 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4.948597\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.8824   \u001b[0m | \u001b[0m0.0245   \u001b[0m | \u001b[0m27.56    \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m351.8    \u001b[0m | \u001b[0m40.23    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000147 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4.948597\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.8529   \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m28.03    \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m366.4    \u001b[0m | \u001b[0m42.57    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000237 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4.948597\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.8981   \u001b[0m | \u001b[0m0.1721   \u001b[0m | \u001b[0m28.45    \u001b[0m | \u001b[0m14.68    \u001b[0m | \u001b[0m358.8    \u001b[0m | \u001b[0m46.18    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000147 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4.948597\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.8801   \u001b[0m | \u001b[0m0.2925   \u001b[0m | \u001b[0m30.0     \u001b[0m | \u001b[0m16.04    \u001b[0m | \u001b[0m358.3    \u001b[0m | \u001b[0m42.36    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000241 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4.948597\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.8932   \u001b[0m | \u001b[0m0.1264   \u001b[0m | \u001b[0m25.14    \u001b[0m | \u001b[0m13.54    \u001b[0m | \u001b[0m360.0    \u001b[0m | \u001b[0m42.12    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000240 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4.948597\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.8934   \u001b[0m | \u001b[0m0.1774   \u001b[0m | \u001b[0m24.63    \u001b[0m | \u001b[0m17.02    \u001b[0m | \u001b[0m361.2    \u001b[0m | \u001b[0m47.65    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000263 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4.948597\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.8978   \u001b[0m | \u001b[0m0.1145   \u001b[0m | \u001b[0m23.84    \u001b[0m | \u001b[0m11.99    \u001b[0m | \u001b[0m357.2    \u001b[0m | \u001b[0m45.23    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000160 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4.948597\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.892    \u001b[0m | \u001b[0m0.2686   \u001b[0m | \u001b[0m28.57    \u001b[0m | \u001b[0m11.72    \u001b[0m | \u001b[0m357.8    \u001b[0m | \u001b[0m49.74    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000151 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4.948597\n",
      "| \u001b[95m15       \u001b[0m | \u001b[95m0.8996   \u001b[0m | \u001b[95m0.1674   \u001b[0m | \u001b[95m20.72    \u001b[0m | \u001b[95m14.84    \u001b[0m | \u001b[95m357.9    \u001b[0m | \u001b[95m48.32    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000370 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4.948597\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m0.8921   \u001b[0m | \u001b[0m0.2118   \u001b[0m | \u001b[0m19.08    \u001b[0m | \u001b[0m14.89    \u001b[0m | \u001b[0m353.1    \u001b[0m | \u001b[0m46.84    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000150 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4.948597\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m0.8937   \u001b[0m | \u001b[0m0.2312   \u001b[0m | \u001b[0m16.2     \u001b[0m | \u001b[0m16.06    \u001b[0m | \u001b[0m362.0    \u001b[0m | \u001b[0m43.27    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000162 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4.948597\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m0.8625   \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m17.33    \u001b[0m | \u001b[0m10.45    \u001b[0m | \u001b[0m359.1    \u001b[0m | \u001b[0m47.31    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000257 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4.948597\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m0.8912   \u001b[0m | \u001b[0m0.08146  \u001b[0m | \u001b[0m21.3     \u001b[0m | \u001b[0m18.18    \u001b[0m | \u001b[0m358.5    \u001b[0m | \u001b[0m46.03    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000231 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4.948597\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.8988   \u001b[0m | \u001b[0m0.1556   \u001b[0m | \u001b[0m24.14    \u001b[0m | \u001b[0m13.22    \u001b[0m | \u001b[0m351.9    \u001b[0m | \u001b[0m48.78    \u001b[0m |\n",
      "=====================================================================================\n",
      "Optimal Parameters: {'learning_rate': 0.1674, 'max_depth': 20, 'min_child_samples': 14, 'n_estimators': 357, 'num_leaves': 48}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000153 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4.948597\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "R2 Score Train: 1.0\n",
      "R2 Score Test: 0.9\n",
      "|   iter    |  target   | learni... | max_depth | min_ch... | n_esti... | num_le... |\n",
      "-------------------------------------------------------------------------------------\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000189 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 8.596943\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.8829   \u001b[0m | \u001b[0m0.1309   \u001b[0m | \u001b[0m23.01    \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m186.0    \u001b[0m | \u001b[0m24.4     \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000151 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 8.596943\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m0.876    \u001b[0m | \u001b[0m0.03678  \u001b[0m | \u001b[0m9.657    \u001b[0m | \u001b[0m16.91    \u001b[0m | \u001b[0m228.5    \u001b[0m | \u001b[0m36.16    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000238 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 8.596943\n",
      "| \u001b[95m3        \u001b[0m | \u001b[95m0.8852   \u001b[0m | \u001b[95m0.1316   \u001b[0m | \u001b[95m22.13    \u001b[0m | \u001b[95m14.09    \u001b[0m | \u001b[95m445.2    \u001b[0m | \u001b[95m20.82    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000316 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 8.596943\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.8743   \u001b[0m | \u001b[0m0.2044   \u001b[0m | \u001b[0m15.43    \u001b[0m | \u001b[0m21.17    \u001b[0m | \u001b[0m113.2    \u001b[0m | \u001b[0m25.94    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000239 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 8.596943\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.8793   \u001b[0m | \u001b[0m0.2422   \u001b[0m | \u001b[0m29.21    \u001b[0m | \u001b[0m16.27    \u001b[0m | \u001b[0m361.5    \u001b[0m | \u001b[0m46.29    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000149 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 8.596943\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.8828   \u001b[0m | \u001b[0m0.09664  \u001b[0m | \u001b[0m23.45    \u001b[0m | \u001b[0m12.73    \u001b[0m | \u001b[0m445.7    \u001b[0m | \u001b[0m21.49    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000243 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 8.596943\n",
      "| \u001b[95m7        \u001b[0m | \u001b[95m0.8862   \u001b[0m | \u001b[95m0.1017   \u001b[0m | \u001b[95m17.77    \u001b[0m | \u001b[95m13.48    \u001b[0m | \u001b[95m443.5    \u001b[0m | \u001b[95m22.22    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000148 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 8.596943\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.8455   \u001b[0m | \u001b[0m0.01679  \u001b[0m | \u001b[0m17.51    \u001b[0m | \u001b[0m21.74    \u001b[0m | \u001b[0m443.9    \u001b[0m | \u001b[0m22.53    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000246 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 8.596943\n",
      "| \u001b[95m9        \u001b[0m | \u001b[95m0.8908   \u001b[0m | \u001b[95m0.268    \u001b[0m | \u001b[95m23.81    \u001b[0m | \u001b[95m10.21    \u001b[0m | \u001b[95m439.2    \u001b[0m | \u001b[95m23.13    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000446 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 8.596943\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.8807   \u001b[0m | \u001b[0m0.04921  \u001b[0m | \u001b[0m18.05    \u001b[0m | \u001b[0m10.31    \u001b[0m | \u001b[0m437.7    \u001b[0m | \u001b[0m26.33    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000262 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 8.596943\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.8551   \u001b[0m | \u001b[0m0.02248  \u001b[0m | \u001b[0m23.73    \u001b[0m | \u001b[0m12.75    \u001b[0m | \u001b[0m434.9    \u001b[0m | \u001b[0m20.19    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000147 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 8.596943\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.8829   \u001b[0m | \u001b[0m0.07214  \u001b[0m | \u001b[0m20.17    \u001b[0m | \u001b[0m13.18    \u001b[0m | \u001b[0m444.1    \u001b[0m | \u001b[0m24.93    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000162 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 8.596943\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.8864   \u001b[0m | \u001b[0m0.2207   \u001b[0m | \u001b[0m24.15    \u001b[0m | \u001b[0m12.54    \u001b[0m | \u001b[0m441.9    \u001b[0m | \u001b[0m23.96    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000149 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 8.596943\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.8899   \u001b[0m | \u001b[0m0.1732   \u001b[0m | \u001b[0m25.09    \u001b[0m | \u001b[0m10.26    \u001b[0m | \u001b[0m438.2    \u001b[0m | \u001b[0m29.84    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000332 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 8.596943\n",
      "| \u001b[95m15       \u001b[0m | \u001b[95m0.893    \u001b[0m | \u001b[95m0.1766   \u001b[0m | \u001b[95m25.44    \u001b[0m | \u001b[95m11.57    \u001b[0m | \u001b[95m443.2    \u001b[0m | \u001b[95m32.76    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000160 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 8.596943\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m0.8841   \u001b[0m | \u001b[0m0.2322   \u001b[0m | \u001b[0m29.32    \u001b[0m | \u001b[0m11.32    \u001b[0m | \u001b[0m439.1    \u001b[0m | \u001b[0m33.62    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000162 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 8.596943\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m0.8874   \u001b[0m | \u001b[0m0.247    \u001b[0m | \u001b[0m26.12    \u001b[0m | \u001b[0m10.18    \u001b[0m | \u001b[0m445.1    \u001b[0m | \u001b[0m38.05    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000159 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 8.596943\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m0.893    \u001b[0m | \u001b[0m0.05101  \u001b[0m | \u001b[0m22.8     \u001b[0m | \u001b[0m14.03    \u001b[0m | \u001b[0m441.1    \u001b[0m | \u001b[0m37.53    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000148 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 8.596943\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m0.891    \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m20.28    \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m441.9    \u001b[0m | \u001b[0m34.69    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000339 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 8.596943\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.8923   \u001b[0m | \u001b[0m0.1814   \u001b[0m | \u001b[0m24.52    \u001b[0m | \u001b[0m18.22    \u001b[0m | \u001b[0m446.2    \u001b[0m | \u001b[0m36.09    \u001b[0m |\n",
      "=====================================================================================\n",
      "Optimal Parameters: {'learning_rate': 0.1766, 'max_depth': 25, 'min_child_samples': 11, 'n_estimators': 443, 'num_leaves': 32}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000245 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 8.596943\n",
      "R2 Score Train: 1.0\n",
      "R2 Score Test: 0.89\n",
      "|   iter    |  target   | learni... | max_depth | min_ch... | n_esti... | num_le... |\n",
      "-------------------------------------------------------------------------------------\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000236 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 13.462858\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.8807   \u001b[0m | \u001b[0m0.1309   \u001b[0m | \u001b[0m23.01    \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m186.0    \u001b[0m | \u001b[0m24.4     \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000214 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 13.462858\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m0.8671   \u001b[0m | \u001b[0m0.03678  \u001b[0m | \u001b[0m9.657    \u001b[0m | \u001b[0m16.91    \u001b[0m | \u001b[0m228.5    \u001b[0m | \u001b[0m36.16    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000157 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 13.462858\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.8794   \u001b[0m | \u001b[0m0.1316   \u001b[0m | \u001b[0m22.13    \u001b[0m | \u001b[0m14.09    \u001b[0m | \u001b[0m445.2    \u001b[0m | \u001b[0m20.82    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000145 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 13.462858\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.8619   \u001b[0m | \u001b[0m0.2044   \u001b[0m | \u001b[0m15.43    \u001b[0m | \u001b[0m21.17    \u001b[0m | \u001b[0m113.2    \u001b[0m | \u001b[0m25.94    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000151 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 13.462858\n",
      "| \u001b[95m5        \u001b[0m | \u001b[95m0.8833   \u001b[0m | \u001b[95m0.2422   \u001b[0m | \u001b[95m29.21    \u001b[0m | \u001b[95m16.27    \u001b[0m | \u001b[95m361.5    \u001b[0m | \u001b[95m46.29    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000152 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 13.462858\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.8806   \u001b[0m | \u001b[0m0.09664  \u001b[0m | \u001b[0m23.45    \u001b[0m | \u001b[0m12.73    \u001b[0m | \u001b[0m445.7    \u001b[0m | \u001b[0m21.49    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000153 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 13.462858\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.8725   \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m30.0     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m450.8    \u001b[0m | \u001b[0m43.52    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000308 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 13.462858\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.8765   \u001b[0m | \u001b[0m0.2026   \u001b[0m | \u001b[0m20.46    \u001b[0m | \u001b[0m17.8     \u001b[0m | \u001b[0m370.7    \u001b[0m | \u001b[0m36.06    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000143 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 13.462858\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.8707   \u001b[0m | \u001b[0m0.2966   \u001b[0m | \u001b[0m30.0     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m347.7    \u001b[0m | \u001b[0m45.41    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000243 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 13.462858\n",
      "| \u001b[95m10       \u001b[0m | \u001b[95m0.884    \u001b[0m | \u001b[95m0.1649   \u001b[0m | \u001b[95m30.0     \u001b[0m | \u001b[95m23.14    \u001b[0m | \u001b[95m367.9    \u001b[0m | \u001b[95m50.0     \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000147 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 13.462858\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.8733   \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m23.26    \u001b[0m | \u001b[0m30.0     \u001b[0m | \u001b[0m358.2    \u001b[0m | \u001b[0m50.0     \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000142 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 13.462858\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.88     \u001b[0m | \u001b[0m0.01968  \u001b[0m | \u001b[0m30.0     \u001b[0m | \u001b[0m12.51    \u001b[0m | \u001b[0m372.0    \u001b[0m | \u001b[0m50.0     \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000162 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 13.462858\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[95m13       \u001b[0m | \u001b[95m0.8871   \u001b[0m | \u001b[95m0.1793   \u001b[0m | \u001b[95m28.96    \u001b[0m | \u001b[95m29.16    \u001b[0m | \u001b[95m382.1    \u001b[0m | \u001b[95m47.71    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000148 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 13.462858\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.8618   \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m22.0     \u001b[0m | \u001b[0m30.0     \u001b[0m | \u001b[0m391.2    \u001b[0m | \u001b[0m50.0     \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000141 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 13.462858\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.8815   \u001b[0m | \u001b[0m0.2182   \u001b[0m | \u001b[0m27.81    \u001b[0m | \u001b[0m24.95    \u001b[0m | \u001b[0m378.0    \u001b[0m | \u001b[0m49.98    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000238 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 13.462858\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m0.8774   \u001b[0m | \u001b[0m0.2652   \u001b[0m | \u001b[0m30.0     \u001b[0m | \u001b[0m30.0     \u001b[0m | \u001b[0m377.5    \u001b[0m | \u001b[0m43.5     \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000143 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 13.462858\n",
      "| \u001b[95m17       \u001b[0m | \u001b[95m0.8883   \u001b[0m | \u001b[95m0.07261  \u001b[0m | \u001b[95m28.99    \u001b[0m | \u001b[95m15.78    \u001b[0m | \u001b[95m368.1    \u001b[0m | \u001b[95m44.31    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000147 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 13.462858\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m0.8593   \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m30.0     \u001b[0m | \u001b[0m20.98    \u001b[0m | \u001b[0m366.6    \u001b[0m | \u001b[0m42.35    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000221 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 13.462858\n",
      "| \u001b[95m19       \u001b[0m | \u001b[95m0.8911   \u001b[0m | \u001b[95m0.1579   \u001b[0m | \u001b[95m29.85    \u001b[0m | \u001b[95m13.33    \u001b[0m | \u001b[95m366.5    \u001b[0m | \u001b[95m44.03    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000143 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 13.462858\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.888    \u001b[0m | \u001b[0m0.1021   \u001b[0m | \u001b[0m26.51    \u001b[0m | \u001b[0m11.36    \u001b[0m | \u001b[0m366.8    \u001b[0m | \u001b[0m39.72    \u001b[0m |\n",
      "=====================================================================================\n",
      "Optimal Parameters: {'learning_rate': 0.1579, 'max_depth': 29, 'min_child_samples': 13, 'n_estimators': 366, 'num_leaves': 44}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000140 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 13.462858\n",
      "R2 Score Train: 0.99\n",
      "R2 Score Test: 0.89\n",
      "|   iter    |  target   | learni... | max_depth | min_ch... | n_esti... | num_le... |\n",
      "-------------------------------------------------------------------------------------\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000140 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 16.449515\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.8453   \u001b[0m | \u001b[0m0.1309   \u001b[0m | \u001b[0m23.01    \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m186.0    \u001b[0m | \u001b[0m24.4     \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000140 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 16.449515\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m0.84     \u001b[0m | \u001b[0m0.03678  \u001b[0m | \u001b[0m9.657    \u001b[0m | \u001b[0m16.91    \u001b[0m | \u001b[0m228.5    \u001b[0m | \u001b[0m36.16    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000226 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 16.449515\n",
      "| \u001b[95m3        \u001b[0m | \u001b[95m0.8464   \u001b[0m | \u001b[95m0.1316   \u001b[0m | \u001b[95m22.13    \u001b[0m | \u001b[95m14.09    \u001b[0m | \u001b[95m445.2    \u001b[0m | \u001b[95m20.82    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000144 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 16.449515\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.8406   \u001b[0m | \u001b[0m0.2044   \u001b[0m | \u001b[0m15.43    \u001b[0m | \u001b[0m21.17    \u001b[0m | \u001b[0m113.2    \u001b[0m | \u001b[0m25.94    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000139 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 16.449515\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.8411   \u001b[0m | \u001b[0m0.2422   \u001b[0m | \u001b[0m29.21    \u001b[0m | \u001b[0m16.27    \u001b[0m | \u001b[0m361.5    \u001b[0m | \u001b[0m46.29    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000144 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 16.449515\n",
      "| \u001b[95m6        \u001b[0m | \u001b[95m0.8517   \u001b[0m | \u001b[95m0.09664  \u001b[0m | \u001b[95m23.45    \u001b[0m | \u001b[95m12.73    \u001b[0m | \u001b[95m445.7    \u001b[0m | \u001b[95m21.49    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000242 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 16.449515\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.8402   \u001b[0m | \u001b[0m0.2871   \u001b[0m | \u001b[0m23.87    \u001b[0m | \u001b[0m13.36    \u001b[0m | \u001b[0m443.9    \u001b[0m | \u001b[0m21.87    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000240 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 16.449515\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.8421   \u001b[0m | \u001b[0m0.2235   \u001b[0m | \u001b[0m22.26    \u001b[0m | \u001b[0m11.92    \u001b[0m | \u001b[0m353.6    \u001b[0m | \u001b[0m39.56    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000225 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 16.449515\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.8473   \u001b[0m | \u001b[0m0.1227   \u001b[0m | \u001b[0m28.14    \u001b[0m | \u001b[0m26.98    \u001b[0m | \u001b[0m313.9    \u001b[0m | \u001b[0m26.58    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000232 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 16.449515\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.8472   \u001b[0m | \u001b[0m0.03346  \u001b[0m | \u001b[0m26.25    \u001b[0m | \u001b[0m26.03    \u001b[0m | \u001b[0m359.6    \u001b[0m | \u001b[0m42.68    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000143 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 16.449515\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.8258   \u001b[0m | \u001b[0m0.07782  \u001b[0m | \u001b[0m8.434    \u001b[0m | \u001b[0m27.68    \u001b[0m | \u001b[0m52.65    \u001b[0m | \u001b[0m42.47    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000243 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 16.449515\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.8487   \u001b[0m | \u001b[0m0.0988   \u001b[0m | \u001b[0m23.49    \u001b[0m | \u001b[0m12.3     \u001b[0m | \u001b[0m447.0    \u001b[0m | \u001b[0m20.4     \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000251 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 16.449515\n",
      "| \u001b[95m13       \u001b[0m | \u001b[95m0.8521   \u001b[0m | \u001b[95m0.05624  \u001b[0m | \u001b[95m24.24    \u001b[0m | \u001b[95m26.09    \u001b[0m | \u001b[95m359.0    \u001b[0m | \u001b[95m40.32    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000243 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 16.449515\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.8481   \u001b[0m | \u001b[0m0.04587  \u001b[0m | \u001b[0m21.59    \u001b[0m | \u001b[0m24.71    \u001b[0m | \u001b[0m360.8    \u001b[0m | \u001b[0m39.12    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000298 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 16.449515\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.8452   \u001b[0m | \u001b[0m0.02958  \u001b[0m | \u001b[0m24.03    \u001b[0m | \u001b[0m25.72    \u001b[0m | \u001b[0m359.9    \u001b[0m | \u001b[0m38.18    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000144 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 16.449515\n",
      "| \u001b[95m16       \u001b[0m | \u001b[95m0.8547   \u001b[0m | \u001b[95m0.1064   \u001b[0m | \u001b[95m22.12    \u001b[0m | \u001b[95m24.17    \u001b[0m | \u001b[95m359.9    \u001b[0m | \u001b[95m41.67    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000240 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 16.449515\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m0.8292   \u001b[0m | \u001b[0m0.298    \u001b[0m | \u001b[0m21.1     \u001b[0m | \u001b[0m21.82    \u001b[0m | \u001b[0m357.9    \u001b[0m | \u001b[0m43.21    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000314 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 16.449515\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m0.8445   \u001b[0m | \u001b[0m0.1595   \u001b[0m | \u001b[0m23.44    \u001b[0m | \u001b[0m27.02    \u001b[0m | \u001b[0m356.7    \u001b[0m | \u001b[0m41.14    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000149 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 16.449515\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m0.8348   \u001b[0m | \u001b[0m0.2928   \u001b[0m | \u001b[0m23.44    \u001b[0m | \u001b[0m24.17    \u001b[0m | \u001b[0m360.0    \u001b[0m | \u001b[0m40.05    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000242 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 16.449515\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.8475   \u001b[0m | \u001b[0m0.1069   \u001b[0m | \u001b[0m23.88    \u001b[0m | \u001b[0m27.42    \u001b[0m | \u001b[0m359.2    \u001b[0m | \u001b[0m38.24    \u001b[0m |\n",
      "=====================================================================================\n",
      "Optimal Parameters: {'learning_rate': 0.1064, 'max_depth': 22, 'min_child_samples': 24, 'n_estimators': 359, 'num_leaves': 41}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000235 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 16.449515\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "R2 Score Train: 0.97\n",
      "R2 Score Test: 0.85\n",
      "|   iter    |  target   | learni... | max_depth | min_ch... | n_esti... | num_le... |\n",
      "-------------------------------------------------------------------------------------\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000323 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 19.166156\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.7731   \u001b[0m | \u001b[0m0.1309   \u001b[0m | \u001b[0m23.01    \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m186.0    \u001b[0m | \u001b[0m24.4     \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000323 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 19.166156\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m0.7737   \u001b[0m | \u001b[95m0.03678  \u001b[0m | \u001b[95m9.657    \u001b[0m | \u001b[95m16.91    \u001b[0m | \u001b[95m228.5    \u001b[0m | \u001b[95m36.16    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000159 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 19.166156\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.7697   \u001b[0m | \u001b[0m0.1316   \u001b[0m | \u001b[0m22.13    \u001b[0m | \u001b[0m14.09    \u001b[0m | \u001b[0m445.2    \u001b[0m | \u001b[0m20.82    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000147 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 19.166156\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.771    \u001b[0m | \u001b[0m0.2044   \u001b[0m | \u001b[0m15.43    \u001b[0m | \u001b[0m21.17    \u001b[0m | \u001b[0m113.2    \u001b[0m | \u001b[0m25.94    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000147 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 19.166156\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.7505   \u001b[0m | \u001b[0m0.2422   \u001b[0m | \u001b[0m29.21    \u001b[0m | \u001b[0m16.27    \u001b[0m | \u001b[0m361.5    \u001b[0m | \u001b[0m46.29    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000145 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 19.166156\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.7659   \u001b[0m | \u001b[0m0.2633   \u001b[0m | \u001b[0m17.75    \u001b[0m | \u001b[0m20.42    \u001b[0m | \u001b[0m114.4    \u001b[0m | \u001b[0m25.96    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000257 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 19.166156\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.7522   \u001b[0m | \u001b[0m0.256    \u001b[0m | \u001b[0m13.53    \u001b[0m | \u001b[0m19.49    \u001b[0m | \u001b[0m224.1    \u001b[0m | \u001b[0m35.27    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000529 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 19.166156\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.7679   \u001b[0m | \u001b[0m0.145    \u001b[0m | \u001b[0m9.342    \u001b[0m | \u001b[0m19.13    \u001b[0m | \u001b[0m230.1    \u001b[0m | \u001b[0m34.93    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000162 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 19.166156\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.7524   \u001b[0m | \u001b[0m0.2415   \u001b[0m | \u001b[0m19.12    \u001b[0m | \u001b[0m10.2     \u001b[0m | \u001b[0m187.0    \u001b[0m | \u001b[0m27.31    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000271 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 19.166156\n",
      "| \u001b[95m10       \u001b[0m | \u001b[95m0.7761   \u001b[0m | \u001b[95m0.1051   \u001b[0m | \u001b[95m24.56    \u001b[0m | \u001b[95m12.22    \u001b[0m | \u001b[95m188.6    \u001b[0m | \u001b[95m24.73    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000159 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 19.166156\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.7625   \u001b[0m | \u001b[0m0.2957   \u001b[0m | \u001b[0m25.19    \u001b[0m | \u001b[0m12.07    \u001b[0m | \u001b[0m191.7    \u001b[0m | \u001b[0m23.71    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000152 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 19.166156\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.7686   \u001b[0m | \u001b[0m0.07429  \u001b[0m | \u001b[0m25.62    \u001b[0m | \u001b[0m13.04    \u001b[0m | \u001b[0m185.9    \u001b[0m | \u001b[0m24.77    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000227 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 19.166156\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[95m13       \u001b[0m | \u001b[95m0.7802   \u001b[0m | \u001b[95m0.06045  \u001b[0m | \u001b[95m11.54    \u001b[0m | \u001b[95m14.39    \u001b[0m | \u001b[95m231.0    \u001b[0m | \u001b[95m36.14    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000306 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 19.166156\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.7755   \u001b[0m | \u001b[0m0.1347   \u001b[0m | \u001b[0m11.7     \u001b[0m | \u001b[0m13.64    \u001b[0m | \u001b[0m233.9    \u001b[0m | \u001b[0m39.55    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000310 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 19.166156\n",
      "| \u001b[95m15       \u001b[0m | \u001b[95m0.7811   \u001b[0m | \u001b[95m0.03559  \u001b[0m | \u001b[95m13.78    \u001b[0m | \u001b[95m13.15    \u001b[0m | \u001b[95m234.3    \u001b[0m | \u001b[95m36.33    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000146 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 19.166156\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m0.7736   \u001b[0m | \u001b[0m0.1566   \u001b[0m | \u001b[0m15.81    \u001b[0m | \u001b[0m15.46    \u001b[0m | \u001b[0m232.2    \u001b[0m | \u001b[0m33.57    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000151 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 19.166156\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m0.7554   \u001b[0m | \u001b[0m0.2552   \u001b[0m | \u001b[0m11.93    \u001b[0m | \u001b[0m15.46    \u001b[0m | \u001b[0m235.2    \u001b[0m | \u001b[0m33.62    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000379 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 19.166156\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m0.7626   \u001b[0m | \u001b[0m0.2215   \u001b[0m | \u001b[0m14.5     \u001b[0m | \u001b[0m12.08    \u001b[0m | \u001b[0m233.2    \u001b[0m | \u001b[0m40.47    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000237 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 19.166156\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m0.7753   \u001b[0m | \u001b[0m0.1529   \u001b[0m | \u001b[0m13.24    \u001b[0m | \u001b[0m14.33    \u001b[0m | \u001b[0m228.1    \u001b[0m | \u001b[0m38.29    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000146 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 19.166156\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.764    \u001b[0m | \u001b[0m0.1923   \u001b[0m | \u001b[0m12.91    \u001b[0m | \u001b[0m13.82    \u001b[0m | \u001b[0m233.1    \u001b[0m | \u001b[0m38.21    \u001b[0m |\n",
      "=====================================================================================\n",
      "Optimal Parameters: {'learning_rate': 0.0356, 'max_depth': 13, 'min_child_samples': 13, 'n_estimators': 234, 'num_leaves': 36}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000353 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 19.166156\n",
      "R2 Score Train: 0.87\n",
      "R2 Score Test: 0.78\n",
      "|   iter    |  target   | learni... | max_depth | min_ch... | n_esti... | num_le... |\n",
      "-------------------------------------------------------------------------------------\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000334 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 108.936655\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.8344   \u001b[0m | \u001b[0m0.1309   \u001b[0m | \u001b[0m23.01    \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m186.0    \u001b[0m | \u001b[0m24.4     \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000308 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 108.936655\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m0.8184   \u001b[0m | \u001b[0m0.03678  \u001b[0m | \u001b[0m9.657    \u001b[0m | \u001b[0m16.91    \u001b[0m | \u001b[0m228.5    \u001b[0m | \u001b[0m36.16    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000236 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 108.936655\n",
      "| \u001b[95m3        \u001b[0m | \u001b[95m0.8423   \u001b[0m | \u001b[95m0.1316   \u001b[0m | \u001b[95m22.13    \u001b[0m | \u001b[95m14.09    \u001b[0m | \u001b[95m445.2    \u001b[0m | \u001b[95m20.82    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000144 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 108.936655\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.8223   \u001b[0m | \u001b[0m0.2044   \u001b[0m | \u001b[0m15.43    \u001b[0m | \u001b[0m21.17    \u001b[0m | \u001b[0m113.2    \u001b[0m | \u001b[0m25.94    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000144 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 108.936655\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.8374   \u001b[0m | \u001b[0m0.2422   \u001b[0m | \u001b[0m29.21    \u001b[0m | \u001b[0m16.27    \u001b[0m | \u001b[0m361.5    \u001b[0m | \u001b[0m46.29    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000159 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 108.936655\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.8402   \u001b[0m | \u001b[0m0.09664  \u001b[0m | \u001b[0m23.45    \u001b[0m | \u001b[0m12.73    \u001b[0m | \u001b[0m445.7    \u001b[0m | \u001b[0m21.49    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000232 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 108.936655\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.8345   \u001b[0m | \u001b[0m0.2829   \u001b[0m | \u001b[0m13.22    \u001b[0m | \u001b[0m21.44    \u001b[0m | \u001b[0m436.4    \u001b[0m | \u001b[0m21.44    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000260 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 108.936655\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.834    \u001b[0m | \u001b[0m0.1776   \u001b[0m | \u001b[0m21.64    \u001b[0m | \u001b[0m23.39    \u001b[0m | \u001b[0m450.1    \u001b[0m | \u001b[0m22.39    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000144 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 108.936655\n",
      "| \u001b[95m9        \u001b[0m | \u001b[95m0.8463   \u001b[0m | \u001b[95m0.203    \u001b[0m | \u001b[95m13.24    \u001b[0m | \u001b[95m13.2     \u001b[0m | \u001b[95m445.3    \u001b[0m | \u001b[95m20.18    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000142 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 108.936655\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.8352   \u001b[0m | \u001b[0m0.2525   \u001b[0m | \u001b[0m7.553    \u001b[0m | \u001b[0m10.5     \u001b[0m | \u001b[0m458.9    \u001b[0m | \u001b[0m21.8     \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000305 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 108.936655\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.8455   \u001b[0m | \u001b[0m0.1761   \u001b[0m | \u001b[0m15.8     \u001b[0m | \u001b[0m10.38    \u001b[0m | \u001b[0m442.4    \u001b[0m | \u001b[0m32.55    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000163 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 108.936655\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.8324   \u001b[0m | \u001b[0m0.1494   \u001b[0m | \u001b[0m6.484    \u001b[0m | \u001b[0m10.86    \u001b[0m | \u001b[0m445.2    \u001b[0m | \u001b[0m35.42    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000238 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 108.936655\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.8288   \u001b[0m | \u001b[0m0.3      \u001b[0m | \u001b[0m16.35    \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m438.4    \u001b[0m | \u001b[0m24.18    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000254 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 108.936655\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.8371   \u001b[0m | \u001b[0m0.2405   \u001b[0m | \u001b[0m15.04    \u001b[0m | \u001b[0m13.3     \u001b[0m | \u001b[0m445.3    \u001b[0m | \u001b[0m26.16    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000162 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 108.936655\n",
      "| \u001b[95m15       \u001b[0m | \u001b[95m0.8529   \u001b[0m | \u001b[95m0.1213   \u001b[0m | \u001b[95m19.92    \u001b[0m | \u001b[95m10.42    \u001b[0m | \u001b[95m444.0    \u001b[0m | \u001b[95m33.25    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000254 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 108.936655\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m0.84     \u001b[0m | \u001b[0m0.2328   \u001b[0m | \u001b[0m18.25    \u001b[0m | \u001b[0m10.42    \u001b[0m | \u001b[0m447.9    \u001b[0m | \u001b[0m37.1     \u001b[0m |\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000219 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 108.936655\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m0.8519   \u001b[0m | \u001b[0m0.1517   \u001b[0m | \u001b[0m22.89    \u001b[0m | \u001b[0m12.61    \u001b[0m | \u001b[0m446.9    \u001b[0m | \u001b[0m31.13    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000152 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 108.936655\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m0.8365   \u001b[0m | \u001b[0m0.03526  \u001b[0m | \u001b[0m22.66    \u001b[0m | \u001b[0m13.81    \u001b[0m | \u001b[0m440.4    \u001b[0m | \u001b[0m32.4     \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000146 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 108.936655\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m0.8409   \u001b[0m | \u001b[0m0.2958   \u001b[0m | \u001b[0m27.07    \u001b[0m | \u001b[0m10.86    \u001b[0m | \u001b[0m449.6    \u001b[0m | \u001b[0m29.69    \u001b[0m |\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000143 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 108.936655\n",
      "| \u001b[95m20       \u001b[0m | \u001b[95m0.8546   \u001b[0m | \u001b[95m0.09438  \u001b[0m | \u001b[95m21.5     \u001b[0m | \u001b[95m10.81    \u001b[0m | \u001b[95m445.6    \u001b[0m | \u001b[95m32.6     \u001b[0m |\n",
      "=====================================================================================\n",
      "Optimal Parameters: {'learning_rate': 0.0944, 'max_depth': 21, 'min_child_samples': 10, 'n_estimators': 445, 'num_leaves': 32}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000236 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 108.936655\n",
      "R2 Score Train: 0.97\n",
      "R2 Score Test: 0.85\n"
     ]
    }
   ],
   "source": [
    "print(\"========================= LGBM Regressor ================================\")\n",
    "%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/LGBM_Regression_Optimized.ipynb\n",
    "\n",
    "for k,v in enumerate(Palas):\n",
    "    X = Palas[v].drop([v+\"Palas\"],axis = 1)\n",
    "    y = Palas[v][v+\"Palas\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state= 40)\n",
    "    # print(\"x train\",X_train.shape)\n",
    "    # print(\"x test\",X_test.shape)\n",
    "    # print(\"y train\",y_train.shape)\n",
    "    # print(\"y test\",y_test.shape)\n",
    "    \n",
    "    model,hyperparameters,r2_score_test  =  LGBM_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "    trained_model_lgbm[v] = model\n",
    "    hyperparameters_lgbm[v] = hyperparameters\n",
    "    r2_score_test_lgbm_optim[v] = r2_score_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "085ac933-86e6-4705-bf74-09aca2ed1def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k,v in enumerate(Palas):\n",
    "#     X = Palas[v].drop([v+\"Palas\"],axis = 1)\n",
    "#     y = Palas[v][v+\"Palas\"]\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state= 40)\n",
    "#     print(\"x train\",X_train.shape)\n",
    "#     print(\"x test\",X_test.shape)\n",
    "#     print(\"y train\",y_train.shape)\n",
    "#     print(\"y test\",y_test.shape)\n",
    "\n",
    "#     # print(\"========================= Random Forest Regressor ================================\")\n",
    "#     # model,hyperparameters,r2_score_test = Random_Forest_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "#     # trained_model_rf[v] = model\n",
    "#     # hyperparameters_rf[v] = hyperparameters\n",
    "#     # r2_score_test_rf[v] = r2_score_test\n",
    "\n",
    "\n",
    "#     # print(\"========================= Decision Tree Regressor ================================\")\n",
    "#     # model,hyperparameters,r2_score_test = Decision_Tree_Regression(X_train,X_test,y_train,y_test,filtered_data)   \n",
    "#     # trained_model_dt[v] = model\n",
    "#     # hyperparameters_dt[v] = hyperparameters\n",
    "#     # r2_score_test_dt[v] = r2_score_test\n",
    "\n",
    "\n",
    "#     # print(\"========================= LGBM Regressor ================================\")\n",
    "#     # model,hyperparameters,r2_score_test  =  LGBM_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "#     # trained_model_lgbm[v] = model\n",
    "#     # hyperparameters_lgbm[v] = hyperparameters\n",
    "#     # r2_score_test_lgbm[v] = r2_score_test\n",
    "\n",
    "#     # print(\"========================= Ensemble Bagging Regressor ================================\")\n",
    "#     # model,hyperparameters,r2_score_test = Ensemble_Bagging_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "#     # trained_model_br[v] = model\n",
    "#     # hyperparameters_br[v] = hyperparameters\n",
    "#     # r2_score_test_br[v] = r2_score_test\n",
    "\n",
    "#     # print(\"========================= Neural Network Regressor ================================\")\n",
    "#     # model,hyperparameters,r2_score_test = Neural_Network_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "#     # trained_model_nn[v] = model\n",
    "#     # hyperparameters_nn[v] = hyperparameters\n",
    "#     # r2_score_test_nn[v] = r2_score_test\n",
    "\n",
    "#     print(\"========================= Linear Regressor ================================\")\n",
    "#     Linear_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "\n",
    "\n",
    "# # # #     print(\"========================= Gaussian Regressor ================================\")\n",
    "# # # # #    Gaussian_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "\n",
    "\n",
    "\n",
    "# # # #     print(\"========================= Ridge Regressor ================================\")\n",
    "# # # #     Ridge_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "\n",
    "# # # #     print(\"========================= KNN Regressor ================================\")\n",
    "# # # #     KNN_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "899f55b4-5f99-4dd7-bbe3-ee41148b4e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Target Variable</th>\n",
       "      <th>pm1</th>\n",
       "      <th>pm2_5</th>\n",
       "      <th>pm4</th>\n",
       "      <th>pm10</th>\n",
       "      <th>pmTotal</th>\n",
       "      <th>dCn</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ensemble Bagging</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBM</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Network</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Target Variable    pm1  pm2_5   pm4  pm10  pmTotal   dCn\n",
       "Model                                                   \n",
       "Random Forest     0.89   0.90  0.89  0.86     0.80  0.86\n",
       "Ensemble Bagging  0.90   0.89  0.89  0.86     0.79  0.85\n",
       "LGBM              0.90   0.89  0.89  0.85     0.78  0.85\n",
       "Neural Network    0.74   0.82  0.81  0.76     0.71  0.65"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################### Run this cell again ################################\n",
    "combined_dict ={key: [r2_score_test_rf_optim.get(key), r2_score_test_br_optim.get(key),r2_score_test_lgbm_optim.get(key), r2_score_test_nn_optim.get(key)] for key in r2_score_test_nn_optim}\n",
    "r2_values = combined_dict\n",
    "# Creating a DataFrame from the dictionary\n",
    "models = [\"Random Forest\", \"Ensemble Bagging\",\"LGBM\", \"Neural Network\"]\n",
    "r2_values\n",
    "df_updated_r2 = pd.DataFrame(r2_values, index=models)\n",
    "df_updated_r2.index.name = 'Model'\n",
    "df_updated_r2.columns.name = 'Target Variable'\n",
    "\n",
    "df_updated_r2.to_csv(\"After_filtering_and_Tuned_R2.csv\",index=True)\n",
    "df_updated_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4b8ab38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "hyperparameter_dict_after_filtering ={key: [hyperparameters_rf.get(key), hyperparameters_br.get(key),hyperparameters_lgbm.get(key)] for key in hyperparameters_lgbm}\n",
    "pickle.dump(hyperparameter_dict_after_filtering, open(\"hyperparameter_dict_after_filtering.p\", \"wb\"))\n",
    "pickle.dump(Palas, open(\"Palas.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cc06e0c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pm1': [{'max_depth': 30,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 46},\n",
       "  {'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 53},\n",
       "  {'learning_rate': 0.1674,\n",
       "   'max_depth': 20,\n",
       "   'min_child_samples': 14,\n",
       "   'n_estimators': 357,\n",
       "   'num_leaves': 48}],\n",
       " 'pm2_5': [{'max_depth': 24,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 32},\n",
       "  {'max_features': 1.0, 'max_samples': 0.6968, 'n_estimators': 34},\n",
       "  {'learning_rate': 0.1766,\n",
       "   'max_depth': 25,\n",
       "   'min_child_samples': 11,\n",
       "   'n_estimators': 443,\n",
       "   'num_leaves': 32}],\n",
       " 'pm4': [{'max_depth': 19,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 41},\n",
       "  {'max_features': 1.0, 'max_samples': 0.5, 'n_estimators': 36},\n",
       "  {'learning_rate': 0.1579,\n",
       "   'max_depth': 29,\n",
       "   'min_child_samples': 13,\n",
       "   'n_estimators': 366,\n",
       "   'num_leaves': 44}],\n",
       " 'pm10': [{'max_depth': 23,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 91},\n",
       "  {'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 37},\n",
       "  {'learning_rate': 0.1064,\n",
       "   'max_depth': 22,\n",
       "   'min_child_samples': 24,\n",
       "   'n_estimators': 359,\n",
       "   'num_leaves': 41}],\n",
       " 'pmTotal': [{'max_depth': 23,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 91},\n",
       "  {'max_features': 1.0, 'max_samples': 0.5, 'n_estimators': 95},\n",
       "  {'learning_rate': 0.0356,\n",
       "   'max_depth': 13,\n",
       "   'min_child_samples': 13,\n",
       "   'n_estimators': 234,\n",
       "   'num_leaves': 36}],\n",
       " 'dCn': [{'max_depth': 23,\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 81},\n",
       "  {'max_features': 1.0, 'max_samples': 0.6775, 'n_estimators': 32},\n",
       "  {'learning_rate': 0.0944,\n",
       "   'max_depth': 21,\n",
       "   'min_child_samples': 10,\n",
       "   'n_estimators': 445,\n",
       "   'num_leaves': 32}]}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"hyperparameter_dict_after_filtering.p\", 'rb') as f:\n",
    "    hyperparameter_dict_after_filtering= pickle.load(f)\n",
    "hyperparameter_dict_after_filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4952f1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= Stacking  Regressor  =  Linear Regressor ================================\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000156 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4.948597\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000233 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 3916, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4.923943\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000358 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4.940425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000324 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4.956135\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000239 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4.951894\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000206 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 4.970582\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "        Actual  Predicted  Category\n",
      "0     2.718542   1.458578   Testing\n",
      "1     2.067500   1.840839   Testing\n",
      "2     1.904375   1.704284  Training\n",
      "4     1.540208   1.424383  Training\n",
      "5     1.505000   1.412155  Training\n",
      "...        ...        ...       ...\n",
      "8326  4.982917   4.881658  Training\n",
      "8327  4.544583   4.621553  Training\n",
      "8328  4.834167   4.797415  Training\n",
      "8329  4.537917   4.607185  Training\n",
      "8330  4.430000   4.704604   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.99\n",
      "r2 test 0.9\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000272 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 8.596943\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000570 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 3916, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 8.567252\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000232 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 8.560356\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000321 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 8.591212\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000283 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 8.624063\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000362 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 8.641826\n",
      "         Actual  Predicted  Category\n",
      "0      2.820000   1.877822   Testing\n",
      "1      2.130000   2.131558   Testing\n",
      "2      2.000000   1.687906  Training\n",
      "4      1.661458   1.424853  Training\n",
      "5      1.545208   1.325419  Training\n",
      "...         ...        ...       ...\n",
      "8326  12.055000  11.838364  Training\n",
      "8327   9.235833  10.089922  Training\n",
      "8328  10.393750  10.715925  Training\n",
      "8329   9.765417  10.254346  Training\n",
      "8330   9.886667  10.957790   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.99\n",
      "r2 test 0.9\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000345 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 13.462858\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000192 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 3916, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 13.430824\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000218 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 13.384571\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000269 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 13.431217\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000270 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 13.530998\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000283 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 13.536672\n",
      "         Actual  Predicted  Category\n",
      "0      3.085417   1.803779   Testing\n",
      "1      2.232708   2.020194   Testing\n",
      "2      2.135833   1.739885  Training\n",
      "4      1.834792   1.863664  Training\n",
      "5      1.643333   1.456288  Training\n",
      "...         ...        ...       ...\n",
      "8326  21.223333  20.759407  Training\n",
      "8327  15.932083  17.774590  Training\n",
      "8328  16.646667  17.695829  Training\n",
      "8329  16.986667  17.002071  Training\n",
      "8330  17.401667  17.738441   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.99\n",
      "r2 test 0.89\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000338 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 16.449515\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000317 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 3916, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 16.395062\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000272 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 16.357173\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000120 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 16.397727\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000232 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 16.550529\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000410 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 16.547072\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "         Actual  Predicted  Category\n",
      "0      3.112500   3.475490   Testing\n",
      "1      2.231875   2.108810   Testing\n",
      "2      2.132083   1.417415  Training\n",
      "4      1.824792   1.311491  Training\n",
      "5      1.640833   1.514103  Training\n",
      "...         ...        ...       ...\n",
      "8326  31.216250  29.126878  Training\n",
      "8327  17.556667  22.256189  Training\n",
      "8328  24.890000  25.476195  Training\n",
      "8329  18.576667  20.053199  Training\n",
      "8330  20.421250  23.371030   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.98\n",
      "r2 test 0.86\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000273 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 19.166156\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000289 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 3916, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 19.098892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000385 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 19.061176\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000398 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 19.078987\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000208 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 19.301172\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000225 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 19.290535\n",
      "         Actual  Predicted  Category\n",
      "0      3.148958   2.745350   Testing\n",
      "1      2.232708   2.179954   Testing\n",
      "2      2.135833   1.806685  Training\n",
      "4      1.834792   1.681864  Training\n",
      "5      1.643333   1.444083  Training\n",
      "...         ...        ...       ...\n",
      "8326  37.864167  34.016261  Training\n",
      "8327  19.295000  29.191280  Training\n",
      "8328  29.965000  30.829755  Training\n",
      "8329  20.265833  24.881324  Training\n",
      "8330  22.930000  26.405880   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.94\n",
      "r2 test 0.8\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000266 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 4896, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 108.936655\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000215 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 3916, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 108.291859\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000409 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 109.023613\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001062 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 109.162239\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000293 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 108.893231\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000240 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2090\n",
      "[LightGBM] [Info] Number of data points in the train set: 3917, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 109.312168\n",
      "         Actual  Predicted  Category\n",
      "0     97.440417  58.442796   Testing\n",
      "1     73.388750  67.969501   Testing\n",
      "2     66.655000  67.845602  Training\n",
      "4     57.921875  58.374122  Training\n",
      "5     58.833958  56.818274  Training\n",
      "...         ...        ...       ...\n",
      "8326  77.197500  75.007344  Training\n",
      "8327  71.974583  73.396505  Training\n",
      "8328  72.302500  73.553525  Training\n",
      "8329  70.609167  71.449509  Training\n",
      "8330  70.784167  74.898300   Testing\n",
      "\n",
      "[6528 rows x 3 columns]\n",
      "r2 train 0.98\n",
      "r2 test 0.86\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pm1': 0.9,\n",
       " 'pm2_5': 0.9,\n",
       " 'pm4': 0.89,\n",
       " 'pm10': 0.86,\n",
       " 'pmTotal': 0.8,\n",
       " 'dCn': 0.86}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score_test_stacking_optim_after = {}\n",
    "print(\"========================= Stacking  Regressor  =  Linear Regressor ================================\")\n",
    "%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Stacking_Regression_Linear_Regression.ipynb\n",
    "\n",
    "for k,v in enumerate(Palas):\n",
    "    X = Palas[v].drop([v+\"Palas\"],axis = 1)\n",
    "    y = Palas[v][v+\"Palas\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state= 40)\n",
    "    # print(\"x train\",X_train.shape)\n",
    "    # print(\"x test\",X_test.shape)\n",
    "    # print(\"y train\",y_train.shape)\n",
    "    # print(\"y test\",y_test.shape)\n",
    "    \n",
    "    r2_score_test  =  Stacking_Regression(X_train,X_test,y_train,y_test,filtered_data,hyperparameter_dict_after_filtering[v])\n",
    "    r2_score_test_stacking_optim_after[v] = r2_score_test\n",
    "r2_score_test_stacking_optim_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "be44e6f3-f926-4a98-b8dd-ff7330a77f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     print(\"========================= Random Forest Regressor ================================\")\n",
    "#     model,hyperparameters = Random_Forest_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "#     trained_model_rf[v] = model\n",
    "#     hyperparameters_rf[v] = hyperparameters\n",
    "    \n",
    "#     print(\"========================= Decision Tree Regressor ================================\")\n",
    "#     model,hyperparameters = Decision_Tree_Regression(X_train,X_test,y_train,y_test,filtered_data)   \n",
    "#     trained_model_dt[v] = model\n",
    "#     hyperparameters_dt[v] = hyperparameters\n",
    "#     # print(\"========================= LGBM Regressor ================================\")\n",
    "#     # model,hyperparameters =  LGBM_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "#     # trained_model_lgbm[v] = model\n",
    "#     # hyperparameters_lgbm[v] = hyperparameters\n",
    "# # #     print(\"========================= Gaussian Regressor ================================\")\n",
    "# # # #    Gaussian_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "\n",
    "#     print(\"========================= Ensemble Bagging Regressor ================================\")\n",
    "#     model,hyperparameters = Ensemble_Bagging_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "#     trained_model_br[v] = model\n",
    "#     hyperparameters_br[v] = hyperparameters\n",
    "# # #     print(\"========================= Ridge Regressor ================================\")\n",
    "# # #     Ridge_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "    \n",
    "# # #     print(\"========================= KNN Regressor ================================\")\n",
    "# # #     KNN_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "    \n",
    "#     print(\"========================= Neural Network Regressor ================================\")\n",
    "#     model,hyperparameters = Neural_Network_Regression(X_train,X_test,y_train,y_test,filtered_data)\n",
    "#     trained_model_nn[v] = model\n",
    "#     hyperparameters_nn[v] = hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4b1eab1a-c733-4c5b-9dce-4ad81c688e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"hyperparameters_nn.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(hyperparameters_nn, f)\n",
    "# with open('hyperparameters_nn.pkl', 'rb') as file:\n",
    "#     data = pickle.load(file)\n",
    "# print(data)\n",
    "\n",
    "# with open(\"trained_model_nn.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(trained_model_nn, f)\n",
    "# with open('trained_model_nn.pkl', 'rb') as file:\n",
    "#     data = pickle.load(file)\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4803d203-06f3-487e-ad3b-c74211081933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"hyperparameters_rf.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(hyperparameters_rf, f)\n",
    "# with open('hyperparameters_rf.pkl', 'rb') as file:\n",
    "#     data = pickle.load(file)\n",
    "# print(data)\n",
    "\n",
    "# with open(\"trained_model_rf.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(trained_model_rf, f)\n",
    "# with open('trained_model_rf.pkl', 'rb') as file:\n",
    "#     data = pickle.load(file)\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0c28bfc8-e85c-4d15-ad23-e4bdbf10a953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"hyperparameters_dt.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(hyperparameters_dt, f)\n",
    "# with open('hyperparameters_dt.pkl', 'rb') as file:\n",
    "#     data = pickle.load(file)\n",
    "# print(data)\n",
    "\n",
    "# with open(\"trained_model_dt.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(trained_model_dt, f)\n",
    "# with open('trained_model_dt.pkl', 'rb') as file:\n",
    "#     data = pickle.load(file)\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "04785894-9afd-4c3d-82be-2689b17bdae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"hyperparameters_br.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(hyperparameters_br, f)\n",
    "# with open('hyperparameters_br.pkl', 'rb') as file:\n",
    "#     data = pickle.load(file)\n",
    "# print(data)\n",
    "\n",
    "# with open(\"trained_model_br.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(trained_model_br, f)\n",
    "# with open('trained_model_br.pkl', 'rb') as file:\n",
    "#     data = pickle.load(file)\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6e9d9eac-0b54-4f9b-91e4-fc16cfe0c161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"hyperparameters_lgbm.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(hyperparameters_lgbm, f)\n",
    "# with open('hyperparameters_lgbm.pkl', 'rb') as file:\n",
    "#     data = pickle.load(file)\n",
    "# print(data)\n",
    "\n",
    "# with open(\"trained_model_lgbm.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(trained_model_lgbm, f)\n",
    "# with open('trained_model_lgbm.pkl', 'rb') as file:\n",
    "#     data = pickle.load(file)\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7759e146-4c9a-4974-a136-b053e96b652c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Stacking_Regression.ipynb\n",
    "# for k,v in enumerate(Palas):\n",
    "#     #v='pm1'\n",
    "#     X = Palas[v].drop([v+\"Palas\"],axis = 1)\n",
    "#     y = Palas[v][v+\"Palas\"]\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state= 40)\n",
    "#     print(\"x train\",X_train.shape)\n",
    "#     print(\"x test\",X_test.shape)\n",
    "#     print(\"y train\",y_train.shape)\n",
    "#     print(\"y test\",y_test.shape)\n",
    "#     print(\"========================= Stacking Regressor ================================\")\n",
    "#     Stacking_Regression(X_train,X_test,y_train,y_test,filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "96251911-13fc-4e04-bb23-99e229a2b789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for k,v in enumerate(Palas):\n",
    "# v='pm1'\n",
    "# X = Palas[v].drop([v+\"Palas\"],axis = 1)\n",
    "# y = Palas[v][v+\"Palas\"]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state= 40)\n",
    "# print(\"x train\",X_train.shape)\n",
    "# print(\"x test\",X_test.shape)\n",
    "# print(\"y train\",y_train.shape)\n",
    "# print(\"y test\",y_test.shape)\n",
    "\n",
    "\n",
    "# print(\"========================= Linear Regressor ================================\")\n",
    "# Linear_Regression(X_train,X_test,y_train,y_test,filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f34272-dbbf-4244-b920-5e3b1a7ee7df",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
