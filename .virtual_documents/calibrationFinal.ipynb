import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy
import pickle
# from statsmodels.graphics.gofplots import qqplot

from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPRegressor
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Lasso
from sklearn.svm import SVR
import sklearn.gaussian_process as gp
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn import metrics
import matplotlib.dates as mdates
import lightgbm as lgb
from sklearn.linear_model import Ridge
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.ensemble import BaggingRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import StackingRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import r2_score
from skopt import gp_minimize
from sklearn.model_selection import cross_val_score
from sklearn.metrics import r2_score
from skopt import gp_minimize
from skopt.space import Real, Categorical, Integer
from lightgbm import LGBMRegressor
import nbformat
from bayes_opt import BayesianOptimization
import xgboost as xgb
from statsmodels.graphics.gofplots import qqplot_2samples
import random


# Set random seed
np.random.seed(101)
random.seed(101)


data = pd.read_csv(r'D:\UTD\UTDFall2023\Calibration-of-LoRaNodes-using-Super-Learners\data\calibrate.csv')
data


data.columns


df= pd.concat([data.iloc[:,6], data.iloc[:,0:6],data.iloc[:,15:data.shape[1]]],axis = 1)
df.info()


def data_checker(df):
    # data_frame = df.iloc[:,1:16]
    data_frame =  df.iloc[:,7:-3] 
    # Palas limits are 0-100 mg/m3
    #BME limits are :Temp -40C to 85C
                   #:Pressure 300hPa to 1100 hPa or  300*100Pa to 1100*100 Pa
                   #:Humidity 0% to 100%
     #PPD42NS :Operating Temp is 0C to 45C
    idx = data_frame[(data_frame['Temperature_loRa']>=0) & (data_frame['Temperature_loRa']<=45) &
                    (data_frame['Pressure_loRa']>=300*100) & (data_frame['Pressure_loRa']<=1100*100) &
                    (data_frame['Humidity_loRa']>=0) & (data_frame['Humidity_loRa']<=100) &
                    (data_frame['pm1Palas']>=0) & (data_frame['pm2_5Palas']>=0) &
                    (data_frame['pm4Palas']>=0) & (data_frame['pm10Palas']>=0) &
                    (data_frame['pmTotalPalas']>=0) & (data_frame['dCnPalas']>=0)&
                    (data_frame['pm1Palas']<=100000) & (data_frame['pm2_5Palas']<=100000) &
                    (data_frame['pm4Palas']<=100000) & (data_frame['pm10Palas']<=100000) &
                    (data_frame['pmTotalPalas']<=100000) & (data_frame['dCnPalas']<=100000)].index
                    # &
                    # (data_frame['P1_conc_loRa'] + data_frame['P2_conc_loRa']>=0) &
                    # (data_frame['P1_conc_loRa'] + data_frame['P2_conc_loRa']<=28000)].index
    return df.loc[idx]
df = data_checker(df)
df


df.describe()


dict_col_names = {'pm1Palas':'PM$_{1.0}$ in $\mu g/m^3 $ (Palas)', 
                  'pm2_5Palas': 'PM$_{2.5}$ in $\mu g/m^3$ (Palas)',
                  'pm4Palas': 'PM$_{4.0}$ in $\mu g/m^3$ (Palas)',
                  'pm10Palas': 'PM$_{10.0}$ in $\mu g/m^3$ (Palas)',
                  'pmTotalPalas': 'Total PM Concentration in $\mu g/m^3$ (Palas)', 
                  'dCnPalas': 'Particle Count Density in #/cm$^{3}$ (Palas)', 
                  'P1_lpo_loRa': '> 1 μm LPO (LoRa)',
                  'P1_ratio_loRa': '> 1 μm Ratio (LoRa)',
                  'P1_conc_loRa':'> 1 μm Concentration in $\mu g/m^3$ (LoRa)' , 
                  'P2_lpo_loRa': '> 2.5 μm LPO (LoRa)',
                  'P2_ratio_loRa': '> 2.5 μm Ratio (LoRa)', 
                  'P2_conc_loRa': '> 2.5 μm Concentration in $\mu g/m^3$ (LoRa)',
                  'Temperature_loRa': 'Temperature in ℃ (LoRa)', 
                  'Pressure_loRa': 'Pressure in Pa (LoRa)',
                  'Humidity_loRa': 'Humidity in % (LoRa)'} 


# def hist_plots(df):
#     for col in df.columns[1:len(df.columns)]:
#         sns.histplot(df[col], kde = True, color = 'blue', bins = 30)
#         plt.title(f'{dict_col_names[col]} Histogram with KDE')
#         plt.xlabel(col)
#         plt.ylabel('Count')
#         plt.show()
# hist_plots(df)
# def box_plots(df):
#     for col in df.columns[1:len(df.columns)]:
#         plt.boxplot(df[col])
#         plt.title(f'{dict_col_names[col]} BoxPlot')
#         plt.xlabel(col)
#         plt.ylabel('Count')
#         plt.show()
# box_plots(df)


filtered_data = df.iloc[:,7:-3]
filtered_data. describe()


col_name = list(filtered_data)
x =[]
y_Palas  = []
for i in col_name:
    if "_loRa" in i:
        x.append(i)
    if "Palas" in i:
        y_Palas.append(i)
Palas = {}
for i in y_Palas:
    Palas_cols = x + [i]
    Palas[i[:-len("Palas")]] = filtered_data[Palas_cols]
y_Palas


def plot_residuals(predict_test,y_test,filtered_data):
    idx = X_test.index
    data_test = filtered_data.loc[idx]
    data_test["dateTime"] = pd.to_datetime(data_test["dateTime"])
    residuals = (np.array(y_test) - predict_test)**2
    plt.figure()
    plt.scatter(data_test["dateTime"], residuals)
    plt.xticks(rotation = 45)
    plt.xlabel('Date Time')
    plt.ylabel(y_test.name)
    plt.show()


dict_col_regression = {'pm1Palas':'PM$_{1.0}$', 
                       'pm2_5Palas': 'PM$_{2.5}$',
                       'pm4Palas': 'PM$_{4.0}$',
                       'pm10Palas': 'PM$_{10.0}$',
                       'pmTotalPalas': 'Total PM Concentration', 
                       'dCnPalas': 'Particle Count Density',
                       'dateTime': 'Date Time',
                       'P1_lpo_loRa':'> 1 μm LPO',
                       'P1_ratio_loRa': '> 1 μm ratio', 
                       'P1_conc_loRa': '> 1 μm Concentration', 
                       'P2_lpo_loRa': '> 2.5 μm LPO', 
                       'P2_ratio_loRa': '> 2.5 μm ratio',
                       'P2_conc_loRa': '> 2.5 μm Concentration' , 
                       'Temperature_loRa': 'Temperature', 
                       'Pressure_loRa': 'Pressure', 
                       'Humidity_loRa':'Humidity'}
unit_regression = {'pm_conc':'(μg/m³)',
                   'dCn':'(#/cm³)'} 


%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Linear_Regression.ipynb
%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Decision_Tree_Regression.ipynb
%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Ensemble_Bagging_Regression.ipynb
%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/LGBM_Regression.ipynb
%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Ridge_Regression.ipynb
%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/KNN_Regression.ipynb
%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Neural_Network_Regression.ipynb
%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/XGBoost_Regression.ipynb
%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Random_Forest_Regression.ipynb
%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Stacking_Regression.ipynb






r2_score_train_lr ={}
r2_score_train_rr = {}
r2_score_train_knn = {}
r2_score_train_dt ={}
r2_score_train_xgb = {}
r2_score_train_lgbm ={}
r2_score_train_br ={}
r2_score_train_nn ={}
r2_score_train_rf ={}

r2_score_test_lr ={}
r2_score_test_rr = {}
r2_score_test_knn = {}
r2_score_test_dt ={}
r2_score_test_xgb = {}
r2_score_test_lgbm ={}
r2_score_test_br ={}
r2_score_test_nn ={}
r2_score_test_rf ={}


# r2_score_test_sl = {}

for k,v in enumerate(Palas):
    X = Palas[v].drop([v+"Palas"],axis = 1)
    y = Palas[v][v+"Palas"]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,random_state = 20)
    X_train.to_csv("X_train.csv")
    X_test.to_csv("X_test.csv")
    y_train.to_csv("y_train.csv")
    y_test.to_csv("y_test.csv")

    r2_score_test_lr[v],r2_score_train_lr[v] =  Linear_Regression(X_train,X_test,y_train,y_test,filtered_data)
    r2_score_test_rr[v],r2_score_train_rr[v] = Ridge_Regression(X_train,X_test,y_train,y_test,filtered_data)
    r2_score_test_knn[v],r2_score_train_knn[v] = KNN_Regression(X_train,X_test,y_train,y_test,filtered_data)
    r2_score_test_dt[v],r2_score_train_dt[v] = Decision_Tree_Regression(X_train,X_test,y_train,y_test,filtered_data)   
    r2_score_test_xgb[v],r2_score_train_xgb[v] = XGBoost_Regression(X_train,X_test,y_train,y_test,filtered_data)
    r2_score_test_lgbm[v],r2_score_train_lgbm[v]  =  LGBM_Regression(X_train,X_test,y_train,y_test,filtered_data)
    r2_score_test_br[v],r2_score_train_br[v] = Ensemble_Bagging_Regression(X_train,X_test,y_train,y_test,filtered_data)
    r2_score_test_nn[v],r2_score_train_nn[v] = Neural_Network_Regression(X_train,X_test,y_train,y_test,filtered_data)
    r2_score_test_rf[v],r2_score_train_rf[v] = Random_Forest_Regression(X_train,X_test,y_train,y_test,filtered_data)
    
   





import pandas as pd

models = [
    "Linear_Regression",
    "Random_Forest_Regression",
    "XGBoost_Regression",
    "Decision_Tree_Regression",
    "LGBM_Regression",
    "Ensemble_Bagging_Regression",
    "Neural_Network_Regression",
    "Ridge_Regression",
    "KNN_Regression"
]

v_values = ["pm1", "pm2_5", "pm4", "pm10", "pmTotal", "dCn"]

# Initialize a DataFrame to store the R^2 scores
data = {
    "Model": models
}

# Populate the R^2 scores
for v in v_values:
    data[v] = [
        r2_score_train_lr[v],
        r2_score_train_rf[v],
        r2_score_train_xgb[v],
        r2_score_train_dt[v],
        r2_score_train_lgbm[v],
        r2_score_train_br[v],
        r2_score_train_nn[v],
        r2_score_train_rr[v],
        r2_score_train_knn[v]
    ]

# Create the DataFrame
r2_score_table = pd.DataFrame(data)

# Calculate the overall R^2 average for each model
r2_score_table["R2_Average"] = r2_score_table[v_values].mean(axis=1)

# Sort the table by the overall R^2 average in descending order
r2_score_table = r2_score_table.sort_values(by="R2_Average", ascending=False)

# Save the sorted table to a CSV file
r2_score_table.to_csv("sorted_r2_score_table.csv", index=False)

# Display the sorted table
print(r2_score_table)






import pandas as pd

models = [
    "Linear_Regression",
    "Random_Forest_Regression",
    "XGBoost_Regression",
    "Decision_Tree_Regression",
    "LGBM_Regression",
    "Ensemble_Bagging_Regression",
    "Neural_Network_Regression",
    "Ridge_Regression",
    "KNN_Regression"
]

v_values = ["pm1", "pm2_5", "pm4", "pm10", "pmTotal", "dCn"]

# Initialize a DataFrame to store the R^2 scores
data = {
    "Model": models
}

for v in v_values:
    data[v] = [
        r2_score_test_lr[v],
        r2_score_test_rf[v],
        r2_score_test_xgb[v],
        r2_score_test_dt[v],
        r2_score_test_lgbm[v],
        r2_score_test_br[v],
        r2_score_test_nn[v],
        r2_score_test_rr[v],
        r2_score_test_knn[v]
    ]

# Create the DataFrame
r2_score_table = pd.DataFrame(data)

# Sort the table by decreasing order of R^2 scores for each column in v_values
for v in v_values:
    r2_score_table = r2_score_table.sort_values(by=v, ascending=False)

# Save the sorted table to a CSV file
r2_score_table.to_csv("sorted_r2_score_table.csv", index=False)

# Display the sorted table
print(r2_score_table)



import pandas as pd

models = [
    "Linear_Regression",
    "Random_Forest_Regression",
    "XGBoost_Regression",
    "Decision_Tree_Regression",
    "LGBM_Regression",
    "Ensemble_Bagging_Regression",
    "Neural_Network_Regression",
    "Ridge_Regression",
    "KNN_Regression"
]

v_values = ["pm1", "pm2_5", "pm4", "pm10", "pmTotal", "dCn"]

# Initialize a DataFrame to store the R^2 scores
data = {
    "Model": models
}

for v in v_values:
    data[v] = [
        r2_score_test_lr[v],
        r2_score_test_rf[v],
        r2_score_test_xgb[v],
        r2_score_test_dt[v],
        r2_score_test_lgbm[v],
        r2_score_test_br[v],
        r2_score_test_nn[v],
        r2_score_test_rr[v],
        r2_score_test_knn[v]
    ]

# Create the DataFrame
r2_score_table = pd.DataFrame(data)

# Calculate the overall R^2 average for each model
r2_score_table["R2_Average"] = r2_score_table[v_values].mean(axis=1)

# Sort the table by the overall R^2 average in descending order
r2_score_table = r2_score_table.sort_values(by="R2_Average", ascending=False)

# Save the sorted table to a CSV file
r2_score_table.to_csv("sorted_r2_score_table.csv", index=False)

# Display the sorted table
print(r2_score_table)






# r2_score_test_sl = {}

# def Stacking_Regression(X_train,X_test,y_train,y_test,filtered_data):
#     ml_type = 'SL'
#     estimators = [
#         ('rf', RandomForestRegressor(n_estimators = 50, random_state = 0)),
#         ('br', BaggingRegressor(estimator = RandomForestRegressor(), n_estimators=10, random_state=42)),
#         ('lgbm',LGBMRegressor(
#         n_estimators=100,
#         max_depth=-1,
#         learning_rate=0.1,
#         num_leaves=31,
#         min_child_samples=20,
#         random_state=1
#         )),
#         ('dt', DecisionTreeRegressor(random_state = 0))
#     ]
    
#     model = StackingRegressor(
#         estimators = estimators,
#         final_estimator = LinearRegression())

#     model.fit(X_train, y_train)
    
#     predict_train = model.predict(X_train)
#     predict_test = model.predict(X_test)
    
#     train_df = pd.DataFrame({'Actual': y_train, 'Predicted': predict_train, 'Category': 'Training'}, index=y_train.index)
#     test_df = pd.DataFrame({'Actual': y_test, 'Predicted': predict_test, 'Category': 'Testing'}, index=y_test.index)
    
#     # Concatenate the DataFrames
#     combined_df = pd.concat([train_df, test_df])
#     combined_df = combined_df.sort_index()
#     # Print or use the combined DataFrame as needed
#     # print(combined_df)
    
#     r2_score_train = round(metrics.r2_score(y_train, predict_train),2) 
#     r2_score_test = round(metrics.r2_score(y_test, predict_test),2)
#     print('r2 train',r2_score_train)    
#     print('r2 test',r2_score_test) 
    
#     if (v == 'dCn'):
#         unit = 'dCn'
#     else:
#         unit = 'pm_conc'
    
#     # Scatter_Plot(combined_df, train_df, test_df, r2_score_train, r2_score_test, v, unit,ml_type)
#     # qq_plot(test_df, v, unit,ml_type)
#     return r2_score_test

# for k,v in enumerate(Palas):
#     X = Palas[v].drop([v+"Palas"],axis = 1)
#     y = Palas[v][v+"Palas"]
#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,random_state = 42)
#     X_train.to_csv("X_train.csv")
#     X_test.to_csv("X_test.csv")
#     y_train.to_csv("y_train.csv")
#     y_test.to_csv("y_test.csv")
#     r2_score_test_sl[v] = Stacking_Regression(X_train,X_test,y_train,y_test,filtered_data)
# r2_score_test_sl


# models = [
#     "Stacking_Learner",
#     "Random_Forest_Regression",
#     "Decision_Tree_Regression",
#     "Ensemble_Bagging_Regression",
#     "LGBM_Regression",
#     "XGBoost_Regression",
#     "Neural_Network_Regression",
#     "KNN_Regression",
#     "Linear_Regression",
#     "Ridge_Regression"
# ]

# v_values = ["pm1", "pm2_5", "pm4", "pm10", "pmTotal", "dCn"]

# # Initialize a DataFrame to store the R^2 scores
# data = {
#     "Model": models,
#     "pm1": [0.99, 0.99, 0.98, 0.99, 0.99, 0.97, 0.96, 0.96, 0.48, 0.48],
#     "pm2_5": [0.99, 0.97, 0.98, 0.96, 0.96, 0.97, 0.90, 0.85, 0.46, 0.45],
#     "pm4": [0.99, 0.99, 0.98, 0.99, 0.99, 0.98, 0.93, 0.93, 0.60, 0.61],
#     "pm10": [0.91, 0.91, 0.86, 0.91, 0.90, 0.89, 0.66, 0.57, 0.20, 0.19],
#     "pmTotal": [0.86, 0.76, 0.75, 0.72, 0.72, 0.72, 0.44, 0.19, 0.16, 0.16],
#     "dCn": [0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.97, 0.94, 0.43, 0.43]
# }

# # Create the DataFrame
# r2_score_table = pd.DataFrame(data)

# # Save the table to a CSV file
# r2_score_table.to_csv("sorted_r2_score_table.csv", index=False)

# # Display the table
# print(r2_score_table)



# # Suppress all warnings
# import warnings
# warnings.filterwarnings("ignore")
# import itertools
# from sklearn.ensemble import StackingRegressor
# from sklearn.linear_model import LinearRegression, Ridge
# from sklearn.tree import DecisionTreeRegressor
# from sklearn.ensemble import RandomForestRegressor, BaggingRegressor
# from sklearn.neighbors import KNeighborsRegressor
# from xgboost import XGBRegressor
# from lightgbm import LGBMRegressor
# from sklearn.neural_network import MLPRegressor
# from sklearn.metrics import r2_score
# import pandas as pd

# # Define all learners
# learners = {
#     'lr': LinearRegression(),
#     'rr': Ridge(random_state=42),
#     'knn': KNeighborsRegressor(),
#     'dt': DecisionTreeRegressor(random_state=42),
#     'rf': RandomForestRegressor(random_state=42),
#     'br': BaggingRegressor(random_state=42),
#     'xgb': XGBRegressor(random_state=42),
#     'lgbm': LGBMRegressor(random_state=42),
#     'nn': MLPRegressor(random_state=42,max_iter = 1000)  # Default Neural Network with random_state
# }

# # Function to evaluate a stacking model
# def evaluate_stacking(base_learners, meta_learner, X_train, X_test, y_train, y_test):
#     estimators = [(name, model) for name, model in base_learners.items()]
#     model = StackingRegressor(estimators=estimators, final_estimator=meta_learner)
#     model.fit(X_train, y_train)
#     y_pred = model.predict(X_test)
#     return r2_score(y_test, y_pred)

# # Prepare data for modeling
# results = []

# for v in Palas.keys():
#     print(v)
#     if v == "pmTotal":
#         X = Palas[v].drop([v + "Palas"], axis=1)
#         y = Palas[v][v + "Palas"]
#         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)
        
#          # Generate all combinations of 4 base learners
#         base_combinations = list(itertools.combinations(learners.keys(), 4))
#         counter = 0
#         for base_comb in base_combinations:
#             counter+=1
#             print("comb:",counter)
#             for meta_learner_name in learners.keys():
#                 base_learners = {name: learners[name] for name in base_comb}
#                 meta_learner = learners[meta_learner_name]
                
#                  # Evaluate the stacking model
#                 r2 = evaluate_stacking(base_learners, meta_learner, X_train, X_test, y_train, y_test)
                
#                  # Store the result
#                 results.append({
#                     'Base Learners': ', '.join(base_comb),
#                      'Meta Learner': meta_learner_name,
#                     'R2 Score': r2
#                  })
        
#         # Convert results to a DataFrame and sort by R2 score
#         results_df = pd.DataFrame(results)
#         results_df.sort_values(by='R2 Score', ascending=False, inplace=True)
        
#         # Display the top-performing combinations
#         # import ace_tools as tools; tools.display_dataframe_to_user(name="Stacking Model Performance", dataframe=results_df)
        
# results_df



# results_df_pmTotal = results_df
# results_df_pmTotal


# Suppress all warnings
import warnings
warnings.filterwarnings("ignore")
import itertools
from sklearn.ensemble import StackingRegressor
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, BaggingRegressor
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split
import pandas as pd

# Define all learners
learners = {
    'lr': LinearRegression(),
    'rr': Ridge(random_state=42),
    'knn': KNeighborsRegressor(),
    'dt': DecisionTreeRegressor(random_state=42),
    'rf': RandomForestRegressor(random_state=42),
    'br': BaggingRegressor(random_state=42),
    'xgb': XGBRegressor(random_state=42),
    'lgbm': LGBMRegressor(random_state=42),
    'nn': MLPRegressor(random_state=42, max_iter=1000)  # Default Neural Network with random_state
}

# Function to evaluate a stacking model
def evaluate_stacking(base_learners, meta_learner, X_train, X_test, y_train, y_test):
    estimators = [(name, model) for name, model in base_learners.items()]
    model = StackingRegressor(estimators=estimators, final_estimator=meta_learner)
    model.fit(X_train, y_train)
    
    # Predictions for training and test data
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)
    
    # Compute R2 scores
    train_r2 = r2_score(y_train, y_train_pred)
    test_r2 = r2_score(y_test, y_test_pred)
    return train_r2, test_r2

# Prepare data for modeling
results = []

for v in Palas.keys():
    print(v)
    if v == "pm1": # ['pm1Palas', 'pm2_5Palas', 'pm4Palas', 'pm10Palas', 'pmTotalPalas', 'dCnPalas']

        X = Palas[v].drop([v + "Palas"], axis=1)
        y = Palas[v][v + "Palas"]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)
        
        # Generate all combinations of 4 base learners
        base_combinations = list(itertools.combinations(learners.keys(), 4))
        counter = 0
        for base_comb in base_combinations:
            for meta_learner_name in learners.keys():
                # Ensure meta-learner can also be a base learner
                base_learners = {name: learners[name] for name in base_comb}
                meta_learner = learners[meta_learner_name]

                # Evaluate the stacking model
                counter += 1
                print("comb:", counter)
                train_r2, test_r2 = evaluate_stacking(base_learners, meta_learner, X_train, X_test, y_train, y_test)

                # Store the result
                results.append({
                    'Base Learners': ', '.join(base_comb),
                    'Meta Learner': meta_learner_name,
                    'Train R2 Score': train_r2,
                    'Test R2 Score': test_r2,
                    'Overfitting': 'Yes' if (train_r2 - test_r2) > 0.1 else 'No'
                })

# Convert results to a DataFrame and sort by Test R2 score
results_df = pd.DataFrame(results)
results_df.sort_values(by='Test R2 Score', ascending=False, inplace=True)
results_df_pm1_fit = results_df

# Filter for cases where Train R2 Score > Test R2 Score
results_df_pm1_fit_overfit = results_df_pm1_fit[results_df_pm1_fit['Train R2 Score'] > results_df_pm1_fit['Test R2 Score']]

results_df_pm1_fit, results_df_pm1_fit_overfit


# Suppress all warnings
import warnings
warnings.filterwarnings("ignore")
import itertools
from sklearn.ensemble import StackingRegressor
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, BaggingRegressor
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split
import pandas as pd

# Define all learners
learners = {
    'lr': LinearRegression(),
    'rr': Ridge(random_state=42),
    'knn': KNeighborsRegressor(),
    'dt': DecisionTreeRegressor(random_state=42),
    'rf': RandomForestRegressor(random_state=42),
    'br': BaggingRegressor(random_state=42),
    'xgb': XGBRegressor(random_state=42),
    'lgbm': LGBMRegressor(random_state=42),
    'nn': MLPRegressor(random_state=42, max_iter=1000)  # Default Neural Network with random_state
}

# Function to evaluate a stacking model
def evaluate_stacking(base_learners, meta_learner, X_train, X_test, y_train, y_test):
    estimators = [(name, model) for name, model in base_learners.items()]
    model = StackingRegressor(estimators=estimators, final_estimator=meta_learner)
    model.fit(X_train, y_train)
    
    # Predictions for training and test data
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)
    
    # Compute R2 scores
    train_r2 = r2_score(y_train, y_train_pred)
    test_r2 = r2_score(y_test, y_test_pred)
    return train_r2, test_r2

# Prepare data for modeling
results = []

for v in Palas.keys():
    print(v)
    if v == "pm2_5": # ['pm1Palas', 'pm2_5Palas', 'pm4Palas', 'pm10Palas', 'pmTotalPalas', 'dCnPalas']

        X = Palas[v].drop([v + "Palas"], axis=1)
        y = Palas[v][v + "Palas"]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)
        
        # Generate all combinations of 4 base learners
        base_combinations = list(itertools.combinations(learners.keys(), 4))
        counter = 0
        for base_comb in base_combinations:
            for meta_learner_name in learners.keys():
                # Ensure meta-learner can also be a base learner
                base_learners = {name: learners[name] for name in base_comb}
                meta_learner = learners[meta_learner_name]

                # Evaluate the stacking model
                counter += 1
                print("comb:", counter)
                train_r2, test_r2 = evaluate_stacking(base_learners, meta_learner, X_train, X_test, y_train, y_test)

                # Store the result
                results.append({
                    'Base Learners': ', '.join(base_comb),
                    'Meta Learner': meta_learner_name,
                    'Train R2 Score': train_r2,
                    'Test R2 Score': test_r2,
                    'Overfitting': 'Yes' if (train_r2 - test_r2) > 0.1 else 'No'
                })

# Convert results to a DataFrame and sort by Test R2 score
results_df = pd.DataFrame(results)
results_df.sort_values(by='Test R2 Score', ascending=False, inplace=True)
results_df_pm2_5_fit = results_df
results_df_pm2_5_fit


# Suppress all warnings
import warnings
warnings.filterwarnings("ignore")
import itertools
from sklearn.ensemble import StackingRegressor
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, BaggingRegressor
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split
import pandas as pd

# Define all learners
learners = {
    'lr': LinearRegression(),
    'rr': Ridge(random_state=42),
    'knn': KNeighborsRegressor(),
    'dt': DecisionTreeRegressor(random_state=42),
    'rf': RandomForestRegressor(random_state=42),
    'br': BaggingRegressor(random_state=42),
    'xgb': XGBRegressor(random_state=42),
    'lgbm': LGBMRegressor(random_state=42),
    'nn': MLPRegressor(random_state=42, max_iter=1000)  # Default Neural Network with random_state
}

# Function to evaluate a stacking model
def evaluate_stacking(base_learners, meta_learner, X_train, X_test, y_train, y_test):
    estimators = [(name, model) for name, model in base_learners.items()]
    model = StackingRegressor(estimators=estimators, final_estimator=meta_learner)
    model.fit(X_train, y_train)
    
    # Predictions for training and test data
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)
    
    # Compute R2 scores
    train_r2 = r2_score(y_train, y_train_pred)
    test_r2 = r2_score(y_test, y_test_pred)
    return train_r2, test_r2

# Prepare data for modeling
results = []

for v in Palas.keys():
    print(v)
    if v == "pm4": #['pm1Palas', 'pm2_5Palas', 'pm4Palas', 'pm10Palas', 'pmTotalPalas', 'dCnPalas']

        X = Palas[v].drop([v + "Palas"], axis=1)
        y = Palas[v][v + "Palas"]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)
        
        # Generate all combinations of 4 base learners
        base_combinations = list(itertools.combinations(learners.keys(), 4))
        counter = 0
        for base_comb in base_combinations:
            for meta_learner_name in learners.keys():
                # Ensure meta-learner can also be a base learner
                base_learners = {name: learners[name] for name in base_comb}
                meta_learner = learners[meta_learner_name]

                # Evaluate the stacking model
                counter += 1
                print("comb:", counter)
                train_r2, test_r2 = evaluate_stacking(base_learners, meta_learner, X_train, X_test, y_train, y_test)

                # Store the result
                results.append({
                    'Base Learners': ', '.join(base_comb),
                    'Meta Learner': meta_learner_name,
                    'Train R2 Score': train_r2,
                    'Test R2 Score': test_r2,
                    'Overfitting': 'Yes' if (train_r2 - test_r2) > 0.1 else 'No'
                })

# Convert results to a DataFrame and sort by Test R2 score
results_df = pd.DataFrame(results)
results_df.sort_values(by='Test R2 Score', ascending=False, inplace=True)
results_df_pm4_fit = results_df
results_df_pm4_fit


# Suppress all warnings
import warnings
warnings.filterwarnings("ignore")
import itertools
from sklearn.ensemble import StackingRegressor
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, BaggingRegressor
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split
import pandas as pd

# Define all learners
learners = {
    'lr': LinearRegression(),
    'rr': Ridge(random_state=42),
    'knn': KNeighborsRegressor(),
    'dt': DecisionTreeRegressor(random_state=42),
    'rf': RandomForestRegressor(random_state=42),
    'br': BaggingRegressor(random_state=42),
    'xgb': XGBRegressor(random_state=42),
    'lgbm': LGBMRegressor(random_state=42),
    'nn': MLPRegressor(random_state=42, max_iter=1000)  # Default Neural Network with random_state
}

# Function to evaluate a stacking model
def evaluate_stacking(base_learners, meta_learner, X_train, X_test, y_train, y_test):
    estimators = [(name, model) for name, model in base_learners.items()]
    model = StackingRegressor(estimators=estimators, final_estimator=meta_learner)
    model.fit(X_train, y_train)
    
    # Predictions for training and test data
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)
    
    # Compute R2 scores
    train_r2 = r2_score(y_train, y_train_pred)
    test_r2 = r2_score(y_test, y_test_pred)
    return train_r2, test_r2

# Prepare data for modeling
results = []

for v in Palas.keys():
    print(v)
    if v == "pm10": # ['pm1Palas', 'pm2_5Palas', 'pm4Palas', 'pm10Palas', 'pmTotalPalas', 'dCnPalas']

        X = Palas[v].drop([v + "Palas"], axis=1)
        y = Palas[v][v + "Palas"]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)
        
        # Generate all combinations of 4 base learners
        base_combinations = list(itertools.combinations(learners.keys(), 4))
        counter = 0
        for base_comb in base_combinations:
            for meta_learner_name in learners.keys():
                # Ensure meta-learner can also be a base learner
                base_learners = {name: learners[name] for name in base_comb}
                meta_learner = learners[meta_learner_name]

                # Evaluate the stacking model
                counter += 1
                print("comb:", counter)
                train_r2, test_r2 = evaluate_stacking(base_learners, meta_learner, X_train, X_test, y_train, y_test)

                # Store the result
                results.append({
                    'Base Learners': ', '.join(base_comb),
                    'Meta Learner': meta_learner_name,
                    'Train R2 Score': train_r2,
                    'Test R2 Score': test_r2,
                    'Overfitting': 'Yes' if (train_r2 - test_r2) > 0.1 else 'No'
                })

# Convert results to a DataFrame and sort by Test R2 score
results_df = pd.DataFrame(results)
results_df.sort_values(by='Test R2 Score', ascending=False, inplace=True)
results_df_pm10_fit = results_df
results_df_pm10_fit


# Suppress all warnings
import warnings
warnings.filterwarnings("ignore")
import itertools
from sklearn.ensemble import StackingRegressor
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, BaggingRegressor
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split
import pandas as pd

# Define all learners
learners = {
    'lr': LinearRegression(),
    'rr': Ridge(random_state=42),
    'knn': KNeighborsRegressor(),
    'dt': DecisionTreeRegressor(random_state=42),
    'rf': RandomForestRegressor(random_state=42),
    'br': BaggingRegressor(random_state=42),
    'xgb': XGBRegressor(random_state=42),
    'lgbm': LGBMRegressor(random_state=42),
    'nn': MLPRegressor(random_state=42, max_iter=1000)  # Default Neural Network with random_state
}

# Function to evaluate a stacking model
def evaluate_stacking(base_learners, meta_learner, X_train, X_test, y_train, y_test):
    estimators = [(name, model) for name, model in base_learners.items()]
    model = StackingRegressor(estimators=estimators, final_estimator=meta_learner)
    model.fit(X_train, y_train)
    
    # Predictions for training and test data
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)
    
    # Compute R2 scores
    train_r2 = r2_score(y_train, y_train_pred)
    test_r2 = r2_score(y_test, y_test_pred)
    return train_r2, test_r2

# Prepare data for modeling
results = []

for v in Palas.keys():
    print(v)
    if v == "pmTotal": # ['pm1Palas', 'pm2_5Palas', 'pm4Palas', 'pm10Palas', 'pmTotalPalas', 'dCnPalas']

        X = Palas[v].drop([v + "Palas"], axis=1)
        y = Palas[v][v + "Palas"]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)
        
        # Generate all combinations of 4 base learners
        base_combinations = list(itertools.combinations(learners.keys(), 4))
        counter = 0
        for base_comb in base_combinations:
            for meta_learner_name in learners.keys():
                # Ensure meta-learner can also be a base learner
                base_learners = {name: learners[name] for name in base_comb}
                meta_learner = learners[meta_learner_name]

                # Evaluate the stacking model
                counter += 1
                print("comb:", counter)
                train_r2, test_r2 = evaluate_stacking(base_learners, meta_learner, X_train, X_test, y_train, y_test)

                # Store the result
                results.append({
                    'Base Learners': ', '.join(base_comb),
                    'Meta Learner': meta_learner_name,
                    'Train R2 Score': train_r2,
                    'Test R2 Score': test_r2,
                    'Overfitting': 'Yes' if (train_r2 - test_r2) > 0.1 else 'No'
                })

# Convert results to a DataFrame and sort by Test R2 score
results_df = pd.DataFrame(results)
results_df.sort_values(by='Test R2 Score', ascending=False, inplace=True)
results_df_pmTotal_fit = results_df
results_df_pmTotal_fit


# Suppress all warnings
import warnings
warnings.filterwarnings("ignore")
import itertools
from sklearn.ensemble import StackingRegressor
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, BaggingRegressor
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split
import pandas as pd

# Define all learners
learners = {
    'lr': LinearRegression(),
    'rr': Ridge(random_state=42),
    'knn': KNeighborsRegressor(),
    'dt': DecisionTreeRegressor(random_state=42),
    'rf': RandomForestRegressor(random_state=42),
    'br': BaggingRegressor(random_state=42),
    'xgb': XGBRegressor(random_state=42),
    'lgbm': LGBMRegressor(random_state=42),
    'nn': MLPRegressor(random_state=42, max_iter=1000)  # Default Neural Network with random_state
}

# Function to evaluate a stacking model
def evaluate_stacking(base_learners, meta_learner, X_train, X_test, y_train, y_test):
    estimators = [(name, model) for name, model in base_learners.items()]
    model = StackingRegressor(estimators=estimators, final_estimator=meta_learner)
    model.fit(X_train, y_train)
    
    # Predictions for training and test data
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)
    
    # Compute R2 scores
    train_r2 = r2_score(y_train, y_train_pred)
    test_r2 = r2_score(y_test, y_test_pred)
    return train_r2, test_r2

# Prepare data for modeling
results = []

for v in Palas.keys():
    print(v)
    if v == "dCn": # ['pm1Palas', 'pm2_5Palas', 'pm4Palas', 'pm10Palas', 'pmTotalPalas', 'dCnPalas']

        X = Palas[v].drop([v + "Palas"], axis=1)
        y = Palas[v][v + "Palas"]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)
        
        # Generate all combinations of 4 base learners
        base_combinations = list(itertools.combinations(learners.keys(), 4))
        counter = 0
        for base_comb in base_combinations:
            for meta_learner_name in learners.keys():
                # Ensure meta-learner can also be a base learner
                base_learners = {name: learners[name] for name in base_comb}
                meta_learner = learners[meta_learner_name]

                # Evaluate the stacking model
                counter += 1
                print("comb:", counter)
                train_r2, test_r2 = evaluate_stacking(base_learners, meta_learner, X_train, X_test, y_train, y_test)

                # Store the result
                results.append({
                    'Base Learners': ', '.join(base_comb),
                    'Meta Learner': meta_learner_name,
                    'Train R2 Score': train_r2,
                    'Test R2 Score': test_r2,
                    'Overfitting': 'Yes' if (train_r2 - test_r2) > 0.1 else 'No'
                })

# Convert results to a DataFrame and sort by Test R2 score
results_df = pd.DataFrame(results)
results_df.sort_values(by='Test R2 Score', ascending=False, inplace=True)
results_df_dCn_fit = results_df
results_df_dCn_fit


from sklearn.inspection import permutation_importance
import matplotlib.pyplot as plt

def Permutation_Importance_Stack_Regressor(model, X_test, y_test, v, unit, ml_type, dict_col_regression):
    # Calculate permutation importance
    result = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42, scoring='r2')

    # Extract importance values and their corresponding feature names
    importance_values = result.importances_mean
    importance_std = result.importances_std
    feature_names = X_test.columns

    # Sort by importance values
    sorted_idx = importance_values.argsort()
    sorted_importance_values = importance_values[sorted_idx]
    # sorted_importance_std = importance_std[sorted_idx]
    sorted_feature_names = [dict_col_regression[n] for n in feature_names[sorted_idx]]

    # Create the plot
    plt.figure(figsize=(10, 6))
    plt.barh(sorted_feature_names, sorted_importance_values, color='#1f77b4', capsize=4) #xerr=sorted_importance_std
    plt.xlabel('Permutation Importance Ranking', fontsize=14,fontweight='bold')
    plt.ylabel('Features', fontsize=14,fontweight='bold')

    # Adjust layout for automatic spacing
    plt.tight_layout()

    # Save and show the plot
    plt.savefig(f'D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/plots/Permutation Importance/permutation_importance_plot_{ml_type}_{v}.png', format='png')
    plt.show()
# r2_score_test_sl[v] = Stacking_Regression(X_train, X_test, y_train, y_test, filtered_data)




def plot_error_distribution(test_df, v, ml_type, threshold=5):
    # Calculate prediction error
    test_df['Error'] = test_df['Actual'] - test_df['Predicted']

    # Identify outliers beyond the threshold
    outliers = test_df[(test_df['Error'] > threshold) | (test_df['Error'] < -threshold)]

    # Create the error distribution plot
    plt.figure(figsize=(8, 6))
    sns.histplot(test_df['Error'], bins=30, kde=True, color='red', edgecolor='black')

    # Customize plot labels and title
    plt.title(f"Error Distribution in {dict_col_regression[v + 'Palas']} Predictions", fontsize=16, fontweight='bold')
    plt.xlabel('Prediction Error (Actual - Predicted) for Test data', fontsize=14, fontweight='bold')
    plt.ylabel('Frequency', fontsize=14, fontweight='bold')

    # # Highlight outliers with vertical lines
    # for _, row in outliers.iterrows():
    #     plt.axvline(x=row['Error'], color='red', linestyle='--', alpha=0.7)

    # plt.legend(['Outliers (|Error| > threshold)'], fontsize=10, loc='upper right')
    plt.tight_layout()
    plt.savefig(f'D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/plots/Error Distribution/error_distribution_plot_{ml_type}_{v}.png', format='png')

    plt.show()



%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/plots/Scatter_Plot.ipynb
%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/plots/QQ_Plot.ipynb


from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.ensemble import RandomForestRegressor, StackingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn import metrics
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

r2_score_test_sl = {}

def Stacking_Regression(X_train, X_test, y_train, y_test, filtered_data):
    ml_type = 'SL'
    estimators = [
        ('lr', LinearRegression()),
        ('knn', KNeighborsRegressor()),
        ('xgb', XGBRegressor(random_state=42)),
        ('nn', MLPRegressor(random_state=42, max_iter=1000))  # Neural network
    ]

    model = StackingRegressor(
        estimators=estimators,
        final_estimator=RandomForestRegressor(random_state=42)  # Meta learner
    )

    model.fit(X_train, y_train)

    predict_train = model.predict(X_train)
    predict_test = model.predict(X_test)

    train_df = pd.DataFrame({'Actual': y_train, 'Predicted': predict_train, 'Category': 'Training'}, index=y_train.index)
    test_df = pd.DataFrame({'Actual': y_test, 'Predicted': predict_test, 'Category': 'Testing'}, index=y_test.index)

    # Concatenate the DataFrames
    combined_df = pd.concat([train_df, test_df])
    combined_df = combined_df.sort_index()

    r2_score_train = round(metrics.r2_score(y_train, predict_train), 2)
    r2_score_test = round(metrics.r2_score(y_test, predict_test), 2)
    print('R² (Train):', r2_score_train)
    print('R² (Test):', r2_score_test)

    Scatter_Plot(combined_df, train_df, test_df, r2_score_train, r2_score_test, v, 'ug/m³', ml_type)
    qq_plot(test_df, v, 'ug/m³', ml_type)
    Permutation_Importance_Stack_Regressor(model, X_test, y_test, v, 'ug/m³', ml_type, dict_col_regression)
    plot_error_distribution(test_df, v,ml_type, 5)
    return r2_score_test, model

# def plot_error_distribution(test_df, v, threshold=5):
#     # Calculate prediction error
#     test_df['Error'] = test_df['Actual'] - test_df['Predicted']

#     # Identify outliers beyond the threshold
#     outliers = test_df[(test_df['Error'] > threshold) | (test_df['Error'] < -threshold)]

#     # Create the error distribution plot
#     plt.figure(figsize=(8, 6))
#     sns.histplot(test_df['Error'], bins=30, kde=True, color='skyblue', edgecolor='black')

#     # Customize plot labels and title
#     plt.title(f'Error Distribution in {v.upper()} Predictions', fontsize=14, fontweight='bold')
#     plt.xlabel('Prediction Error (Actual - Predicted)', fontsize=12, fontweight='bold')
#     plt.ylabel('Frequency', fontsize=12, fontweight='bold')

#     # Highlight outliers with vertical lines
#     for _, row in outliers.iterrows():
#         plt.axvline(x=row['Error'], color='red', linestyle='--', alpha=0.7)

#     plt.legend(['Outliers (|Error| > threshold)'], fontsize=10, loc='upper right')
#     plt.tight_layout()
#     plt.show()

for k, v in enumerate(Palas):
    if v == "pm1":
        X = Palas[v].drop([v + "Palas"], axis=1)
        y = Palas[v][v + "Palas"]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

        # Save training and test sets as CSV files for inspection
        X_train.to_csv("X_train.csv")
        X_test.to_csv("X_test.csv")
        y_train.to_csv("y_train.csv")
        y_test.to_csv("y_test.csv")

        # Perform Stacking Regression and plot error distribution
        r2_score_test_sl[v], model = Stacking_Regression(X_train, X_test, y_train, y_test, filtered_data)

r2_score_test_sl



r2_score_test_sl = {}

def Stacking_Regression(X_train, X_test, y_train, y_test, filtered_data):
    ml_type = 'SL'
    estimators = [
        ('knn', KNeighborsRegressor()),
        ('dt', DecisionTreeRegressor(random_state=42)),
        ('br', BaggingRegressor(random_state=42)),
        ('nn', MLPRegressor(random_state=42, max_iter=1000))  # Neural network
    ]
    
    model = StackingRegressor(
        estimators=estimators,
        final_estimator=RandomForestRegressor(random_state=42)  # Meta learner
    )

    model.fit(X_train, y_train)
    
    predict_train = model.predict(X_train)
    predict_test = model.predict(X_test)
    
    train_df = pd.DataFrame({'Actual': y_train, 'Predicted': predict_train, 'Category': 'Training'}, index=y_train.index)
    test_df = pd.DataFrame({'Actual': y_test, 'Predicted': predict_test, 'Category': 'Testing'}, index=y_test.index)
    
    # Concatenate the DataFrames
    combined_df = pd.concat([train_df, test_df])
    combined_df = combined_df.sort_index()
    
    r2_score_train = round(metrics.r2_score(y_train, predict_train), 2)
    r2_score_test = round(metrics.r2_score(y_test, predict_test), 2)
    print('r2 train', r2_score_train)    
    print('r2 test', r2_score_test) 
    
    if (v == 'dCn'):
        unit = 'dCn'
    else:
        unit = 'pm_conc'
    
    Scatter_Plot(combined_df, train_df, test_df, r2_score_train, r2_score_test, v, unit, ml_type)
    qq_plot(test_df, v, unit, ml_type)
    Permutation_Importance_Stack_Regressor(model, X_test, y_test, v, unit, ml_type, dict_col_regression)
    plot_error_distribution(test_df, v,ml_type, 5)

    return r2_score_test, model



for k, v in enumerate(Palas):
    if v == "pm2_5":
        X = Palas[v].drop([v + "Palas"], axis=1)
        y = Palas[v][v + "Palas"]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)
        X_train.to_csv("X_train.csv")
        X_test.to_csv("X_test.csv")
        y_train.to_csv("y_train.csv")
        y_test.to_csv("y_test.csv")
        r2_score_test_sl[v], model = Stacking_Regression(X_train, X_test, y_train, y_test, filtered_data)

r2_score_test_sl



r2_score_test_sl = {}

def Stacking_Regression(X_train, X_test, y_train, y_test, filtered_data):
    ml_type = 'SL'
    estimators = [
        ('lr', LinearRegression()),
        ('knn', KNeighborsRegressor()),
        ('dt', DecisionTreeRegressor(random_state=42)),
        ('xgb', XGBRegressor(random_state=42))
    ]
    
    model = StackingRegressor(
        estimators=estimators,
        final_estimator=RandomForestRegressor(random_state=42)
    )

    model.fit(X_train, y_train)
    
    predict_train = model.predict(X_train)
    predict_test = model.predict(X_test)
    
    train_df = pd.DataFrame({'Actual': y_train, 'Predicted': predict_train, 'Category': 'Training'}, index=y_train.index)
    test_df = pd.DataFrame({'Actual': y_test, 'Predicted': predict_test, 'Category': 'Testing'}, index=y_test.index)
    
    # Concatenate the DataFrames
    combined_df = pd.concat([train_df, test_df])
    combined_df = combined_df.sort_index()
    
    r2_score_train = round(metrics.r2_score(y_train, predict_train), 2)
    r2_score_test = round(metrics.r2_score(y_test, predict_test), 2)
    print('r2 train', r2_score_train)    
    print('r2 test', r2_score_test) 
    
    if (v == 'dCn'):
        unit = 'dCn'
    else:
        unit = 'pm_conc'
    
    Scatter_Plot(combined_df, train_df, test_df, r2_score_train, r2_score_test, v, unit, ml_type)
    qq_plot(test_df, v, unit, ml_type)
    Permutation_Importance_Stack_Regressor(model, X_test, y_test, v, unit, ml_type, dict_col_regression)
    plot_error_distribution(test_df, v,ml_type, 5)
    return r2_score_test, model

for k, v in enumerate(Palas):
    if v == "pm4":
        X = Palas[v].drop([v + "Palas"], axis=1)
        y = Palas[v][v + "Palas"]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,random_state=42)
        X_train.to_csv("X_train.csv")
        X_test.to_csv("X_test.csv")
        y_train.to_csv("y_train.csv")
        y_test.to_csv("y_test.csv")
        r2_score_test_sl[v], model = Stacking_Regression(X_train, X_test, y_train, y_test, filtered_data)

r2_score_test_sl



r2_score_test_sl = {}

def Stacking_Regression(X_train, X_test, y_train, y_test, filtered_data):
    ml_type = 'SL'

    # Base learners
    estimators = [
        ('knn', KNeighborsRegressor()),
        ('rf', RandomForestRegressor(random_state=42)),
        ('xgb', XGBRegressor(random_state=42)),
        ('lgbm', LGBMRegressor(random_state=42))
    ]

    # Meta learner
    model = StackingRegressor(
        estimators=estimators,
        final_estimator=MLPRegressor(random_state=42, max_iter=1000)
    )

    model.fit(X_train, y_train)

    predict_train = model.predict(X_train)
    predict_test = model.predict(X_test)

    train_df = pd.DataFrame({'Actual': y_train, 'Predicted': predict_train, 'Category': 'Training'}, index=y_train.index)
    test_df = pd.DataFrame({'Actual': y_test, 'Predicted': predict_test, 'Category': 'Testing'}, index=y_test.index)

    # Concatenate the DataFrames
    combined_df = pd.concat([train_df, test_df])
    combined_df = combined_df.sort_index()

    r2_score_train = round(metrics.r2_score(y_train, predict_train), 2)
    r2_score_test = round(metrics.r2_score(y_test, predict_test), 2)
    print('r2 train', r2_score_train)
    print('r2 test', r2_score_test)
    
    if v == 'dCn':
        unit = 'dCn'
    else:
        unit = 'pm_conc'

    Scatter_Plot(combined_df, train_df, test_df, r2_score_train, r2_score_test, v, unit, ml_type)
    qq_plot(test_df, v, unit, ml_type)
    Permutation_Importance_Stack_Regressor(model, X_test, y_test, v, unit, ml_type, dict_col_regression)
    plot_error_distribution(test_df, v,ml_type, 5)
    return r2_score_test, model

for k, v in enumerate(Palas):
    if v == "pm10":
        X = Palas[v].drop([v + "Palas"], axis=1)
        y = Palas[v][v + "Palas"]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,random_state=42)
        X_train.to_csv("X_train.csv")
        X_test.to_csv("X_test.csv")
        y_train.to_csv("y_train.csv")
        y_test.to_csv("y_test.csv")
        r2_score_test_sl[v], model = Stacking_Regression(X_train, X_test, y_train, y_test, filtered_data)

r2_score_test_sl



r2_score_test_sl = {}

def Stacking_Regression(X_train, X_test, y_train, y_test, filtered_data):
    ml_type = 'SL'
    estimators = [
        ('knn', KNeighborsRegressor()),
        ('rf', RandomForestRegressor(random_state=42)),
        ('xgb', XGBRegressor(random_state=42)),
        ('lgbm', LGBMRegressor(random_state=42))
    ]
    
    model = StackingRegressor(
        estimators=estimators,
        final_estimator=MLPRegressor(random_state=42, max_iter=1000))

    model.fit(X_train, y_train)
    
    predict_train = model.predict(X_train)
    predict_test = model.predict(X_test)
    
    train_df = pd.DataFrame({'Actual': y_train, 'Predicted': predict_train, 'Category': 'Training'}, index=y_train.index)
    test_df = pd.DataFrame({'Actual': y_test, 'Predicted': predict_test, 'Category': 'Testing'}, index=y_test.index)
    
    # Concatenate the DataFrames
    combined_df = pd.concat([train_df, test_df])
    combined_df = combined_df.sort_index()

    r2_score_train = round(metrics.r2_score(y_train, predict_train), 2)
    r2_score_test = round(metrics.r2_score(y_test, predict_test), 2)
    print('r2 train', r2_score_train)    
    print('r2 test', r2_score_test) 
    
    if (v == 'dCn'):
        unit = 'dCn'
    else:
        unit = 'pm_conc'
    
    Scatter_Plot(combined_df, train_df, test_df, r2_score_train, r2_score_test, v, unit, ml_type)
    qq_plot(test_df, v, unit, ml_type)
    Permutation_Importance_Stack_Regressor(model, X_test, y_test, v, unit, ml_type, dict_col_regression)
    plot_error_distribution(test_df, v,ml_type, 5)
    return r2_score_test, model

for k, v in enumerate(Palas):
    if v == "pmTotal":
        X = Palas[v].drop([v + "Palas"], axis=1)
        y = Palas[v][v + "Palas"]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)
        X_train.to_csv("X_train.csv")
        X_test.to_csv("X_test.csv")
        y_train.to_csv("y_train.csv")
        y_test.to_csv("y_test.csv")
        r2_score_test_sl[v], model = Stacking_Regression(X_train, X_test, y_train, y_test, filtered_data)

r2_score_test_sl



r2_score_test_sl = {}

def Stacking_Regression(X_train,X_test,y_train,y_test,filtered_data):
    ml_type = 'SL'
    estimators = [
        ('lr', LinearRegression()),
        ('rr', Ridge(random_state=42)),
        ('dt', DecisionTreeRegressor(random_state=42)),
        ('lgbm', LGBMRegressor(random_state=42))
    ]
    
    model = StackingRegressor(
        estimators = estimators,
        final_estimator =  RandomForestRegressor(random_state=42))

    model.fit(X_train, y_train)
    
    predict_train = model.predict(X_train)
    predict_test = model.predict(X_test)
    
    train_df = pd.DataFrame({'Actual': y_train, 'Predicted': predict_train, 'Category': 'Training'}, index=y_train.index)
    test_df = pd.DataFrame({'Actual': y_test, 'Predicted': predict_test, 'Category': 'Testing'}, index=y_test.index)
    
    # Concatenate the DataFrames
    combined_df = pd.concat([train_df, test_df])
    combined_df = combined_df.sort_index()
    # Print or use the combined DataFrame as needed
    # print(combined_df)
    
    r2_score_train = round(metrics.r2_score(y_train, predict_train),2) 
    r2_score_test = round(metrics.r2_score(y_test, predict_test),2)
    print('r2 train',r2_score_train)    
    print('r2 test',r2_score_test) 
    
    if (v == 'dCn'):
        unit = 'dCn'
    else:
        unit = 'pm_conc'
    
    Scatter_Plot(combined_df, train_df, test_df, r2_score_train, r2_score_test, v, unit,ml_type)
    qq_plot(test_df, v, unit,ml_type)
    Permutation_Importance_Stack_Regressor(model, X_test, y_test, v, unit, ml_type, dict_col_regression)
    plot_error_distribution(test_df, v,ml_type, 5)
    return r2_score_test,model

for k,v in enumerate(Palas):
    if v=="dCn":
        X = Palas[v].drop([v+"Palas"],axis = 1)
        y = Palas[v][v+"Palas"]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,random_state=42)
        X_train.to_csv("X_train.csv")
        X_test.to_csv("X_test.csv")
        y_train.to_csv("y_train.csv")
        y_test.to_csv("y_test.csv")
        r2_score_test_sl[v],model = Stacking_Regression(X_train,X_test,y_train,y_test,filtered_data)
r2_score_test_sl


# R^2 values for train and test from the table
r2_train = [1.00, 1.00, 1.00, 0.98, 0.97, 1.00]
r2_test = [0.99, 0.99, 0.99, 0.91, 0.86, 0.99]

# Calculate overall R^2 for train and test
overall_r2_train = sum(r2_train) / len(r2_train)
overall_r2_test = sum(r2_test) / len(r2_test)

# Print the results
print(f"Overall R^2 (Train): {overall_r2_train:.2f}")
print(f"Overall R^2 (Test): {overall_r2_test:.2f}")

