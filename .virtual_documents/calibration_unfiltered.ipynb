# %pip install numpy 
# %pip install pandas
# %pip install matplotlib
# %pip install scikit-learn
# %pip install seaborn
# %pip install plotly-express
# %pip install lightgbm
# %pip install statsmodels
# %pip install scikit-optimize
# %pip install nbformat
# %pip install bayesian-optimization==1.4.1
# %pip install xgboost


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy
import pickle
# from statsmodels.graphics.gofplots import qqplot

from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPRegressor
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Lasso
from sklearn.svm import SVR
import sklearn.gaussian_process as gp
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn import metrics
import matplotlib.dates as mdates
import lightgbm as lgb
from sklearn.linear_model import Ridge
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.ensemble import BaggingRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import StackingRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import r2_score
from skopt import gp_minimize
from sklearn.model_selection import cross_val_score
from sklearn.metrics import r2_score
from skopt import gp_minimize
from skopt.space import Real, Categorical, Integer
from lightgbm import LGBMRegressor
import nbformat
from bayes_opt import BayesianOptimization
import xgboost as xgb
from statsmodels.graphics.gofplots import qqplot_2samples


# %run models/Linear_Regression.ipynb


###################### Null values are already removed from this data set ##############################


data = pd.read_csv(r'D:\UTD\UTDFall2023\Calibration-of-LoRaNodes-using-Super-Learners\data\calibrate.csv')
data


data.columns


len(data.columns)


df= pd.concat([data.iloc[:,6], data.iloc[:,0:6],data.iloc[:,15:data.shape[1]]],axis = 1)
df.info()


def data_checker(df):
    # data_frame = df.iloc[:,1:16]
    data_frame =  df.iloc[:,7:-3] 
    # Palas limits are 0-100 mg/m3
    #BME limits are :Temp -40C to 85C
                   #:Pressure 300hPa to 1100 hPa or  300*100Pa to 1100*100 Pa
                   #:Humidity 0% to 100%
     #PPD42NS :Operating Temp is 0C to 45C
    idx = data_frame[(data_frame['Temperature_loRa']>=0) & (data_frame['Temperature_loRa']<=45) &
                    (data_frame['Pressure_loRa']>=300*100) & (data_frame['Pressure_loRa']<=1100*100) &
                    (data_frame['Humidity_loRa']>=0) & (data_frame['Humidity_loRa']<=100) &
                    (data_frame['pm1Palas']>=0) & (data_frame['pm2_5Palas']>=0) &
                    (data_frame['pm4Palas']>=0) & (data_frame['pm10Palas']>=0) &
                    (data_frame['pmTotalPalas']>=0) & (data_frame['dCnPalas']>=0)&
                    (data_frame['pm1Palas']<=100000) & (data_frame['pm2_5Palas']<=100000) &
                    (data_frame['pm4Palas']<=100000) & (data_frame['pm10Palas']<=100000) &
                    (data_frame['pmTotalPalas']<=100000) & (data_frame['dCnPalas']<=100000)].index
                    # &
                    # (data_frame['P1_conc_loRa'] + data_frame['P2_conc_loRa']>=0) &
                    # (data_frame['P1_conc_loRa'] + data_frame['P2_conc_loRa']<=28000)].index
    return df.loc[idx]
data_checker(df)


df = data_checker(df)
df


df.describe()


########### Plot the distribution of all the input parameters ############


dict_col_names = {'pm1Palas':'PM$_{1.0}$ in $\mu g/m^3 $ (Palas)', 
                  'pm2_5Palas': 'PM$_{2.5}$ in $\mu g/m^3$ (Palas)',
                  'pm4Palas': 'PM$_{4.0}$ in $\mu g/m^3$ (Palas)',
                  'pm10Palas': 'PM$_{10.0}$ in $\mu g/m^3$ (Palas)',
                  'pmTotalPalas': 'Total PM Concentration in $\mu g/m^3$ (Palas)', 
                  'dCnPalas': 'Particle Count Density in #/cm$^{3}$ (Palas)', 
                  'P1_lpo_loRa': '> 1 μm LPO (LoRa)',
                  'P1_ratio_loRa': '> 1 μm Ratio (LoRa)',
                  'P1_conc_loRa':'> 1 μm Concentration in $\mu g/m^3$ (LoRa)' , 
                  'P2_lpo_loRa': '> 2.5 μm LPO (LoRa)',
                  'P2_ratio_loRa': '> 2.5 μm Ratio (LoRa)', 
                  'P2_conc_loRa': '> 2.5 μm Concentration in $\mu g/m^3$ (LoRa)',
                  'Temperature_loRa': 'Temperature in ℃ (LoRa)', 
                  'Pressure_loRa': 'Pressure in Pa (LoRa)',
                  'Humidity_loRa': 'Humidity in % (LoRa)'} 


def hist_plots(df):
    for col in df.columns[1:len(df.columns)]:
        sns.histplot(df[col], kde = True, color = 'blue', bins = 30)
        plt.title(f'{dict_col_names[col]} Histogram with KDE')
        plt.xlabel(col)
        plt.ylabel('Count')
        plt.show()
#hist_plots(df)


############# Plotting box plot for each parameter ################


def box_plots(df):
    for col in df.columns[1:len(df.columns)]:
        plt.boxplot(df[col])
        plt.title(f'{dict_col_names[col]} BoxPlot')
        plt.xlabel(col)
        plt.ylabel('Count')
        plt.show()
# box_plots(df)


df_up = df.iloc[:,7:-3]
df_up


###################### Removing Outliers using IQR method ############################


# hist_plots(filtered_data)


# box_plots(filtered_data)


######################### Start Running various Machine Learning Models ###########################


#Insted of filtered data lets use df and see the difference
filtered_data = df_up
filtered_data.to_csv(r'D:\UTD\UTDFall2023\Calibration-of-LoRaNodes-using-Super-Learners\data\calibrate_data.csv', index=False)
filtered_data.columns


col_name = list(df_up)
x =[]
y_Palas  = []
for i in col_name:
    if "_loRa" in i:
        x.append(i)
    if "Palas" in i:
        y_Palas.append(i)
Palas = {}
for i in y_Palas:
    Palas_cols = x + [i]
    Palas[i[:-len("Palas")]] = filtered_data[Palas_cols]
y_Palas


def plot_residuals(predict_test,y_test,filtered_data):
    idx = X_test.index
    data_test = filtered_data.loc[idx]
    data_test["dateTime"] = pd.to_datetime(data_test["dateTime"])
    residuals = (np.array(y_test) - predict_test)**2
    plt.figure()
    plt.scatter(data_test["dateTime"], residuals)
    plt.xticks(rotation = 45)
    plt.xlabel('Date Time')
    plt.ylabel(y_test.name)
    plt.show()


dict_col_regression = {'pm1Palas':'PM$_{1.0}$', 
                       'pm2_5Palas': 'PM$_{2.5}$',
                       'pm4Palas': 'PM$_{4.0}$',
                       'pm10Palas': 'PM$_{10.0}$',
                       'pmTotalPalas': 'Total PM Concentration', 
                       'dCnPalas': 'Particle Count Density',
                       'dateTime': 'Date Time',
                       'P1_lpo_loRa':'> 1 μm LPO',
                       'P1_ratio_loRa': '> 1 μm ratio', 
                       'P1_conc_loRa': '> 1 μm Concentration', 
                       'P2_lpo_loRa': '> 2.5 μm LPO', 
                       'P2_ratio_loRa': '> 2.5 μm ratio',
                       'P2_conc_loRa': '> 2.5 μm Concentration' , 
                       'Temperature_loRa': 'Temperature', 
                       'Pressure_loRa': 'Pressure', 
                       'Humidity_loRa':'Humidity'}
unit_regression = {'pm_conc':'($\mu g/m^3$)',
                   'dCn':'(#/cm$^{3}$)'} 


for k,v in enumerate(Palas):
    print(v)


# import logging

# # Suppress all logs
# logging.getLogger("lightgbm").setLevel(logging.ERROR)

# r2_score_test_lr ={}
# r2_score_test_nn ={}
# r2_score_test_rf ={}
# r2_score_test_br ={}
# r2_score_test_dt ={}
# r2_score_test_lgbm ={}
# r2_score_test_gr = {}
# r2_score_test_rr = {}
# r2_score_test_knn = {}
# r2_score_test_xgb = {}
# r2_score_test_et = {}
# r2_score_test_sl = {}

# %run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Linear_Regression.ipynb
# %run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Random_Forest_Regression.ipynb
# #%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Extra_Trees_Regression.ipynb
# %run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Decision_Tree_Regression.ipynb
# %run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Ensemble_Bagging_Regression.ipynb
# %run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/LGBM_Regression.ipynb
# %run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Gaussian_Regression.ipynb
# %run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Ridge_Regression.ipynb
# %run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/KNN_Regression.ipynb
# %run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Neural_Network_Regression.ipynb
# %run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/XGBoost_Regression.ipynb
# %run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Stacking_Regression.ipynb
# %run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Stacking_Regression_Ridge_Regression.ipynb
# # %run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Random_Forest_Regression_Optimized.ipynb
# # %run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Neural_Network_Regression_Optimized.ipynb
# # %run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Decision_Tree_Regression_Optimized.ipynb
# # %run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Ensemble_Bagging_Regression_Optimized.ipynb
# # %run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/LGBM_Regression_Optimized.ipynb


# trained_model_nn={}
# trained_model_rf={}
# trained_model_br={}
# trained_model_dt={}
# trained_model_lgbm={}

# hyperparameters_nn={}
# hyperparameters_rf={}
# hyperparameters_br={}
# hyperparameters_dt={}
# hyperparameters_lgbm={} 

# for k,v in enumerate(Palas):
#     X = Palas[v].drop([v+"Palas"],axis = 1)
#     y = Palas[v][v+"Palas"]
#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,shuffle=False)
#     X_train.to_csv("X_train.csv")
#     X_test.to_csv("X_test.csv")
#     y_train.to_csv("y_train.csv")
#     y_test.to_csv("y_test.csv")
    
#     r2_score_test_sl[v] = Stacking_Regression(X_train,X_test,y_train,y_test,filtered_data)

  
#     # r2_score_test_lr[v] =  Linear_Regression(X_train,X_test,y_train,y_test,filtered_data)
#     # r2_score_test_rf[v] = Random_Forest_Regression(X_train,X_test,y_train,y_test,filtered_data)
#     # r2_score_test_xgb[v] = XGBoost_Regression(X_train,X_test,y_train,y_test,filtered_data)
#     # r2_score_test_dt[v] = Decision_Tree_Regression(X_train,X_test,y_train,y_test,filtered_data)   
#     # r2_score_test_lgbm[v]  =  LGBM_Regression(X_train,X_test,y_train,y_test,filtered_data)
#     # r2_score_test_br[v] = Ensemble_Bagging_Regression(X_train,X_test,y_train,y_test,filtered_data)
#     # r2_score_test_nn[v] = Neural_Network_Regression(X_train,X_test,y_train,y_test,filtered_data)
#     # r2_score_test_gr[v] =  Gaussian_Regression(X_train,X_test,y_train,y_test,filtered_data)
#     # # r2_score_test_rr[v] = Ridge_Regression(X_train,X_test,y_train,y_test,filtered_data)
#     # # r2_score_test_knn[v] = KNN_Regression(X_train,X_test,y_train,y_test,filtered_data)


# print('before filtering')
# # print('lr',r2_score_test_lr)
# # print('rf',r2_score_test_rf)
# # print('xgb',r2_score_test_xgb)
# # print('dt',r2_score_test_dt)   
# # print('lgbm',r2_score_test_lgbm)
# # print('br',r2_score_test_br)
# # print('nn',r2_score_test_nn)
# # ## print('gr',r2_score_test_gr[v])
# # print('rr',r2_score_test_rr)
# # print('knn',r2_score_test_knn)
# print('sl',r2_score_test_sl)

# print('nn', hyperparameters_nn)
# print('rf', hyperparameters_rf)
# print('nn', hyperparameters_nn)
# print('dt', hyperparameters_dt)
# print('lgbm', hyperparameters_lgbm) 





# %run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Stacking_Regression_Optimized.ipynb


# r2_score_test_lr ={}
# r2_score_test_nn ={}
# r2_score_test_rf ={}
# r2_score_test_br ={}
# r2_score_test_dt ={}
# r2_score_test_lgbm ={}
# r2_score_test_gr = {}
# r2_score_test_rr = {}
# r2_score_test_knn = {}
# r2_score_test_sl = {}

# for k,v in enumerate(Palas):
#     X = Palas[v].drop([v+"Palas"],axis = 1)
#     y = Palas[v][v+"Palas"]
#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state= 40)
#     # print("x train",X_train.shape)
#     # print("x test",X_test.shape)
#     # print("y train",y_train.shape)
#     # print("y test",y_test.shape)
    
#     # r2_score_test_lr[v] =  Linear_Regression(X_train,X_test,y_train,y_test,filtered_data)
#     r2_score_test_rf[v] = Random_Forest_Regression(X_train,X_test,y_train,y_test,filtered_data)
#     # r2_score_test_dt[v] = Decision_Tree_Regression(X_train,X_test,y_train,y_test,filtered_data)   
#     # r2_score_test_lgbm[v]  =  LGBM_Regression(X_train,X_test,y_train,y_test,filtered_data)
#     # r2_score_test_br[v] = Ensemble_Bagging_Regression(X_train,X_test,y_train,y_test,filtered_data)
#     # r2_score_test_nn[v] = Neural_Network_Regression(X_train,X_test,y_train,y_test,filtered_data)
#     # #r2_score_test_gr[v] =  Gaussian_Regression(X_train,X_test,y_train,y_test,filtered_data)
#     # r2_score_test_rr[v] = Ridge_Regression(X_train,X_test,y_train,y_test,filtered_data)
#     # r2_score_test_knn[v] = KNN_Regression(X_train,X_test,y_train,y_test,filtered_data)
#     r2_score_test_sl[v] = Stacking_Regression(X_train,X_test,y_train,y_test,filtered_data)


# # print('before filtering')
# # print('lr',r2_score_test_lr)
# print('rf',r2_score_test_rf)
# # print('dt',r2_score_test_dt)   
# # print('lgbm',r2_score_test_lgbm)
# # print('br',r2_score_test_br)
# # print('nn',r2_score_test_nn)
# # #print('gr',r2_score_test_gr[v])
# # print('rr',r2_score_test_rr)
# # print('knn',r2_score_test_knn)
# print('sl',r2_score_test_sl)



# combined_dict ={key: [r2_score_test_lr.get(key),r2_score_test_rf.get(key), r2_score_test_br.get(key),r2_score_test_lgbm.get(key), r2_score_test_nn.get(key), r2_score_test_dt.get(key),r2_score_test_rr.get(key),r2_score_test_knn.get(key),r2_score_test_sl.get(key)] for key in r2_score_test_nn}
# r2_values = combined_dict
# # Creating a DataFrame from the dictionary
# models = ["Linear Regressor","Random Forest", "Ensemble Bagging","LGBM", "Neural Network", "Decision Tree","Ridge Regressor","KNN","Super Learner"]
# r2_values
# df_updated_r2 = pd.DataFrame(r2_values, index=models)
# df_updated_r2.index.name = 'Model'
# df_updated_r2.columns.name = 'Target Variable'


# df_updated_r2.to_csv("Before_filtering_R2.csv",index=True)


r2_score_test_rf_optim ={}
r2_score_test_dt_optim ={}
r2_score_test_nn_optim ={}
r2_score_test_br_optim ={}
r2_score_test_lgbm_optim ={}


trained_model_knn = {}
hyperparameters_knn = {}
r2_score_test_knn_optim = {}
print("========================= KNN Regressor ================================")
%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/KNN_Regression_Optimized.ipynb

for k,v in enumerate(Palas):
    X = Palas[v].drop([v+"Palas"],axis = 1)
    y = Palas[v][v+"Palas"]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state= 40)
    # print("x train",X_train.shape)
    # print("x test",X_test.shape)
    # print("y train",y_train.shape)
    # print("y test",y_test.shape)
    
    # print(Random_Forest_Regression(X_train,X_test,y_train,y_test,filtered_data))
    model,hyperparameters,r2_score_test = KNN_Regression(X_train,X_test,y_train,y_test,filtered_data)
    trained_model_knn[v] = model
    hyperparameters_knn[v] = hyperparameters
    r2_score_test_knn_optim[v] = r2_score_test
r2_score_test_knn_optim


print("========================= Random Forest Regressor ================================")
%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Random_Forest_Regression_Optimized.ipynb

for k,v in enumerate(Palas):
    X = Palas[v].drop([v+"Palas"],axis = 1)
    y = Palas[v][v+"Palas"]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state= 40)
    # print("x train",X_train.shape)
    # print("x test",X_test.shape)
    # print("y train",y_train.shape)
    # print("y test",y_test.shape)
    
    # print(Random_Forest_Regression(X_train,X_test,y_train,y_test,filtered_data))
    model,hyperparameters,r2_score_test = Random_Forest_Regression(X_train,X_test,y_train,y_test,filtered_data)
    trained_model_rf[v] = model
    hyperparameters_rf[v] = hyperparameters
    r2_score_test_rf_optim[v] = r2_score_test
r2_score_test_rf_optim


print("========================= Decision Tree Regressor ================================")
%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Decision_Tree_Regression_Optimized.ipynb

for k,v in enumerate(Palas):
    X = Palas[v].drop([v+"Palas"],axis = 1)
    y = Palas[v][v+"Palas"]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state= 40)
    # print("x train",X_train.shape)
    # print("x test",X_test.shape)
    # print("y train",y_train.shape)
    # print("y test",y_test.shape)
    
    # print(Random_Forest_Regression(X_train,X_test,y_train,y_test,filtered_data))
    model,hyperparameters,r2_score_test = Decision_Tree_Regression(X_train,X_test,y_train,y_test,filtered_data)
    trained_model_dt[v] = model
    hyperparameters_dt[v] = hyperparameters
    r2_score_test_dt_optim[v] = r2_score_test
r2_score_test_dt_optim


# print("========================= Neural Network Regressor ================================")
# %run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Neural_Network_Regression_Optimized.ipynb

# for k,v in enumerate(Palas):
#     X = Palas[v].drop([v+"Palas"],axis = 1)
#     y = Palas[v][v+"Palas"]
#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state= 40)
#     # print("x train",X_train.shape)
#     # print("x test",X_test.shape)
#     # print("y train",y_train.shape)
#     # print("y test",y_test.shape)
    
#     model,hyperparameters,r2_score_test = Neural_Network_Regression(X_train,X_test,y_train,y_test,filtered_data)
#     trained_model_nn[v] = model
#     hyperparameters_nn[v] = hyperparameters
#     r2_score_test_nn_optim[v] = r2_score_test
# r2_score_test_nn_optim


print("========================= Ensemble Bagging Regressor ================================")
%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Ensemble_Bagging_Regression_Optimized.ipynb

for k,v in enumerate(Palas):
    X = Palas[v].drop([v+"Palas"],axis = 1)
    y = Palas[v][v+"Palas"]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state= 40)
    # print("x train",X_train.shape)
    # print("x test",X_test.shape)
    # print("y train",y_train.shape)
    # print("y test",y_test.shape)
    
    model,hyperparameters,r2_score_test = Ensemble_Bagging_Regression(X_train,X_test,y_train,y_test,filtered_data)
    trained_model_br[v] = model
    hyperparameters_br[v] = hyperparameters
    r2_score_test_br_optim[v] = r2_score_test
r2_score_test_br_optim



print("========================= LGBM Regressor ================================")
%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/LGBM_Regression_Optimized.ipynb

for k,v in enumerate(Palas):
    X = Palas[v].drop([v+"Palas"],axis = 1)
    y = Palas[v][v+"Palas"]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state= 40)
    # print("x train",X_train.shape)
    # print("x test",X_test.shape)
    # print("y train",y_train.shape)
    # print("y test",y_test.shape)
    
    model,hyperparameters,r2_score_test  =  LGBM_Regression(X_train,X_test,y_train,y_test,filtered_data)
    trained_model_lgbm[v] = model
    hyperparameters_lgbm[v] = hyperparameters
    r2_score_test_lgbm_optim[v] = r2_score_test
r2_score_test_lgbm_optim


# for k,v in enumerate(Palas):
#     X = Palas[v].drop([v+"Palas"],axis = 1)
#     y = Palas[v][v+"Palas"]
#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state= 40)
#     print("x train",X_train.shape)
#     print("x test",X_test.shape)
#     print("y train",y_train.shape)
#     print("y test",y_test.shape)

#     # print("========================= Random Forest Regressor ================================")
#     # model,hyperparameters,r2_score_test = Random_Forest_Regression(X_train,X_test,y_train,y_test,filtered_data)
#     # trained_model_rf[v] = model
#     # hyperparameters_rf[v] = hyperparameters
#     # r2_score_test_rf[v] = r2_score_test


#     # print("========================= Decision Tree Regressor ================================")
#     # model,hyperparameters,r2_score_test = Decision_Tree_Regression(X_train,X_test,y_train,y_test,filtered_data)   
#     # trained_model_dt[v] = model
#     # hyperparameters_dt[v] = hyperparameters
#     # r2_score_test_dt[v] = r2_score_test


#     # print("========================= LGBM Regressor ================================")
#     # model,hyperparameters,r2_score_test  =  LGBM_Regression(X_train,X_test,y_train,y_test,filtered_data)
#     # trained_model_lgbm[v] = model
#     # hyperparameters_lgbm[v] = hyperparameters
#     # r2_score_test_lgbm[v] = r2_score_test

#     # print("========================= Ensemble Bagging Regressor ================================")
#     # model,hyperparameters,r2_score_test = Ensemble_Bagging_Regression(X_train,X_test,y_train,y_test,filtered_data)
#     # trained_model_br[v] = model
#     # hyperparameters_br[v] = hyperparameters
#     # r2_score_test_br[v] = r2_score_test

#     # print("========================= Neural Network Regressor ================================")
#     # model,hyperparameters,r2_score_test = Neural_Network_Regression(X_train,X_test,y_train,y_test,filtered_data)
#     # trained_model_nn[v] = model
#     # hyperparameters_nn[v] = hyperparameters
#     # r2_score_test_nn[v] = r2_score_test

#     print("========================= Linear Regressor ================================")
#     Linear_Regression(X_train,X_test,y_train,y_test,filtered_data)


# # # #     print("========================= Gaussian Regressor ================================")
# # # # #    Gaussian_Regression(X_train,X_test,y_train,y_test,filtered_data)



# # # #     print("========================= Ridge Regressor ================================")
# # # #     Ridge_Regression(X_train,X_test,y_train,y_test,filtered_data)

# # # #     print("========================= KNN Regressor ================================")
# # # #     KNN_Regression(X_train,X_test,y_train,y_test,filtered_data)





import pickle

hyperparameter_dict_before_filtering ={key: [hyperparameters_rf.get(key), hyperparameters_br.get(key),hyperparameters_lgbm.get(key),hyperparameters_dt.get(key)] for key in hyperparameters_lgbm}
pickle.dump(hyperparameter_dict_before_filtering, open("hyperparameter_dict_before_filtering.p", "wb"))
hyperparameter_dict_before_filtering


# ########################### Run this cell again ################################
# hyperparameter_dict_before_filtering ={key: [hyperparameters_rf.get(key), hyperparameters_br.get(key),hyperparameters_lgbm.get(key)] for key in hyperparameters_lgbm}
# r2_values = hyperparameter_dict_before_filtering
# # Creating a DataFrame from the dictionary
# models = ["Random Forest", "Ensemble Bagging","LGBM", "Neural Network"]
# r2_values
# df_updated_r2 = pd.DataFrame(r2_values, index=models)
# df_updated_r2.index.name = 'Model'
# df_updated_r2.columns.name = 'Target Variable'

# print("Before filtering and tuned")

# df_updated_r2.to_csv("Before_filtering_and_Tuned_R2.csv",index=True)
# df_updated_r2


# r2_score_test_stacking_optim = {}
# print("========================= Stacking  Regressor  = Gradient Boost ================================")
# %run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Stacking_Regression_Gradient_Boost.ipynb

# for k,v in enumerate(Palas):
#     X = Palas[v].drop([v+"Palas"],axis = 1)
#     y = Palas[v][v+"Palas"]
#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state= 40)
#     # print("x train",X_train.shape)
#     # print("x test",X_test.shape)
#     # print("y train",y_train.shape)
#     # print("y test",y_test.shape)
    
#     r2_score_test  =  Stacking_Regression(X_train,X_test,y_train,y_test,filtered_data,hyperparameter_dict_before_filtering[v])
#     r2_score_test_stacking_optim = r2_score_test
# r2_score_test_stacking_optim


r2_score_test_stacking_optim = {}
print("========================= Stacking  Regressor  =  Linear Regressor ================================")
%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Stacking_Regression_Linear_Regression.ipynb

for k,v in enumerate(Palas):
    X = Palas[v].drop([v+"Palas"],axis = 1)
    y = Palas[v][v+"Palas"]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state= 40)
    # print("x train",X_train.shape)
    # print("x test",X_test.shape)
    # print("y train",y_train.shape)
    # print("y test",y_test.shape)
    
    r2_score_test  =  Stacking_Regression(X_train,X_test,y_train,y_test,filtered_data,v,hyperparameter_dict_before_filtering[v])
    r2_score_test_stacking_optim[v] = r2_score_test
r2_score_test_stacking_optim


r2_score_test_stacking_optim = {}
print("========================= Stacking  Regressor  = Ridge  Regressor ================================")
%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Stacking_Regression_Ridge_Regression.ipynb

for k,v in enumerate(Palas):
    X = Palas[v].drop([v+"Palas"],axis = 1)
    y = Palas[v][v+"Palas"]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state= 40)
    # print("x train",X_train.shape)
    # print("x test",X_test.shape)
    # print("y train",y_train.shape)
    # print("y test",y_test.shape)
    
    r2_score_test  =  Stacking_Regression(X_train,X_test,y_train,y_test,filtered_data,hyperparameter_dict_before_filtering[v])
    r2_score_test_stacking_optim[v] = r2_score_test
r2_score_test_stacking_optim


r2_score_test_stacking_optim = {}
print("========================= Stacking  Regressor  = Lasso  Regressor ================================")
%run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Stacking_Regression_Lasso_Regression.ipynb

for k,v in enumerate(Palas):
    X = Palas[v].drop([v+"Palas"],axis = 1)
    y = Palas[v][v+"Palas"]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state= 40)
    # print("x train",X_train.shape)
    # print("x test",X_test.shape)
    # print("y train",y_train.shape)
    # print("y test",y_test.shape)
    
    r2_score_test  =  Stacking_Regression(X_train,X_test,y_train,y_test,filtered_data,hyperparameter_dict_before_filtering[v])
    r2_score_test_stacking_optim[v] = r2_score_test
r2_score_test_stacking_optim


%pip install optuna


import optuna
from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, StackingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import Ridge
from lightgbm import LGBMRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_squared_error, r2_score
import pandas as pd

r2_score_test_stacking_optim = {}

for k,v in enumerate(Palas):
    X = Palas[v].drop([v+"Palas"],axis = 1)
    y = Palas[v][v+"Palas"]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state= 40)
    # Function to optimize base learners and meta-learner
    def optimize_stacking(X_train, y_train):
        # Define a function for Bayesian optimization of each base learner and the meta-learner
        def objective(trial):
            # Hyperparameters for Random Forest
            rf_n_estimators = trial.suggest_int('rf_n_estimators', 50, 300)
            rf_max_depth = trial.suggest_int('rf_max_depth', 5, 50)
            rf_min_samples_split = trial.suggest_int('rf_min_samples_split', 2, 20)
            rf_min_samples_leaf = trial.suggest_int('rf_min_samples_leaf', 1, 10)
    
            # Hyperparameters for Bagging Regressor
            bagging_n_estimators = trial.suggest_int('bagging_n_estimators', 50, 300)
            bagging_max_samples = trial.suggest_float('bagging_max_samples', 0.5, 1.0)
            bagging_max_features = trial.suggest_float('bagging_max_features', 0.5, 1.0)
    
            # Hyperparameters for LGBM
            lgbm_n_estimators = trial.suggest_int('lgbm_n_estimators', 50, 300)
            lgbm_max_depth = trial.suggest_int('lgbm_max_depth', 3, 20)
            lgbm_learning_rate = trial.suggest_loguniform('lgbm_learning_rate', 0.01, 0.1)
            lgbm_num_leaves = trial.suggest_int('lgbm_num_leaves', 20, 50)
            lgbm_min_child_samples = trial.suggest_int('lgbm_min_child_samples', 10, 50)
    
            # Hyperparameters for Neural Network
            nn_hidden_layer_size = trial.suggest_int('nn_hidden_layer_size', 32, 128)
            nn_max_iter = trial.suggest_int('nn_max_iter', 200, 500)
    
            # Hyperparameters for Ridge (Meta-Learner)
            ridge_alpha = trial.suggest_loguniform('ridge_alpha', 0.01, 10.0)
    
            # Define base learners
            rf_regressor = RandomForestRegressor(
                n_estimators=rf_n_estimators,
                max_depth=rf_max_depth,
                min_samples_split=rf_min_samples_split,
                min_samples_leaf=rf_min_samples_leaf,
                random_state=0
            )
    
            bagging_regressor = BaggingRegressor(
                estimator=DecisionTreeRegressor(),
                n_estimators=bagging_n_estimators,
                max_samples=bagging_max_samples,
                max_features=bagging_max_features,
                random_state=42
            )
    
            lgbm_regressor = LGBMRegressor(
                n_estimators=lgbm_n_estimators,
                max_depth=lgbm_max_depth,
                learning_rate=lgbm_learning_rate,
                num_leaves=lgbm_num_leaves,
                min_child_samples=lgbm_min_child_samples,
                random_state=1
            )
    
            nn_regressor = MLPRegressor(
                hidden_layer_sizes=(nn_hidden_layer_size, nn_hidden_layer_size),
                activation='relu',
                random_state=1,
                max_iter=nn_max_iter
            )
    
            # Meta-Learner
            final_estimator = Ridge(alpha=ridge_alpha, random_state=42)
    
            # Define stacking regressor
            model = StackingRegressor(
                estimators=[
                    ('rf', rf_regressor),
                    ('bagging', bagging_regressor),
                    ('lgbm', lgbm_regressor),
                    ('nn', nn_regressor)],
                final_estimator=final_estimator
            )
    
            # Perform cross-validation (or train-test split)
            model.fit(X_train, y_train)
            y_pred = model.predict(X_train)
            score = -mean_squared_error(y_train, y_pred)  # Negative because Optuna minimizes
    
            return score
    
        # Use Optuna for optimization
        study = optuna.create_study(direction='minimize')
        study.optimize(objective, n_trials=50)
    
        # Print best hyperparameters
        print("Best Hyperparameters:")
        print(study.best_params)
        return study.best_params
    
    # Stacking regression function with optimization
    def Stacking_Regression(X_train, X_test, y_train, y_test):
        # Optimize hyperparameters
        best_params = optimize_stacking(X_train, y_train)
    
        # Define base learners with best parameters
        rf_regressor = RandomForestRegressor(
            n_estimators=best_params['rf_n_estimators'],
            max_depth=best_params['rf_max_depth'],
            min_samples_split=best_params['rf_min_samples_split'],
            min_samples_leaf=best_params['rf_min_samples_leaf'],
            random_state=0
        )
    
        bagging_regressor = BaggingRegressor(
            estimator=DecisionTreeRegressor(),
            n_estimators=best_params['bagging_n_estimators'],
            max_samples=best_params['bagging_max_samples'],
            max_features=best_params['bagging_max_features'],
            random_state=42
        )
    
        lgbm_regressor = LGBMRegressor(
            n_estimators=best_params['lgbm_n_estimators'],
            max_depth=best_params['lgbm_max_depth'],
            learning_rate=best_params['lgbm_learning_rate'],
            num_leaves=best_params['lgbm_num_leaves'],
            min_child_samples=best_params['lgbm_min_child_samples'],
            random_state=1
        )
    
        nn_regressor = MLPRegressor(
            hidden_layer_sizes=(best_params['nn_hidden_layer_size'], best_params['nn_hidden_layer_size']),
            activation='relu',
            random_state=1,
            max_iter=best_params['nn_max_iter']
        )
    
        # Meta-Learner
        final_estimator = Ridge(alpha=best_params['ridge_alpha'], random_state=42)
    
        # Stacking Regressor
        model = StackingRegressor(
            estimators=[
                ('rf', rf_regressor),
                ('bagging', bagging_regressor),
                ('lgbm', lgbm_regressor),
                ('nn', nn_regressor)],
            final_estimator=final_estimator
        )
    
        # Train and evaluate
        model.fit(X_train, y_train)
        predict_train = model.predict(X_train)
        predict_test = model.predict(X_test)
    
        train_df = pd.DataFrame({'Actual': y_train, 'Predicted': predict_train, 'Category': 'Training'}, index=y_train.index)
        test_df = pd.DataFrame({'Actual': y_test, 'Predicted': predict_test, 'Category': 'Testing'}, index=y_test.index)
    
        r2_score_train = r2_score(y_train, predict_train)
        r2_score_test = r2_score(y_test, predict_test)
        print(f"R² Train: {r2_score_train:.2f}")
        print(f"R² Test: {r2_score_test:.2f}")
    
        return r2_score_test
    
    r2_score_test  =  Stacking_Regression(X_train, X_test, y_train, y_test)
    r2_score_test_stacking_optim[v] = r2_score_test
r2_score_test_stacking_optim



r2_score_test_stacking_optim


print(rf'${{{dict_col_regression.get("pm1Palas", "")}}}$')



# r2_score_test_stacking_optim = {}
# print("========================= Stacking  Regressor  =  XGB Regressor ================================")
# %run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Stacking_Regression_XGB_Regression.ipynb

# for k,v in enumerate(Palas):
#     X = Palas[v].drop([v+"Palas"],axis = 1)
#     y = Palas[v][v+"Palas"]
#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state= 40)
#     # print("x train",X_train.shape)
#     # print("x test",X_test.shape)
#     # print("y train",y_train.shape)
#     # print("y test",y_test.shape)
    
#     r2_score_test  =  Stacking_Regression(X_train,X_test,y_train,y_test,filtered_data,hyperparameter_dict_before_filtering[v])
#     r2_score_test_stacking_optim[v] = r2_score_test
# r2_score_test_stacking_optim


# r2_score_test_stacking_optim = {}
# print("========================= Stacking  Regressor  =  Random Forest Regressor ================================")
# %run D:/UTD/UTDFall2023/Calibration-of-LoRaNodes-using-Super-Learners/models/Stacking_Regression_Random_Forest_Regression.ipynb

# for k,v in enumerate(Palas):
#     X = Palas[v].drop([v+"Palas"],axis = 1)
#     y = Palas[v][v+"Palas"]
#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state= 40)
#     # print("x train",X_train.shape)
#     # print("x test",X_test.shape)
#     # print("y train",y_train.shape)
#     # print("y test",y_test.shape)
    
#     r2_score_test  =  Stacking_Regression(X_train,X_test,y_train,y_test,filtered_data,hyperparameter_dict_before_filtering[v])
#     r2_score_test_stacking_optim[v] = r2_score_test
# r2_score_test_stacking_optim
